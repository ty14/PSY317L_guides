# Correlation

intro lines about what correlation is etc...


## Pearson Correlation

Pearson's correlation is measured by `r` and ranges between -1 and +1.  +1 indicates that the variables `X` and `Y` are maximally positively correlated, such that as values of X increase so do values of Y. -1 indicates a compleltely negative correlation such that as values of `X` increase, values of `Y` decrease.  A value of 0 indicates that there is no overall relationship.


*insert figure of negative 0.6, 0 and positive 0.6 here*



The below image shows scatterplots, each with a sample size of 30. The trendline is to help demonstrate how correlations of different magnitudes look in terms of their association.

![Correlations](img/correlations.png)



## Calculating the Pearson Correlation in R

To calculate the correlation coefficient in R, it's pretty straightforward.  You simply can use the `cor()` function. For instance, let's correlate...


```{r}
cor(1:10,1:10) # replace with an example
```


Before we go further into what we should do with these correlations, and how to signficance test them, let's learn a little bit more about how they come about.



## Cross-products

The formula for calculating the Pearson's correlation coefficient for a sample is:

$r = \frac{\sum_{}^{} z_{x}z_{y}}{n - 1}$


When we have a population, we can use the formula: 


$r = \frac{\sum_{}^{} z_{x}z_{y}}{N}$


Essentially, the steps are to convert all the `X` and `Y` scores into their respective z-scores. Then you mutliply these two values together to get the `cross-product`.  After summing up all the cross-products for each data point, we divide this number by `n-1` if we're dealing with a sample (we usually are), or `N` if we're dealing with a population.  


The sum of the cross-products will therefore be largely positive if positive z-scores are multiple together or if negative z-scores are multiplied together.  The sum of the cross-products will be largely negative if negative z-scores are multipled with positive z-scores.



The following example should help make this clearer.  Look at the following data, its  scatterplot and the correlation coefficient.  They show that we have a positive correlation of `r=0.84`.  Let's break it down how we got that value.

```{r}
x <- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9)
y <- c(2.8, 2.9, 1.6, 5.5, 4.7, 8.1, 3.3, 7.7, 7.1, 5.8)

df <- data.frame(x, y)

df

ggplot(df, aes(x = x, y = y)) + geom_point(size=2)

cor(x,y)
```

First, let's calculate the means and standard deviation (using `sd` so a sample standard deviation) of `x` and `y`.  We need to get these values so we can calculate the z-scores of each.

```{r}

# step 1:  Get the mean and sd of x and y


mean(x)
sd(x)

mean(y)
sd(y)
```


Now, we can calculate the z-scores, remembering that the formula for that is:  

$z = \frac{x - \overline{x}}{s_{x}}$

```{r}
# step 2. Calculate z-scores of x, and z-scores of y.

df$zx <- (x - mean(x)) / sd(x)  # z scores of x
df$zy <- (y - mean(y)) / sd(y)  # z scores of y

df

```


Following this, we simply multiple the z-scores of `x` and `y` against each other for every data point:

```{r}
# step 3. Calculate the cross-product:  zx * zy

df$zxzy <- df$zx * df$zy

df

```


We now have all of our cross-products.  Notice why the majority are positive. This is because we have multiplied positive $z_{x}$ with positive $z_{y}$ or we multiplied negative $z_{x}$ with negative $z_{y}$.  This happens because datapoints that tend to be above the mean for `x` are also above the mean for `y`, and points that are below the mean of `x` are also below the mean of `y`.  

We can add this up to get the sum of the cross-products.  That is the $\sum_{}^{} z_{x}z_{y}$ in the formula.

```{r}

# step 4.  Sum up the cross products.

sum(df$zxzy) # 7.58

```

We now divide that by `n-1` as we have a sample, to get the correlation coefficient `r`. That gives us an estimation of the average cross-product.


```{r}

# step 5- calculate 'r' by dividing by n-1. (for a sample)

sum(df$zxzy) / 9   # our n was 10, so n-1 = 9

sum(df$zxzy) / (nrow(df) - 1)  # nrow(df) is more generalizable

# r=0.84

```



Just as a quick second example, here is a work through calculating a negative correlation.  Notice the $z_{x}$ and   $z_{y}$ scores that are multiplied together.  They are largely opposite in terms of signs. This is what leads to a negative sum of cross-products and the negative correlation.  Why?  Because data points that are above the mean for `x` are generally below the mean in terms of `y` and visa-versa.


```{r}

### Example 2.   Negative Correlation.

x <- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9)
y <- c(10.4, 10.0, 8.4, 8.5, 8.4, 6.3, 7.1, 6.2, 8.1, 10.0)

df <- data.frame(x, y)

ggplot(df, aes(x = x, y = y)) + geom_point(size=2)


cor(df$x,df$y) 

```

Here is the code, truncated for space:


```{r}
# Calculate z-scores for each x and each y
df$zx <- (x - mean(x)) / sd(x)
df$zy <- (y - mean(y)) / sd(y)

# Calculate the cross-product:  zx * zy
df$zxzy <- df$zx * df$zy

# let's look at the dataframe
# notice the cross products:
df


# Sum up the cross products and Calculate 'r' by dividing by N-1.

sum(df$zxzy) / (nrow(df) - 1)


cor(df$x,df$y) 

```




## Conducting a Pearson Correlation Test

Although `cor()` gives you the correlation between two continuous variables, to actually run a significance test, you need to use `cor.test()`.

Let's use some BlueJay data to do this. We'll just use data on male birds.

```{r}
library(tidyverse)
jays <- read_csv("data/BlueJays.csv")
jayM <- jays %>% filter(KnownSex == "M") # we'll just look at Males

nrow(jayM) # 63 observations

head(jayM)
```


Let's say you're interested in examining whether there is an association between Body Mass and Head Size.  First we'll make a scatterplot between the `Mass` and `Head` columns.  We'll also investigate the correlation using `cor()`.

```{r}
ggplot(jayM, aes(x=Mass, y=Head)) + 
  geom_point(shape = 21, colour = "navy", fill = "dodgerblue") +
  stat_smooth(method="lm", se=F)


cor(jayM$Mass, jayM$Head)  # r = 0.58,  a strong positive correlation.

```


To run the significance test, we do the following:

```{r}
cor.test(jayM$Head, jayM$Mass) 

```

This gives us a lot of information. Firstly, at the bottom it repeats the correlation coefficient `cor`. At the top, it gives us the value of `t` which is essentially how surprising it is for us to get the correlation we did assuming we were drawing our sample from a population where there is no correlation.  Associated with this `t` value is the degrees of freedom which is equal to `n-2`, so in this case that is `63-2 = 61`. The p-value is also given. If we are using `alpha=0.05` as our significance level, then we can reject the hypothesis that there is no overall correlation in the population between Body Mass and Head size if `p<0.05`.  

The default for `cor.test()` is to do a two-tailed test.  This is testing whether your observed correlation `r` is different from `r=0` in either the positive or negative direction.  This default version also gives us the confidence interval for the correlation coefficient. Essentially, this gives us the interval in which we have a 95% confidence that the true population `r` lies (remember we just have data from one sample that theoretically comes from a population).

It's also possible however that you had an **a priori** prediction about the direction of the effect. For instance, you may have predicted that Body Mass would be positively correlated with Head Size. In this case, you could do a one-tailed correlation test, where your alternative hypothesis is that there is a positive correlation and the null is that the correlation coefficient is equal to 0 or less than 0.

To do one-tailed tests you need to add the `alternative` argument.

```{r}

# testing if there is a positive correlation
cor.test(jayM$Head, jayM$Mass, alternative = "greater") 

```


```{r}

# testing if there is a negative correlation
cor.test(jayM$Head, jayM$Mass, alternative = "less") 
```






## Assumptions of Pearson's Correlation

The Pearson Correlation Coefficient requires your data to be approximately normally distributed. To do this we have various options how to test for normality.

Firstly, we could do a Shapiro-Wilk test, which formally determines whether our data are normal. This is done using `shapiro.test()`, where we assume our data are from a normal population if the resulting p-value is above 0.05.  If the p-value is below 0.05 then we have evidence to reject that our data come from a normal population.

With our data above, this would look like this when running the test on each variable:

```{r}
shapiro.test(jayM$Mass)  # P > 0.05, therefore cannot reject null that data is not normal
shapiro.test(jayM$Head)  # P > 0.05, therefore cannot reject null that data is not normal

```


We can also make a QQ-plot for each variable. Essentially what we require from this plot is for the majority of our data to fall on the straight line - especially the datapoints in the middle. Some deviation at the tails is ok.  This plot orders our data and plots the observed data against values on the x-axis that we would expect to get if our data was truly from a normal population.

```{r}
qqnorm(jayM$Mass)
qqline(jayM$Mass, col = "steelblue", lwd = 2)
```

```{r}
qqnorm(jayM$Head)
qqline(jayM$Head, col = "steelblue", lwd = 2)

```

Both of these QQ plots are ok, and indicate normality, as does our Shapiro-Wilk tests. Therefore we would be ok to use a Pearson Correlation test with these data.

What should you do though if either of your continuous variables are not approximately normally distributed? In that case, there are other correlation coefficients and associated significance tests that you could run instead.  We describe these in more detail in Section x.xxx





## Confidence Intervals for R

bit more on this and the theory.



## Partial Correlations

why.... the stupid formula.... and how to do in R.... and that there are technically better ways...


## Non-parametric Correlations

examples....

when I can be bothered, edit this code chunk down into words + code

```{r}


# Example 1.

library(tidyverse)

cheese <- read_csv("data/cheese.csv")

nrow(cheese)

head(cheese)

# let's make a scatterplot of saturated fat against cholesterol
ggplot(cheese, aes(x = sat_fat, y = chol)) + geom_point()


# looks like there's a pretty obvious relationship.

# let's do our checks for normality...

shapiro.test(cheese$sat_fat)  # P < 0.05, therefore can reject null that data is normal
shapiro.test(cheese$chol)  # P < 0.05, therefore cannot reject null that data is normal

qqnorm(cheese$sat_fat)
qqline(cheese$sat_fat, col = "steelblue", lwd = 2)

qqnorm(cheese$chol)
qqline(cheese$chol, col = "steelblue", lwd = 2)



## OK, so doing a Pearson correlation is not best...


# Option 1.  Spearman's Rank Test

ggplot(cheese, aes(x = sat_fat, y = chol)) + geom_point()

cor(cheese$sat_fat, cheese$chol, method = "spearman")      # rho = 0.87

cor.test(cheese$sat_fat, cheese$chol, method = "spearman") 

# p<.05 therefore reject null that rho = 0
# don't worry about the warning message.


# you can do 1-tailed tests as with Pearson

cor.test(cheese$sat_fat, cheese$chol, method = "spearman", alternative = "greater") 


## Option 2 - other types...

## There are other types of non-parametric correlation...
# Kendall's tau-b is commonly used also.

cor.test(cheese$sat_fat, cheese$chol, method = "kendall") 





```



If at least one of your variables of your data are rank (ordinal) data, then you should use non-parametric correlations. 

In the following example, the data show the dominance rank, age, body size and testosterone levels for a group of 18 animals.  Lower numbers of the ranks, indicate a higher ranking animal. An animal with rank 1 means that it is the most dominant individual.

Perhaps with such data you may be interested in seeing if there was an association between dominance rank and testosterone levels. Because your dominance rank measure is ordinal (a rank), then you should pick a non-parametric correlation.


```{r}

test <- read_csv("data/testosterone.csv")

head(test)

ggplot(test, aes(x = drank, y = testosterone)) + 
  geom_point() +
  stat_smooth(method = "lm", se=F) +
  xlab("Dominance Rank") +
  ylab("Testosterone Level") 


cor(test$drank, test$testosterone, method = "spearman") # rho = -0.91

```


If you had the *a priori* prediction, that more dominant animals would have higher testosterone, then you could do a one-tailed test. This would mean that you expect there to be a negative correlation - as the rank number gets higher, the levels of testosterone would fall. In this case, you'd use `alternative = "less"`.

```{r}

cor.test(test$drank, test$testosterone, method = "spearman", alternative = "less") # 1- tailed

```






## Point-Biserial Correlation

why, what....

