<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Distributions | PSY317L Guides</title>
  <meta name="description" content="7 Distributions | PSY317L Guides" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Distributions | PSY317L Guides" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Distributions | PSY317L Guides" />
  
  
  

<meta name="author" content="James P. Curley &amp; Tyler M. Milewski" />


<meta name="date" content="2020-06-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="descriptives.html"/>
<link rel="next" href="confidence-intervals.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome to PSY317!</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-this-book-includes-and-what-it-doesnt"><i class="fa fa-check"></i><b>1.1</b> What this book includes and what it doesn't</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.3</b> References</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r"><i class="fa fa-check"></i><b>1.4</b> Other places to find help about R</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r-and-statistics"><i class="fa fa-check"></i><b>1.5</b> Other places to find help about R and Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#downloading-r"><i class="fa fa-check"></i><b>2.2</b> Downloading R</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#downloading-rstudio"><i class="fa fa-check"></i><b>2.3</b> Downloading RStudio</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#using-rcloud-instead-of-rstudio"><i class="fa fa-check"></i><b>2.4</b> Using RCloud instead of RStudio</a></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#importing-data"><i class="fa fa-check"></i><b>2.5</b> Importing Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic-syntax.html"><a href="basic-syntax.html"><i class="fa fa-check"></i><b>3</b> Basic Syntax</a><ul>
<li class="chapter" data-level="3.1" data-path="basic-syntax.html"><a href="basic-syntax.html#boring-mathematical-stuff"><i class="fa fa-check"></i><b>3.1</b> boring mathematical stuff</a></li>
<li class="chapter" data-level="3.2" data-path="basic-syntax.html"><a href="basic-syntax.html#assignment"><i class="fa fa-check"></i><b>3.2</b> assignment</a></li>
<li class="chapter" data-level="3.3" data-path="basic-syntax.html"><a href="basic-syntax.html#vectors"><i class="fa fa-check"></i><b>3.3</b> vectors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Introduction to Data Carpentry</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#tidyverse"><i class="fa fa-check"></i><b>4.1</b> tidyverse</a></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#filter"><i class="fa fa-check"></i><b>4.2</b> filter()</a></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#select"><i class="fa fa-check"></i><b>4.3</b> select()</a></li>
<li class="chapter" data-level="4.4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#mutate"><i class="fa fa-check"></i><b>4.4</b> mutate()</a></li>
<li class="chapter" data-level="4.5" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#arrange"><i class="fa fa-check"></i><b>4.5</b> arrange()</a></li>
<li class="chapter" data-level="4.6" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#wide-vs-long-data"><i class="fa fa-check"></i><b>4.6</b> Wide vs Long Data</a></li>
<li class="chapter" data-level="4.7" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#joins"><i class="fa fa-check"></i><b>4.7</b> Joins</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>5</b> Data Visualization</a><ul>
<li class="chapter" data-level="5.1" data-path="data-visualization.html"><a href="data-visualization.html#intro-to-ggplot2"><i class="fa fa-check"></i><b>5.1</b> Intro to ggplot2</a></li>
<li class="chapter" data-level="5.2" data-path="data-visualization.html"><a href="data-visualization.html#histogram"><i class="fa fa-check"></i><b>5.2</b> Histogram</a></li>
<li class="chapter" data-level="5.3" data-path="data-visualization.html"><a href="data-visualization.html#scatter"><i class="fa fa-check"></i><b>5.3</b> Scatter</a></li>
<li class="chapter" data-level="5.4" data-path="data-visualization.html"><a href="data-visualization.html#boxplot"><i class="fa fa-check"></i><b>5.4</b> Boxplot</a></li>
<li class="chapter" data-level="5.5" data-path="data-visualization.html"><a href="data-visualization.html#adding-details"><i class="fa fa-check"></i><b>5.5</b> Adding details</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>6</b> Descriptives</a><ul>
<li class="chapter" data-level="6.1" data-path="descriptives.html"><a href="descriptives.html#basic-descriptives"><i class="fa fa-check"></i><b>6.1</b> Basic Descriptives</a></li>
<li class="chapter" data-level="6.2" data-path="descriptives.html"><a href="descriptives.html#mean-median-and-mode"><i class="fa fa-check"></i><b>6.2</b> Mean, Median, and Mode</a></li>
<li class="chapter" data-level="6.3" data-path="descriptives.html"><a href="descriptives.html#standard-deviation"><i class="fa fa-check"></i><b>6.3</b> Standard Deviation</a></li>
<li class="chapter" data-level="6.4" data-path="descriptives.html"><a href="descriptives.html#standard-error"><i class="fa fa-check"></i><b>6.4</b> Standard Error</a></li>
<li class="chapter" data-level="6.5" data-path="descriptives.html"><a href="descriptives.html#inter-quartile-ranges"><i class="fa fa-check"></i><b>6.5</b> Inter-quartile Ranges</a></li>
<li class="chapter" data-level="6.6" data-path="descriptives.html"><a href="descriptives.html#descriptives-for-groups"><i class="fa fa-check"></i><b>6.6</b> Descriptives for Groups</a></li>
<li class="chapter" data-level="6.7" data-path="descriptives.html"><a href="descriptives.html#comparing-population-and-sample-means"><i class="fa fa-check"></i><b>6.7</b> Comparing population and sample means</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>7</b> Distributions</a><ul>
<li class="chapter" data-level="7.1" data-path="distributions.html"><a href="distributions.html#what-is-a-distribution"><i class="fa fa-check"></i><b>7.1</b> What is a distribution ?</a><ul>
<li class="chapter" data-level="7.1.1" data-path="distributions.html"><a href="distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>7.1.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="7.1.2" data-path="distributions.html"><a href="distributions.html#bimodal-distribution"><i class="fa fa-check"></i><b>7.1.2</b> Bimodal Distribution</a></li>
<li class="chapter" data-level="7.1.3" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>7.1.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="7.1.4" data-path="distributions.html"><a href="distributions.html#standard-normal-distribution"><i class="fa fa-check"></i><b>7.1.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="7.1.5" data-path="distributions.html"><a href="distributions.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>7.1.5</b> Skewness and Kurtosis</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="distributions.html"><a href="distributions.html#z-scores"><i class="fa fa-check"></i><b>7.2</b> z-scores</a><ul>
<li class="chapter" data-level="7.2.1" data-path="distributions.html"><a href="distributions.html#z-scores-in-samples."><i class="fa fa-check"></i><b>7.2.1</b> z-scores in samples.</a></li>
<li class="chapter" data-level="7.2.2" data-path="distributions.html"><a href="distributions.html#using-z-scores-to-determine-probabilities"><i class="fa fa-check"></i><b>7.2.2</b> Using z-scores to determine probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="distributions.html"><a href="distributions.html#what-is-a-sampling-distribution"><i class="fa fa-check"></i><b>7.3</b> What is a Sampling Distribution ?</a><ul>
<li class="chapter" data-level="7.3.1" data-path="distributions.html"><a href="distributions.html#sample-size-and-the-sampling-distribution"><i class="fa fa-check"></i><b>7.3.1</b> Sample Size and the Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="distributions.html"><a href="distributions.html#central-limit-theorem."><i class="fa fa-check"></i><b>7.4</b> Central Limit Theorem.</a></li>
<li class="chapter" data-level="7.5" data-path="distributions.html"><a href="distributions.html#sampling-distribution-problems."><i class="fa fa-check"></i><b>7.5</b> Sampling distribution problems.</a></li>
<li class="chapter" data-level="7.6" data-path="distributions.html"><a href="distributions.html#the-t-distribution"><i class="fa fa-check"></i><b>7.6</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>8</b> Confidence Intervals</a></li>
<li class="chapter" data-level="9" data-path="inferential-stats.html"><a href="inferential-stats.html"><i class="fa fa-check"></i><b>9</b> Inferential Stats</a><ul>
<li class="chapter" data-level="9.1" data-path="inferential-stats.html"><a href="inferential-stats.html#comparing-two-samples"><i class="fa fa-check"></i><b>9.1</b> Comparing two Samples</a></li>
<li class="chapter" data-level="9.2" data-path="inferential-stats.html"><a href="inferential-stats.html#independent-samples-t-test"><i class="fa fa-check"></i><b>9.2</b> Independent Samples t-test</a></li>
<li class="chapter" data-level="9.3" data-path="inferential-stats.html"><a href="inferential-stats.html#background-to-students-2-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> Background to Student's 2 Sample t-test</a></li>
<li class="chapter" data-level="9.4" data-path="inferential-stats.html"><a href="inferential-stats.html#sampling-distribution-of-the-difference-in-sample-means"><i class="fa fa-check"></i><b>9.4</b> Sampling Distribution of the Difference in Sample Means</a></li>
<li class="chapter" data-level="9.5" data-path="inferential-stats.html"><a href="inferential-stats.html#pooled-standard-deviation"><i class="fa fa-check"></i><b>9.5</b> Pooled Standard Deviation</a></li>
<li class="chapter" data-level="9.6" data-path="inferential-stats.html"><a href="inferential-stats.html#confidence-interval-for-difference-in-means"><i class="fa fa-check"></i><b>9.6</b> Confidence Interval for Difference in Means</a></li>
<li class="chapter" data-level="9.7" data-path="inferential-stats.html"><a href="inferential-stats.html#conducting-student-t-test"><i class="fa fa-check"></i><b>9.7</b> Conducting Student t-test</a></li>
<li class="chapter" data-level="9.8" data-path="inferential-stats.html"><a href="inferential-stats.html#doing-student-t-test-in-r"><i class="fa fa-check"></i><b>9.8</b> Doing Student t-test in R</a></li>
<li class="chapter" data-level="9.9" data-path="inferential-stats.html"><a href="inferential-stats.html#effect-sizes"><i class="fa fa-check"></i><b>9.9</b> Effect Sizes</a></li>
<li class="chapter" data-level="9.10" data-path="inferential-stats.html"><a href="inferential-stats.html#paired-t-tests"><i class="fa fa-check"></i><b>9.10</b> Paired t-tests</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>10</b> Correlation</a><ul>
<li class="chapter" data-level="10.1" data-path="correlation.html"><a href="correlation.html#pearson-correlation"><i class="fa fa-check"></i><b>10.1</b> Pearson Correlation</a></li>
<li class="chapter" data-level="10.2" data-path="correlation.html"><a href="correlation.html#calculating-the-pearson-correlation-in-r"><i class="fa fa-check"></i><b>10.2</b> Calculating the Pearson Correlation in R</a></li>
<li class="chapter" data-level="10.3" data-path="correlation.html"><a href="correlation.html#cross-products"><i class="fa fa-check"></i><b>10.3</b> Cross-products</a></li>
<li class="chapter" data-level="10.4" data-path="correlation.html"><a href="correlation.html#conducting-a-pearson-correlation-test"><i class="fa fa-check"></i><b>10.4</b> Conducting a Pearson Correlation Test</a></li>
<li class="chapter" data-level="10.5" data-path="correlation.html"><a href="correlation.html#assumptions-of-pearsons-correlation"><i class="fa fa-check"></i><b>10.5</b> Assumptions of Pearson's Correlation</a></li>
<li class="chapter" data-level="10.6" data-path="correlation.html"><a href="correlation.html#confidence-intervals-for-r"><i class="fa fa-check"></i><b>10.6</b> Confidence Intervals for R</a></li>
<li class="chapter" data-level="10.7" data-path="correlation.html"><a href="correlation.html#partial-correlations"><i class="fa fa-check"></i><b>10.7</b> Partial Correlations</a></li>
<li class="chapter" data-level="10.8" data-path="correlation.html"><a href="correlation.html#non-parametric-correlations"><i class="fa fa-check"></i><b>10.8</b> Non-parametric Correlations</a></li>
<li class="chapter" data-level="10.9" data-path="correlation.html"><a href="correlation.html#point-biserial-correlation"><i class="fa fa-check"></i><b>10.9</b> Point-Biserial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>11</b> Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="regression.html"><a href="regression.html#introduction-to-linear-regression"><i class="fa fa-check"></i><b>11.1</b> Introduction to Linear Regression</a></li>
<li class="chapter" data-level="11.2" data-path="regression.html"><a href="regression.html#a-and-b"><i class="fa fa-check"></i><b>11.2</b> a and b</a><ul>
<li class="chapter" data-level="11.2.1" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-in-r"><i class="fa fa-check"></i><b>11.2.1</b> How to calculate a and b in R</a></li>
<li class="chapter" data-level="11.2.2" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-by-hand"><i class="fa fa-check"></i><b>11.2.2</b> How to calculate a and b 'by hand'</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="regression.html"><a href="regression.html#residuals"><i class="fa fa-check"></i><b>11.3</b> Residuals</a><ul>
<li class="chapter" data-level="11.3.1" data-path="regression.html"><a href="regression.html#how-to-calculate-the-residuals"><i class="fa fa-check"></i><b>11.3.1</b> How to calculate the residuals</a></li>
<li class="chapter" data-level="11.3.2" data-path="regression.html"><a href="regression.html#visualizing-the-residuals"><i class="fa fa-check"></i><b>11.3.2</b> Visualizing the Residuals</a></li>
<li class="chapter" data-level="11.3.3" data-path="regression.html"><a href="regression.html#comparing-our-trendline-to-other-trendlines"><i class="fa fa-check"></i><b>11.3.3</b> Comparing our trendline to other trendlines</a></li>
<li class="chapter" data-level="11.3.4" data-path="regression.html"><a href="regression.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>11.3.4</b> Coefficient of Determination R2</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="regression.html"><a href="regression.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>11.4</b> Standard Error of the Estimate</a><ul>
<li class="chapter" data-level="11.4.1" data-path="regression.html"><a href="regression.html#what-to-do-with-the-standard-error-of-the-estimate"><i class="fa fa-check"></i><b>11.4.1</b> What to do with the Standard Error of the Estimate ?</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="regression.html"><a href="regression.html#goodness-of-fit-test---f-ratio"><i class="fa fa-check"></i><b>11.5</b> Goodness of Fit Test - F-ratio</a></li>
<li class="chapter" data-level="11.6" data-path="regression.html"><a href="regression.html#assumptions-of-linear-regression"><i class="fa fa-check"></i><b>11.6</b> Assumptions of Linear Regression</a><ul>
<li class="chapter" data-level="11.6.1" data-path="regression.html"><a href="regression.html#normality-of-residuals"><i class="fa fa-check"></i><b>11.6.1</b> Normality of Residuals</a></li>
<li class="chapter" data-level="11.6.2" data-path="regression.html"><a href="regression.html#linearity----"><i class="fa fa-check"></i><b>11.6.2</b> 2. Linearity ---</a></li>
<li class="chapter" data-level="11.6.3" data-path="regression.html"><a href="regression.html#homogeneity-of-variance-homoscedasticity"><i class="fa fa-check"></i><b>11.6.3</b> 3. Homogeneity of Variance / Homoscedasticity</a></li>
<li class="chapter" data-level="11.6.4" data-path="regression.html"><a href="regression.html#no-colinearity"><i class="fa fa-check"></i><b>11.6.4</b> No Colinearity</a></li>
<li class="chapter" data-level="11.6.5" data-path="regression.html"><a href="regression.html#unusual-datapoints"><i class="fa fa-check"></i><b>11.6.5</b> Unusual Datapoints</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="regression.html"><a href="regression.html#examining-individual-predictor-estimates"><i class="fa fa-check"></i><b>11.7</b> Examining individual predictor estimates</a><ul>
<li class="chapter" data-level="11.7.1" data-path="regression.html"><a href="regression.html#confidence-interval-of-b."><i class="fa fa-check"></i><b>11.7.1</b> 95% confidence interval of 'b'.</a></li>
<li class="chapter" data-level="11.7.2" data-path="regression.html"><a href="regression.html#standard-error-of-b"><i class="fa fa-check"></i><b>11.7.2</b> Standard Error of b</a></li>
<li class="chapter" data-level="11.7.3" data-path="regression.html"><a href="regression.html#calculating-95-confidence-interval-of-b-by-hand"><i class="fa fa-check"></i><b>11.7.3</b> Calculating 95% confidence interval of 'b' by hand</a></li>
<li class="chapter" data-level="11.7.4" data-path="regression.html"><a href="regression.html#signifcance-testing-b"><i class="fa fa-check"></i><b>11.7.4</b> Signifcance Testing b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="permutation-testing.html"><a href="permutation-testing.html"><i class="fa fa-check"></i><b>12</b> Permutation Testing</a><ul>
<li class="chapter" data-level="12.1" data-path="permutation-testing.html"><a href="permutation-testing.html#t-test-permutation"><i class="fa fa-check"></i><b>12.1</b> t-test Permutation</a></li>
<li class="chapter" data-level="12.2" data-path="permutation-testing.html"><a href="permutation-testing.html#correlation-coefficient-permutation-tests"><i class="fa fa-check"></i><b>12.2</b> Correlation Coefficient Permutation Tests</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PSY317L Guides</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distributions" class="section level1">
<h1><span class="header-section-number">7</span> Distributions</h1>
<p>blah blah intro blah</p>
<div id="what-is-a-distribution" class="section level2">
<h2><span class="header-section-number">7.1</span> What is a distribution ?</h2>
<p>In statistical terms a distribution refers to the range of possible values that can come from a sample space. Another way of stating that is to say that a distribution represents how the probabilities of getting various values are distributed.</p>
<p>There are several classic distributions in statistics. There are distributions such as the normal, t, Poisson, bimodal etc.</p>
<p>We're going to dig a bit deeper into distributions, in particular the normal distribution. The best way to look at distributions is to plot histograms. Let's look at some distributions.</p>
<div id="uniform-distribution" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Uniform Distribution</h3>
<p>The first distribution we'll look at is the uniform. A uniform distribution is one where there's an equal probability of getting each value from the distribution. In the example below, we've grabbed 1,000,000 numbers from a uniform distribution that starts at 0 and ends at 100. Let's look at it's shape:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<p>As you can see, the histogram that we have generated is roughly flat across the top. This means that we have equal frequency counts in each bin. Each bin here is 5 across, so the first bin is 0-5, the next bin is 5-10, and so on. We have 25 bins in total in this histogram, and each has roughly 50,000 values in it. This is the classic shape of the uniform distribution.</p>
</div>
<div id="bimodal-distribution" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Bimodal Distribution</h3>
<p>Another family of distributions that is worth our attention are bimodal distributions. In these distributions we have two peaks in the distribution. You can see an example below:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
</div>
<div id="normal-distribution" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Normal Distribution</h3>
<p>In most domains, the type of histograms i.e. distributions, that we most commonly observe don't have 0 peaks like the uniform distribution or 2 peaks like the bimodal distribution, but have just one peak. These are called <strong>unimodal</strong> distributions. One such classic distribution that is very important to statistics is the normal distribution.</p>
<p>The normal distribution has one peak and is symmetrical, with the same proportion of data on each side of the distribution. In addition, we say that a normal distribution has a <em>skewness of 0</em> and a <em>kurtosis of 3</em>. We'll talk about what those are a little bit more about what that means very shortly.</p>
<p>Let's look at a normal distribution. The following normal distribution has a mean of 100 and a standard deviation of 5. We can generate it by collecting 1,000,000 datapoints in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)

x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1000000</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="fl">5.0</span>)

<span class="kw">mean</span>(x) </code></pre></div>
<pre><code>## [1] 100.0002</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x) </code></pre></div>
<pre><code>## [1] 5.000926</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dfnorm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">vals =</span> x)

p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(dfnorm, <span class="kw">aes</span>(<span class="dt">x =</span> vals))  +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;purple&quot;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>, <span class="dt">binwidth =</span> <span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">0.7</span>, <span class="dt">fill =</span> <span class="st">&quot;mistyrose&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;values&quot;</span>)

p</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>Normal distributions can vary in their means and standard deviations. Below is an image of a selection of four different normal distributions that all vary in their means and standard deviations. The dotted vertical black line in each graph indicates where their respective means lie.</p>
<p>We use special notation to indicate that a distribution is a Normal Distribution. For instance, for the normal distribution that has a mean of 17 and a standard deviation of 7, we would write:</p>
<p><span class="math inline">\(N(\mu=17, \sigma^{2}=49)\)</span></p>
<p>which demonstrates that the distribution is approximately normal with a mean of 17 and variance of 49 (which is the standard deviation, 7, squared).</p>
<div class="figure">
<img src="img/norm.png" alt="Normal Distributions" />
<p class="caption">Normal Distributions</p>
</div>
</div>
<div id="standard-normal-distribution" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Standard Normal Distribution</h3>
<p>Although normal distributions can have various means or standard deviations, there is one case that we reserve and call the standard normal distribution. This is for the situation where the mean of the distribution is 0 and the standard deviation (and the variance) is equal to 1.</p>
<p><span class="math inline">\(N(\mu=0, \sigma^{2}=1)\)</span></p>
<div class="figure">
<img src="img/stdnorm.png" alt="Normal Distributions" />
<p class="caption">Normal Distributions</p>
</div>
<p>How does the standard normal distribution come about? We will discuss more about this distribution in section xxx.xxx, but briefly it is obtained by converting all the values of a normal distribution into z-scores. z-scores are calculated by:</p>
<p><span class="math inline">\(z=\frac{x - {\mu}_x}{\sigma_x}\)</span></p>
<p>This standard normal distribution is very useful in statistics because we can precisely calcualte the proportion of the distribution that is to the left or right under the curve at any point of it. This principle forms the basis of several statistical tests.</p>
</div>
<div id="skewness-and-kurtosis" class="section level3">
<h3><span class="header-section-number">7.1.5</span> Skewness and Kurtosis</h3>
<p>Above we described that a normal distribution has a skewness of 0 and a kurtosis of 3, but then we just skipped along and didn't really say anything else. It's important to take a quick step back and think about these two things.</p>
<div id="skewness" class="section level4">
<h4><span class="header-section-number">7.1.5.1</span> Skewness</h4>
<p>We most commonly evaluate skewness for unimodal distributions (those with one peak). The skewness of a distribution can be either negative or positive, or, if it has no skew whatsoever it will be 0.</p>
<p>It is probably easiest to describe skewness by looking at examples. In the picture below, all distributions have a mean of 100 and a standard deviation of 20. However, they differ in their skewness. The one on the left has a skew of +0.68, the one on the right has a skew of -0.68. The one in the middle has a skew of 0 and is the only one that is normally distributed.</p>
<div class="figure">
<img src="img/skew.png" alt="Skewness" />
<p class="caption">Skewness</p>
</div>
<p>Distributions that have negative skew are also called left skewed because their longest tail extends to the left of the distribution. Similarly, distributions that have positive skew are called right skewed because their longest tail extends to the right of the distribution.</p>
<p>Another thing to consider about the skew of the distribution is what happens to the mean, median and mode of the distributions.</p>
<p>First, let's look at the normal distribution we made earlier in this section that had a mean of approximately 100 and a standard deviation of approximately 5. If we get the median, mode and mean of that distribution, we get the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(x)</code></pre></div>
<pre><code>## [1] 100.0002</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">median</span>(x)</code></pre></div>
<pre><code>## [1] 100.0025</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">estimate_mode</span>(x) <span class="co"># R doesn&#39;t have a built in mode function, so I&#39;m using this as a proxy</span></code></pre></div>
<pre><code>## [1] 100.05</code></pre>
<p>We can see here, that all three values are really close to 100. We can look at this in our plot of the distribution. We've overlaid a red line for the mean, a blue line for the median and an orange line for the mode. However, they all lie on top of each other at x=100, so it's hard to distinguish them:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">median</span>(x), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(x), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">estimate_mode</span>(x), <span class="dt">color =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<p>Now let's look at some skewed distributions as to what happens to the mode, median and mean in such distributions.</p>
<p>In this dataset, we have 7486 rows of data (observations). Each row is a MLB player. The three numerical columns refer to the career total hits (<code>totalH</code>), career total at bats (<code>totalAB</code>), and career batting average (<code>avg</code>) of each player.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bats &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/batting.csv&quot;</span>)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   playerID = col_character(),
##   totalH = col_integer(),
##   totalAB = col_integer(),
##   avg = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">nrow</span>(bats)</code></pre></div>
<pre><code>## [1] 7486</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(bats)</code></pre></div>
<pre><code>## # A tibble: 6 x 4
##   playerID  totalH totalAB   avg
##   &lt;chr&gt;      &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;
## 1 aaronha01   3771   12364 0.305
## 2 aaronto01    216     944 0.229
## 3 abbated01    772    3044 0.254
## 4 abbeybe01     38     225 0.169
## 5 abbeych01    493    1756 0.281
## 6 abbotfr01    107     513 0.209</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#histogram</span>
p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bats, <span class="kw">aes</span>(<span class="dt">x =</span> avg)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), 
                 <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, 
                 <span class="dt">fill =</span> <span class="st">&quot;lightseagreen&quot;</span>, 
                 <span class="dt">alpha =</span> <span class="fl">0.2</span>,
                 <span class="dt">binwidth =</span> .<span class="dv">005</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">colour =</span> <span class="st">&#39;black&#39;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Career Batting Average&quot;</span>)

p1</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>
<p>As you can see, this distribution is very negatively (left) skewed. This means that there are many players who have career batting averages between 0.2 and 0.3. There are relatively few players with career averages over 0.3. There are more averages that are less than 0.2 causing the skew.</p>
<p>We can directly measure the skewness using the <code>skewness()</code> function from the <code>moments</code> package. We can see that it is highly negatively skewed with a value of -1.01:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(moments)</code></pre></div>
<pre><code>## Warning: package &#39;moments&#39; was built under R version 3.5.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">skewness</span>(bats$avg) </code></pre></div>
<pre><code>## [1] -1.012683</code></pre>
<p>Let's look at where the median, mean and mode are for this negatively skewed distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">median</span>(bats$avg)</code></pre></div>
<pre><code>## [1] 0.2466792</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(bats$avg)</code></pre></div>
<pre><code>## [1] 0.237972</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">estimate_mode</span>(bats$avg)</code></pre></div>
<pre><code>## [1] 0.2538827</code></pre>
<p>This time, these descriptive values are not equal. The median and mean are lower than the mode. In fact, the mean is lowest of all.</p>
<p>In negative skewed distributions, the mean and median get pulled towards the skewed tail of the distribution, but the mean gets pulled further.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p1 +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">median</span>(bats$avg), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(bats$avg), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">estimate_mode</span>(bats$avg), <span class="dt">color =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<p>Now let's look at what happens to the mode, median and mean in right skewed distributions. Let's look at the career at-bats of MLB players.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#histogram</span>
p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(bats, <span class="kw">aes</span>(<span class="dt">x =</span> totalAB)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), 
                 <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, 
                 <span class="dt">fill =</span> <span class="st">&quot;plum&quot;</span>, 
                 <span class="dt">alpha =</span> <span class="fl">0.2</span>,
                 <span class="dt">binwidth =</span> <span class="dv">200</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">colour =</span> <span class="st">&#39;black&#39;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>()

p2</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-101-1.png" width="672" /></p>
<p>This distribution is extremely right (positive) skewed. If we measure the skewness we find that the skewness is 1.63.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">skewness</span>(bats$totalAB)</code></pre></div>
<pre><code>## [1] 1.626115</code></pre>
<p>Now, let's look at the median, mean and mode and plot these on the distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">median</span>(bats$totalAB)</code></pre></div>
<pre><code>## [1] 1037</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(bats$totalAB)</code></pre></div>
<pre><code>## [1] 1958.68</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">estimate_mode</span>(bats$totalAB)</code></pre></div>
<pre><code>## [1] 454.0069</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p2 +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">median</span>(bats$totalAB), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(bats$totalAB), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">estimate_mode</span>(bats$totalAB), <span class="dt">color =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<p>In these severely right skewed distribution, we again see that the median and mean get pulled towards the tail of the distribution, with the mean getting pulled even further than the median.</p>
<p>Let's have a look at a quick summary figure of where the mean, median and mode lie with respect to each other in skewed distributions. As you can see, the mean always gets pulled the furthest to the tail of distributions.The reason for this is that the mean is much more affected by extreme outliers than the median. The median is simply the boundary which divides the top 50% of the data from the bottom 50% of the data. The mean has to include all values in its calculation, so can be largely affected by extreme values more so than the median.</p>
<div class="figure">
<img src="img/skew1.png" alt="Skewness" />
<p class="caption">Skewness</p>
</div>
</div>
<div id="kurtosis" class="section level4">
<h4><span class="header-section-number">7.1.5.2</span> Kurtosis</h4>
<p>blah blah blah</p>
<div class="figure">
<img src="img/kurtosis.png" />

</div>
<p>need to think what to put into this section</p>
<p>probably an image of different kurtoses of different distributions. don't need much more than that.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">kurtosis</span>(bats$totalAB)   <span class="co"># &gt;3 = &#39;peaky&#39; less in shoulders of tails</span></code></pre></div>
<pre><code>## [1] 5.325537</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">kurtosis</span>(bats$avg)   <span class="co"># &gt;3 = &#39;peaky&#39; less in shoulders of tails</span></code></pre></div>
<pre><code>## [1] 4.18612</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># kurtosis close to 3</span>

<span class="co"># Note with a smaller sample size, our distribution will</span>
<span class="co"># not be as normal, i.e. more skewed and not mesokurtic</span>

x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="fl">5.0</span>)
x1</code></pre></div>
<pre><code>##  [1] 101.45280  95.88441  94.74933 103.10539  99.47551  99.23445  98.87608
##  [8] 102.12954 100.12076  99.62868</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(x1)</code></pre></div>
<pre><code>## [1] 99.46569</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x1)</code></pre></div>
<pre><code>## [1] 2.586687</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">skewness</span>(x1)</code></pre></div>
<pre><code>## [1] -0.5130139</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">kurtosis</span>(x1)</code></pre></div>
<pre><code>## [1] 2.484758</code></pre>
</div>
</div>
</div>
<div id="z-scores" class="section level2">
<h2><span class="header-section-number">7.2</span> z-scores</h2>
<p>z-scores are a useful way of comparing different scores in different distributions. As an example, let's look at the two distributions below. On the left we have the population of all Airedale Terriers that is normally distributed with a mean of 60 lbs with a standard deviation of 6 lbs. On the right we have the population of all Scottish Terriers that is normally distributed with a mean of 20 lbs and a standard deviation of 0.4 lbs.</p>
<div class="figure">
<img src="img/z1.png" alt="terriers" />
<p class="caption">terriers</p>
</div>
<p>If you owned an Airedale Terrier that was 65 lbs and a Scottish Terrier that was 20.5 lbs, would you be able to say which one was relatively larger than their breed average? Both of them are above the mean of their breeds, but by how much? The Airedale Terrier is (65-60) 5 lbs heavier than the mean of the breed, whereas the Scottish Terrier is only (20.5-20) 0.5 lbs heavier than its breed average.</p>
<p>Looking at things in absolute terms however is misleading. It would be better if we could somehow standardize these differences. This is where z-scores come in. z-scores enable us to calcualte how far any datapoint is from the mean of its distribution by saying how many &quot;standard deviations&quot; away from the mean it is.</p>
<p>Let's look at the Airedale Terrier. Your Airedale is 5 lbs heavier than the mean of 60 lbs. This is a bit less than one standard deviation above the mean, as the standard deviation is 6 lbs. However, your Scottish Terrier is 0.5 lbs heavier than the mean of 20 lbs, which is a bit more than the standard deviation of 0.4 lbs for that breed. We can calculate precisely how many standard deviations away from the mean they are using z-scores. This is the formula:</p>
<p><span class="math inline">\(z=\frac{x - {\mu}_x}{\sigma_x}\)</span></p>
<p>Using this formula, let's calculate the z-scores for each of our dogs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Airedale</span>
(<span class="dv">65</span> -<span class="st"> </span><span class="dv">60</span>) /<span class="st"> </span><span class="dv">6</span></code></pre></div>
<pre><code>## [1] 0.8333333</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Scottish</span>
(<span class="fl">20.5</span> -<span class="st"> </span><span class="dv">20</span>) /<span class="st"> </span><span class="fl">0.4</span></code></pre></div>
<pre><code>## [1] 1.25</code></pre>
<p>The z-score for our 65 lb Airedale is <code>z=0.83</code>. The z-score for our 20.5 lb Scottish is <code>z=1.25</code>. This shows us that our Scottish Terrier is actual more standard deviations away from its breed mean than is our Airedale Terrier dog.</p>
<p>We could also plot each of these z-scores on top of the standard normal distribution. Remember, this is the specific case of the normal distribution where the mean of the distribution is 0 and the standard deviation is 1.</p>
<p>Shown below, we've plotted on the top row the breed population histograms with red vertical lines the weights of each of these dogs on their respective population histogram. On the bottom row we have these values converted to their z-scores and still shown with a red line. Each is overlaid on top of a standard normal distribution.</p>
<div class="figure">
<img src="img/z2.png" alt="z-scores" />
<p class="caption">z-scores</p>
</div>
<p>z-scores can be very useful ways of standardizing observed values into ways that we can directly compare across different distributions. If we calculate a negative z-score then it's clear that our observed value is below the population mean, and if we calculate a positive z-score then our value is greater than the population mean. The size of the z-score relates to how many population standard deviations from the mean each value is overall.</p>
<div id="z-scores-in-samples." class="section level3">
<h3><span class="header-section-number">7.2.1</span> z-scores in samples.</h3>
<p>Often we may not know the population mean or standard deviation. In such cases if all we have is a sample mean and a sample standard deviation, we can still calcualte z-scores for such samples. We effectively use the same formula to calculate the z-scores, just subsituting in the sample mean and standard deviation.</p>
<p><span class="math inline">\(z=\frac{x - {\mu}_x}{\sigma_x}\)</span></p>
<p>For instance, let's look at the following sample of ages for players on a village cricket team:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ages &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">23</span>, <span class="dv">19</span>, <span class="dv">21</span>, <span class="dv">33</span>, <span class="dv">51</span>, <span class="dv">40</span>, <span class="dv">16</span>, <span class="dv">15</span>, <span class="dv">61</span>, <span class="dv">55</span>, <span class="dv">30</span>, <span class="dv">28</span>)

<span class="kw">mean</span>(ages)</code></pre></div>
<pre><code>## [1] 32.66667</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(ages)</code></pre></div>
<pre><code>## [1] 15.74417</code></pre>
<p>These data clearly don't look normally distributed, but we still are able to calculate a mean and standard deviation. We can also still calculate the z-scores for each age:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span>(ages -<span class="st"> </span><span class="kw">mean</span>(ages)) /<span class="st"> </span><span class="kw">sd</span>(ages)

z</code></pre></div>
<pre><code>##  [1] -0.61398401 -0.86804636 -0.74101519  0.02117186  1.16445243  0.46578097
##  [7] -1.05859312 -1.12210871  1.79960831  1.41851478 -0.16937490 -0.29640607</code></pre>
<p>You'll notice that those individuals that have negative z-scores are younger than the mean age of 32.67. Those individuals with positive z-scores are older than the mean age of 32.67. The largest z-score in terms of magnitude (either in the positive or negative direction) is 1.8. This person was 61 and was 1.8 standard deviations older than the average age.</p>
<p>Although it's possible to calculate z-scores for any sample, if the sample data come from a normally distributed population then we can use this z-score principle to perform inferential statistics (see xxx.xxx)</p>
</div>
<div id="using-z-scores-to-determine-probabilities" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Using z-scores to determine probabilities</h3>
<p>One of the great things about the normal distribution, is that we can calculate quite straightforwardly what proportion of the distribution lies under the curve for any distance away from the mean measured in standard deviation.</p>
<p>With computers, this is a very trivial task to perform, as we'll see shortly. Prior to the computer age, these computations weren't as easy, so we'd often use something called the <strong>empirical rule</strong>. This is basically a framework that tell us what proportion of the distribution lies under the curve at 1<span class="math inline">\(\sigma\)</span>, 2<span class="math inline">\(\sigma\)</span>, 3<span class="math inline">\(\sigma\)</span>, etc. from the mean.</p>
<p>Let's look at the distribution below, which is normally distributed with a mean (<span class="math inline">\(\mu\)</span>) of 14 and a standard deviation (<span class="math inline">\(\sigma\)</span>) of 4.</p>
<p>The first thing to note is that a normal distribution is perfectly symmetrical, with equal area under the curve on either side of the mean. Therefore, in our example, 50% of the distribution lies below the mean of 14, and 50% of datapoints lie above the mean.</p>
<p>The area colored in green in the distribution represents the area of the distribution that lies <span class="math inline">\(\mu \pm1\sigma\)</span>. The area colored in pinky-purple lie between <span class="math inline">\(\mu+1\sigma\)</span> and <span class="math inline">\(\mu+2\sigma\)</span> or between <span class="math inline">\(\mu-1\sigma\)</span> and <span class="math inline">\(\mu-2\sigma\)</span>. The area colored in yellow lie between <span class="math inline">\(\mu+2\sigma\)</span> and <span class="math inline">\(\mu+3\sigma\)</span> or between <span class="math inline">\(\mu-2\sigma\)</span> and <span class="math inline">\(\mu-3\sigma\)</span>. The blue areas represent the proportion of the distribution that lies beyond <span class="math inline">\(\mu \pm4\sigma\)</span>.</p>
<div class="figure">
<img src="img/empirical.png" style="width:90.0%" />

</div>
<p>Rather than look at the proportion that lies between two boundaries, often instead we describe the proportion of the distribution that lies to the left of a certain value. The table below shows what proportion of the distribution lie to the left of each value. For instance, in the above distribution <span class="math inline">\(\mu+2\sigma=22\)</span>. According to the table below, we have 97.72% of the data/distribution that are to the left of <span class="math inline">\(x=22\)</span>.</p>
<div class="figure">
<img src="img/ztable.png" style="width:50.0%" />

</div>
<p>The above table and histogram are obviously useful guides for knowing what proportion of the data exist at certain breakpoints. But, what if you had a value of <span class="math inline">\(x=17.5\)</span> in the above distribution? What proportion of the data are below this value?</p>
<p>Well we can actually work this out if we convert our raw scores to z-scores. Once we have a z-score, we can calculate the area to the left of any point on the standard normal curve. Our value of <span class="math inline">\(x=17.5\)</span> has a z-score of 0.875, so it is 0.875 standard deviations above the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="fl">17.5</span> -<span class="st"> </span><span class="dv">14</span>) /<span class="dv">4</span></code></pre></div>
<pre><code>## [1] 0.875</code></pre>
<p>Let's look at that on the standard normal curve, with the value <span class="math inline">\(z = 0.875\)</span> represented by the red solid line. We can obtain the area in the distribution to the left of this value that is shaded in light red in R, using the function <code>pnorm()</code>.</p>
<div class="figure">
<img src="img/z3.png" />

</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="fl">0.875</span>)</code></pre></div>
<pre><code>## [1] 0.809213</code></pre>
<p>This shows us that 80.92% of the distribution lie to the left of <span class="math inline">\(z=0.875\)</span>. To get what proportion of the distribution lie to the right of this value, we just subtract it from 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pnorm</span>(<span class="fl">0.875</span>)</code></pre></div>
<pre><code>## [1] 0.190787</code></pre>
<p>Let's look at the proportions under the curve to the left of plus or minus 0, 1, 2, or 3 standard deviations from the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">zvals &lt;-<span class="st"> </span><span class="kw">c</span>(-<span class="dv">3</span>,-<span class="dv">2</span>,-<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)
<span class="kw">pnorm</span>(zvals)</code></pre></div>
<pre><code>## [1] 0.001349898 0.022750132 0.158655254 0.500000000 0.841344746 0.977249868
## [7] 0.998650102</code></pre>
<p>Hopefully you can see that these values mirror those in the table provided above.</p>
<div id="z-score-and-probability-problems." class="section level4">
<h4><span class="header-section-number">7.2.2.1</span> z-score and probability problems.</h4>
<p>Let's take this a little further with some small examples.</p>
<p><strong>Example 1</strong></p>
<p>Let's assume that the weights of pineapples are normally distributed with a mean of 1003.5g and a standard deviation of 35g. You bought a random pineapple and it turned out to only be 940g. What proportion of pineapples are less than 940g? How unlucky did you get to buy such a small pineapple?</p>
<p>First, let's take a look at the population distribution of pineapples with <span class="math inline">\(\mu=1003.5\)</span> and <span class="math inline">\(\sigma=35\)</span>. Our pineapple is 940g and is shown with the solid red line below. As our distribution is normal, if we convert this to a z-score we can compare it to where z is in the standard normal distribution on the right.</p>
<div class="figure">
<img src="img/z4.png" />

</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">940</span> -<span class="st"> </span><span class="fl">1003.5</span>) /<span class="st"> </span><span class="dv">35</span></code></pre></div>
<pre><code>## [1] -1.814286</code></pre>
<p>So we calculated that <span class="math inline">\(z = -1.81\)</span>, and we visualize that on our standard normal distribution. We're interested in what proportion of pineapples from the distribution are 940g or less. That is the light red shaded area. To calculate that we can just use <code>pnorm()</code> in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(-<span class="fl">1.81</span>)</code></pre></div>
<pre><code>## [1] 0.03514789</code></pre>
<p>From this we can see that only 3.5% of pineapples are less than 940g. We got super unlucky to get such a tiny pineapple.</p>
<p><strong>Example 2</strong></p>
<p>What is the probability of getting a pineapple of greater than 1050g ?</p>
<p>To answer this we first get the z-score for a pineapple of 1050g, and find that <span class="math inline">\(z = 1.33\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">1050</span> -<span class="st"> </span><span class="fl">1003.5</span>) /<span class="st"> </span><span class="dv">35</span></code></pre></div>
<pre><code>## [1] 1.328571</code></pre>
<p>Next we recognize that if we're interested in what proportion of pineapples weigh more than 1050g, we need to know what proportion of the standard normal curve is greater than <span class="math inline">\(z = 1.33\)</span> (the shaded light red area below).</p>
<div class="figure">
<img src="img/z5.png" />

</div>
<p>We can calculate that by using <code>pnorm()</code> to figure out what proportion is to the left of <span class="math inline">\(z = 1.33\)</span>, and then subtract that from 1 to get what proportion is to the right.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pnorm</span>(<span class="fl">1.33</span>)   <span class="co">#0.003  (so 9.18% chance)</span></code></pre></div>
<pre><code>## [1] 0.09175914</code></pre>
<p><strong>Example 3</strong></p>
<p>What is the probability of getting a pineapple between 950g and 1045g ?</p>
<p>For this question, we're interested in the shaded light red area between <span class="math inline">\(z = x\)</span> and <span class="math inline">\(z = z\)</span> on the standard normal curve. Why these z-values? Because these are the z scores you get if you convert a 950g and a 1045g pineapple to z scores.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z1 &lt;-<span class="st"> </span>(<span class="dv">950</span> -<span class="st"> </span><span class="fl">1003.5</span>)  /<span class="st"> </span><span class="dv">35</span>
z2 &lt;-<span class="st"> </span>(<span class="dv">1045</span> -<span class="st"> </span><span class="fl">1003.5</span>)  /<span class="st"> </span><span class="dv">35</span>

z1 <span class="co">#-1.53</span></code></pre></div>
<pre><code>## [1] -1.528571</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z2 <span class="co">#1.19</span></code></pre></div>
<pre><code>## [1] 1.185714</code></pre>
<div class="figure">
<img src="img/z6.png" />

</div>
<p>In R we can calculate the proportion to the left of each of these z-scores using <code>pnorm()</code>. What we need is the shaded area, which we can get if we subtract the area to the left of <span class="math inline">\(z = -1.53\)</span> from the area to the left of <span class="math inline">\(z = 1.19\)</span>. We do it like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## get proportion of curve to left of each z-score

<span class="kw">pnorm</span>(z1) <span class="co"># 0.06</span></code></pre></div>
<pre><code>## [1] 0.06318536</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(z2) <span class="co"># 0.88</span></code></pre></div>
<pre><code>## [1] 0.8821324</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># so the area between them is:</span>

<span class="kw">pnorm</span>(z2) -<span class="st"> </span><span class="kw">pnorm</span>(z1)  <span class="co">#0.82</span></code></pre></div>
<pre><code>## [1] 0.8189471</code></pre>
<p>So, 82% of the distribution lie between 950g and 1040g.</p>
</div>
</div>
</div>
<div id="what-is-a-sampling-distribution" class="section level2">
<h2><span class="header-section-number">7.3</span> What is a Sampling Distribution ?</h2>
<p>Another type of distribution that we will discuss a lot! is the sampling distribution. There are different types of sampling distributions, so for now we'll focus on the <strong>sampling distribution of the sample means</strong>. The best way to illustrate a sampling distribution, is to show it by example.</p>
<p>Say we have a population of 1 million adult Archerfish. The population mean <span class="math inline">\(\mu\)</span> is 100.0mm, and the population standard deviation <span class="math inline">\(\sigma\)</span> is equal to 15.0mm.</p>
<p>Let's create this population:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3</span>)

archerfish &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1000000</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="dv">15</span>)

<span class="kw">mean</span>(archerfish)</code></pre></div>
<pre><code>## [1] 100.0061</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(archerfish)</code></pre></div>
<pre><code>## [1] 15.02418</code></pre>
<p>Let's also plot what this normally distrubted population looks like by making a histogram:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># histogram of the population:</span>

<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(archerfish), <span class="kw">aes</span>(<span class="dt">x =</span> archerfish)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#f584ed&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">binwidth =</span><span class="dv">2</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Body Length mm&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Frequency&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Body Sizes of 1 million Archerfish&quot;</span>) +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">100.0</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">color=</span><span class="st">&#39;black&#39;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-119-1.png" width="672" /></p>
<p>OK, let's say that you didn't have the time to go out and catch 1 million Archerfish and measure the body length of every single fish. What shoudld you do? One thing you might decide is to just go and take a random sample of 10 anglerfish (you could have picked another sample size - let's just stick with 10 for now). Once you have your sample of 10 archerfish, you could then measure them and you will be able to calculate the sample mean of that sample.</p>
<p>We'll use <code>sample()</code> to randomly select 10 archerfish.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>) <span class="co"># so we all get the same sample.</span>
samp1 &lt;-<span class="st"> </span><span class="kw">sample</span>(archerfish, <span class="dv">10</span>, <span class="dt">replace =</span> T)  

samp1</code></pre></div>
<pre><code>##  [1]  83.93889 103.18896  77.19206 104.34995  98.54528 111.00236  85.95220
##  [8]  94.26505  99.79486  97.50949</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp1)</code></pre></div>
<pre><code>## [1] 95.57391</code></pre>
<p>Our sample mean is 95.6 - that's fairly close to the actual population mean of 100.0mm. Let's grab another three samples of 10 archerfish and see what the sample means are of those sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(archerfish, <span class="dv">10</span>, <span class="dt">replace =</span> T))  <span class="co"># mean of our 2nd sample</span></code></pre></div>
<pre><code>## [1] 98.35256</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(archerfish, <span class="dv">10</span>, <span class="dt">replace =</span> T))  <span class="co"># mean of our 3rd sample</span></code></pre></div>
<pre><code>## [1] 100.4263</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(archerfish, <span class="dv">10</span>, <span class="dt">replace =</span> T))  <span class="co"># mean of our 4th sample</span></code></pre></div>
<pre><code>## [1] 99.98449</code></pre>
<p>These next three samples are closer to 100.0, with some being a little bit less than the population mean, and the other being a bit more.</p>
<p>What would happen if we collected thousands and thousands of samples of size 10? Let's do it - you don't need to follow the exact code here of how we're doing this, but essentially we're grabbing 10,000 samples of size 10. From each of these we're getting the sample mean. That means we'll end up with 10,000 sample means. We're storing those results in the object called <code>res</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### What if we were to collect 10,000 samples and for each one get the mean

results&lt;-<span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>,<span class="dv">10000</span>)
for(i in <span class="dv">1</span>:<span class="dv">10000</span>){
results[[i]]  &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(archerfish, <span class="dv">10</span>, <span class="dt">replace =</span> T))  
}

res &lt;-<span class="st"> </span><span class="kw">unlist</span>(results)</code></pre></div>
<p>Now we have our 10,000 sample means we could make a histogram of these sample means.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">psd &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(res), <span class="kw">aes</span>(<span class="dt">x =</span> res)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#4adbe0&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">binwidth =</span> <span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Mean Body Length of each sample - mm&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Frequency&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Sample Means for n=10&quot;</span>) +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(res), <span class="dt">lwd=</span><span class="dv">1</span>)

psd</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-123-1.png" width="672" /></p>
<p>This distribution that we've just plotted is the <strong>sampling distribution of sample means for samples of size n=10</strong>. We picked 10,000 as the number of samples of size 10 to collect as it's reasonably large enough to get enough sample means that we can see the shape of the distribution. We could have picked 100,000 samples to collect, or 1,000,000... in fact the more the better, but 10,000 is enough to get the point across.</p>
<p>Out of all of these sample means that we &quot;collected&quot;, what is the average across all of the 10,000 samples? - and what's more, what is the standard deviation of that distribution?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(res)</code></pre></div>
<pre><code>## [1] 100.0255</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(res) <span class="co">#standard deviation of the sampling distribution, aka standard error</span></code></pre></div>
<pre><code>## [1] 4.742526</code></pre>
<p>Let's first focus on the mean. Our mean of sample means was 100.0, which is the same as the mean of 1,000,000 archerfish in the population! It turns out that the mean of the sampling distribution is approximately equal to the population mean. By the way, the notation that we use to depcit the mean of the sampling distribution of sample means is <span class="math inline">\(\mu_{\overline{x}}\)</span>. So <span class="math inline">\(\mu_{\overline{x}} = 100.0\)</span>.</p>
<p>What is the standard deviation of this distribution (the sampling distribution of the sample means)? We just calculated it in R to be 4.74. The notation we use for this is <span class="math inline">\(\sigma_{\overline{x}} = 4.74\)</span>. It's called the standard deviation of the sampling distribution of sample means, but for short it gets called the <strong>standard error</strong>.</p>
<p>Of course, we never in reality actually collect thousands and thousands of samples - that defeats the point of sampling. If we had time to collect thousands and thousands of samples, we may as well just measure every archerfish in the population. Usually, we just collect one sample. In later sections, we'll discuss how you can estimate what <span class="math inline">\(\mu_{\overline{x}}\)</span> and <span class="math inline">\(\sigma_{\overline{x}}\)</span> are when you have only collected one sample.</p>
<p>However, given we already know the population standard deviation of the 1 million archerfish is <span class="math inline">\(\sigma=15\)</span>, we can calculate the standard deviation of the sampling distribution of sample means for any sample size. It is calculated using the formula:</p>
<p><span class="math inline">\(\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span></p>
<p>So in our case it should be equal to:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">15</span>/<span class="kw">sqrt</span>(<span class="dv">10</span>) <span class="co"># 4.74</span></code></pre></div>
<pre><code>## [1] 4.743416</code></pre>
<p>As you can see this is the same value that we got in our simulated sampling distribution above.</p>
<div id="sample-size-and-the-sampling-distribution" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Sample Size and the Sampling Distribution</h3>
<p>In the previous section we looked at what would happen if we took samples of size 10 from our archerfish population and looked at the sample means of each sample. We found that when we made a histogram out of these sample means, that the mean of the sample means <span class="math inline">\(\mu_{\overline{x}}\)</span> was approximately equal to the population mean of 100. The standard deviation of the sampling distribution of sample means, or standard error, <span class="math inline">\(\sigma_{\overline{x}}\)</span> was equal to 4.74.</p>
<p>What happens if we were to take a different sized sample each time - say size 50? What would be the mean and standard deviation of this distribution of sample means? Let's find out. We'll again take 1000 samples of size 50 at random from our population of 1 million archerfish. For each sample we'll record the sample mean length of the 50 fish. We'll end up saving these 10,000 sample means in an R object called <code>res1</code>. Again, don't worry about how the code is doing it here, just as long as you follow what we're doing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ok, get 10,000 samples of size 50</span>
results1&lt;-<span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>,<span class="dv">10000</span>)
for(i in <span class="dv">1</span>:<span class="dv">10000</span>){
  results1[[i]]  &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(archerfish, <span class="dv">50</span>, <span class="dt">replace =</span> T))
}

res1 &lt;-<span class="st"> </span><span class="kw">unlist</span>(results1)</code></pre></div>
<p>Now we have our 10,000 samples, let's plot the histogram</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">psd2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(res1), <span class="kw">aes</span>(<span class="dt">x =</span> res1)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#4adbe0&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">binwidth =</span> .<span class="dv">5</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Mean Body Length of each sample - mm&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Frequency&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Sample Means for n=50&quot;</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(res1),<span class="dt">lwd=</span><span class="dv">1</span>)

psd2</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-127-1.png" width="672" /></p>
<p>If we look at the mean and standard deviation of this sampling distribution, we get the following values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(res1)  <span class="co"># still same as population mean</span></code></pre></div>
<pre><code>## [1] 100.0114</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(res1)    <span class="co"># smaller with larger sample size</span></code></pre></div>
<pre><code>## [1] 2.116454</code></pre>
<p>The mean of the sampling distribution of sample means <span class="math inline">\(\mu_{\overline{x}}\)</span> is still a very good estimate of the population mean - in fact, it's a tiny, tiny bit better than when we had samples of size 10.</p>
<p>The biggest difference is in the standard deviation of the sampling distribution, <span class="math inline">\(\sigma_{\overline{x}}\)</span> (the standard error), which is much lower than when we had samples of size 10. The reason for this is that the variability in our sample means is much more reduced when we have samples of size 50. On average, our sample means are much closer individual estimates to the population mean. This is what drastically reduces the standard deviation of this sampling distribution.</p>
<p>We could have calculated the standard error directly using the formula provided earlier, because we already know the population standard deviation. When we use that formala we get 2.12, essentially the same standard deviation as we got with our simulated sampling distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">15</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">50</span>)</code></pre></div>
<pre><code>## [1] 2.12132</code></pre>
<p>Let's directly compare the two sampling distributions for the two different sample sizes. We adjusted the x-axis so it is the same for both figures, so you can see the change in the standard deviation between the two sample sizes:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-130-1.png" width="672" /></p>
</div>
</div>
<div id="central-limit-theorem." class="section level2">
<h2><span class="header-section-number">7.4</span> Central Limit Theorem.</h2>
<p>In the previous section we discovered that if you take many, many samples from a normally distributed population and calculated the sample mean of each sample, that you would get a sampling distribution of sample means. We also saw that that sampling distribution was normally distributed with a mean <span class="math inline">\(\mu_{\overline{x}}\)</span> that was approximately equal to the population mean, and a standard deviation <span class="math inline">\(\sigma_{\overline{x}}\)</span> that was equal to the population standard deviation divided by the square root of n <span class="math inline">\(\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span>.</p>
<p>Is it just coincidence that both the population distribution and the sampling distribution was approximately normally distributed? What we will learn in this chapter is that it does not matter at all what the population distribution is - if we take thousands of samples from <strong>any</strong> shaped distribution and calculate the sample mean of each sample, when we create the histogram of those sample means we will find that they are approximately normally distributed. This is what we refer to as the <strong>central limit theorem</strong>. Further, the larger the sample size that we take, the closer to a normal distribution the sampling distribution becomes.</p>
<p>Let's look at this by taking samples from various different population distributions.</p>
<p><strong>Uniform Distribution</strong></p>
<p>First we'll look at a uniform distribution of 1 million numbers between 0 and 75. You might like to think of this as the distance of trees from the center of a forest in km. Let's graph the distribution and calculate the mean and standard deviation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co">#get data from uniform distribution</span>
x1 &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000000</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">75</span>)

<span class="co"># histogram</span>
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(x1), <span class="kw">aes</span>(<span class="dt">x =</span> x1)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, 
                 <span class="dt">fill =</span> <span class="st">&quot;#894ae0&quot;</span>, 
                 <span class="dt">alpha=</span>.<span class="dv">3</span>, 
                 <span class="dt">binwidth =</span> <span class="dv">5</span>,
                 <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Distance from Center of Forest km&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-131-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### Population Mean &amp; SD
<span class="kw">mean</span>(x1)  <span class="co">#37.5</span></code></pre></div>
<pre><code>## [1] 37.49417</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x1) <span class="co">#21.6</span></code></pre></div>
<pre><code>## [1] 21.6472</code></pre>
<p>We can see that the mean of this population is 37.5, and the population standard deviation is 37.5.</p>
<p>Let's take samples of size 30 at random from this population of 1 million. For each sample, we'll calculate the sample mean. We'll take 10,000 samples (again - we could have picked any really large number here, but 10,000 seems reasonable enough to prove our point). After we get our 10,000 sample means from samples of size 30, we'll plot the histogram of those sample means.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Let&#39;s get 10,000 samples of size 30

results&lt;-<span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>,<span class="dv">10000</span>)
for(i in <span class="dv">1</span>:<span class="dv">10000</span>){
  results[[i]]  &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(x1, <span class="dv">30</span>, <span class="dt">replace =</span> T))  
}

res &lt;-<span class="st"> </span><span class="kw">unlist</span>(results)


<span class="co"># This is the sampling distribution.</span>
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(res), <span class="kw">aes</span>(<span class="dt">x =</span> res)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#894ae0&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">binwidth =</span> <span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(res), <span class="dt">lwd=</span><span class="dv">1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Mean of each sample&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Frequency&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Sample Means for n=30&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-132-1.png" width="672" /></p>
<p>This histogram represents our sampling distribution of sample means when we took samples of size 30 from a uniform distribution. Hopefully you notice that it is approximately normally distributed - even though the original population was uniformally distributed! Let's calculate the mean and standard deviation of this sampling distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(res)  <span class="co"># the mean of the sample means is close to 37.5, the population mean</span></code></pre></div>
<pre><code>## [1] 37.49964</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(res)  <span class="co">#3.9 - this is a lot smaller than the population SD</span></code></pre></div>
<pre><code>## [1] 3.995888</code></pre>
<p>The mean of our sampling distribution <span class="math inline">\(\mu_{\overline{x}} = 37.5\)</span> which is again approximately equal to the population mean. The sampling distribution standard deviation <span class="math inline">\(\sigma_{\overline{x}} = 3.94\)</span> which is a lot lower than the original population standard deviation. Because we only took 10,000 samples, these values aren't exact, but we could have calculated the standard error by taking the population standard deviation and dividing by the square root of n <span class="math inline">\(\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x1)/<span class="kw">sqrt</span>(<span class="dv">30</span>)  <span class="co"># standard error.</span></code></pre></div>
<pre><code>## [1] 3.952219</code></pre>
<p><strong>Skewed Distributions</strong></p>
<p>We can see that the central limit theorem holds true for skewed distributions also. Here, we have a population of 1 million charity donations. Let's draw a hisogram of the population distribution and calculate the mean and standard deviation of the population.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
<span class="co"># e.g. Donations to a charity (in $s)</span>

q &lt;-<span class="st"> </span><span class="kw">rnbinom</span>(<span class="dv">1000000</span>, <span class="dv">5</span>, .<span class="dv">4</span>)

<span class="co"># histogram</span>
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(q), <span class="kw">aes</span>(<span class="dt">x =</span> q)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, 
                 <span class="dt">fill =</span> <span class="st">&quot;#1ad665&quot;</span>, 
                 <span class="dt">alpha=</span>.<span class="dv">3</span>, 
                 <span class="dt">binwidth =</span> <span class="dv">1</span>,
                 <span class="dt">boundary =</span> <span class="dv">0</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Charity Donation in $&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-135-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### Population Mean
<span class="kw">mean</span>(q)  <span class="co">#7.5</span></code></pre></div>
<pre><code>## [1] 7.504368</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(q)  <span class="co">#4.33</span></code></pre></div>
<pre><code>## [1] 4.331767</code></pre>
<p>It is clear that this population distribution is highly positively skewed. The mean of the population is 7.5 and the standard deviation is 4.33.</p>
<p>In the following code we takes samples of size 30 and calculate the sample mean of each sample. This time, we'll collect 50,000 samples, just to be a bit different.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results&lt;-<span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>,<span class="dv">50000</span>)
for(i in <span class="dv">1</span>:<span class="dv">10000</span>){
  results[[i]]  &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(q, <span class="dv">30</span>, <span class="dt">replace =</span> T))  
}

res &lt;-<span class="st"> </span><span class="kw">unlist</span>(results)


### Let&#39;s Draw this as a histogram.

<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(res), <span class="kw">aes</span>(<span class="dt">x =</span> res)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;#31e8d0&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">binwidth =</span> .<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(res), <span class="dt">lwd=</span><span class="dv">1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Mean of each sample&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Frequency&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Sample Means for n=30&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-136-1.png" width="672" /></p>
<p>So, it happened again. This is our sampling distribution of sample means, collected from samples of size 30 from a highly skewed distribution. But once again, the sampling distribution is approximately normally distributed. It might not be perfectly normally distributed, but it is close to being normal. We can calculate the mean and the standard deviation of this sampling distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(res)  <span class="co"># the mean of the sample means is close to 7.5</span></code></pre></div>
<pre><code>## [1] 7.504313</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(res)  <span class="co">#0.79</span></code></pre></div>
<pre><code>## [1] 0.7981555</code></pre>
<p><span class="math inline">\(\mu_{\overline{x}} = 7.5\)</span> which is once again approximately equal to the original population mean. The sampling distribution standard deviation <span class="math inline">\(\sigma_{\overline{x}} = 0.79\)</span>. We could have directly calculated that using the formula for the standard error, n <span class="math inline">\(\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span>, as we know the original population standard deviation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(q)/<span class="kw">sqrt</span>(<span class="dv">30</span>)  <span class="co"># standard error = 0.79</span></code></pre></div>
<pre><code>## [1] 0.7908688</code></pre>
<p>We could keep going with even more types of population distributions. We would find the same thing over and over again. If we take many samples from each population and calculated the sample means of all samples, they would form an approximately normally distribution. This will be especially true for larger samples. This is the basis of the central limit theorem. In the following sections we'll learn more about what we can do with these sampling distributions.</p>
</div>
<div id="sampling-distribution-problems." class="section level2">
<h2><span class="header-section-number">7.5</span> Sampling distribution problems.</h2>
<p>We can use our knowledge of sampling distributions and z-scores to determine how likely or unlikely we are to observe any one particular sample mean.</p>
<p>Let's use an example to illustrate this.</p>
<p style="color:blue">
Q. Say the weight of chicken eggs is normally distributed with mean 60g and standard deviation of 3g. What is the probability of getting a batch of a dozen eggs that have a mean of less than 58g ?
</p>
<p>The way to think about these questions is to recognize that we're dealing with a sampling distribution. We're really being asked what proportion of the sampling distribution is less than a sample mean of 58g, when your sample size is 12 (a dozen).</p>
<p>First, let's plot the population of chicken eggs and then the sampling distribution of samples means for a sample size of 12.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>) <span class="co"># so you get the same values as my script</span>


### First, I&#39;ll make some plots of the &#39;population&#39; and &#39;sampling distribution&#39;


## Population
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000000</span>, <span class="dt">mean =</span> <span class="dv">60</span>, <span class="dt">sd =</span> <span class="dv">3</span>)

p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(x), <span class="kw">aes</span>(<span class="dt">x =</span> x)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;mistyrose&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">60</span>, <span class="dt">lwd=</span><span class="dv">1</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Population of chicken eggs&quot;</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Weight&quot;</span>)


## Sampling Distribution of sample means with Sample size of 12 (a dozen).

results&lt;-<span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>,<span class="dv">10000</span>)
for(i in <span class="dv">1</span>:<span class="dv">10000</span>){
  results[[i]] &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(x, <span class="dv">12</span>, <span class="dt">replace =</span> T))
}

res &lt;-<span class="st"> </span><span class="kw">unlist</span>(results)

p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(res), <span class="kw">aes</span>(<span class="dt">x=</span>res)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&quot;black&quot;</span>, <span class="dt">fill=</span><span class="st">&#39;lightseagreen&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(res),<span class="dt">lwd=</span><span class="dv">1</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Sample Means&quot;</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;sample mean&quot;</span>)

<span class="kw">library</span>(gridExtra)
<span class="kw">grid.arrange</span>(p1,p2,<span class="dt">nrow=</span><span class="dv">1</span>)</code></pre></div>
<p>What are the mean and standard deviation of this sampling distribution of sample means? Well, <span class="math inline">\(\mu_{\overline{x}} = 60\)</span> because that's the population mean, and we know the mean of the sample means is approximately the same. We know we can calculate <span class="math inline">\(\sigma_{\overline{x}\)</span> because we know the population standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>Again, the formula is <span class="math inline">\(\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span>. Therefore, the standard deviation of the sampling distribution is <span class="math inline">\(\sigma_{\overline{x} = 0.87\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sem &lt;-<span class="st"> </span><span class="dv">3</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">12</span>)

sem</code></pre></div>
<pre><code>## [1] 0.8660254</code></pre>
<p>Now, we're interested in a sample of 12 that has a sample mean of 58g. Let's visualize what that looks like on the histogram of the sampling distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p2 +<span class="st"> </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">58</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Sample Means </span><span class="ch">\n</span><span class="st"> for sample size = 12&quot;</span>)</code></pre></div>
<p>Because we know the mean and standard deviation of this distribution, we can actually calculate the sample mean of 58g as a z-score. Doing this we find that <span class="math inline">\(z = -2.31\)</span>, which means that a sample mean of 58g is 2.31 standard deviations below the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># so, 58g as a z-score is...</span>

z &lt;-<span class="st"> </span>(<span class="dv">58</span> -<span class="st"> </span><span class="dv">60</span>) /<span class="st"> </span>sem  <span class="co"># -2.31</span>

z</code></pre></div>
<pre><code>## [1] -2.309401</code></pre>
<p>The original question asked what probability there was of getting a sample mean of less than 58g. This is basically what area is under the curve of the above sampling distribution to the left of the 58g sample mean. Because we converted that sample mean of 58g to a z-score of -2.31, we can look at that value on a standard normal curve. The area we are iterested in is the filled in area to the left of z = -2.31:</p>
<div class="figure">
<img src="img/chicken.png" />

</div>
<p>We can look up this value in R, using the function <code>pnorm</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(z)  <span class="co"># prob = 0.010</span></code></pre></div>
<pre><code>## [1] 0.01046067</code></pre>
<p>So we can see that the probability of getting a sample mean of lower than 58g is <code>p=0.0105</code>.</p>
</div>
<div id="the-t-distribution" class="section level2">
<h2><span class="header-section-number">7.6</span> The t-distribution</h2>
<p>Another distribution that is important to know about is the t-distribution. Like the normal distribution, this is a symmetrical distribution, but it has slighly fatter tails than the normal distribution.</p>
<p>The t-distribution comes up most commonly in sampling distributions. In particular, although the central limit theorem prescribes that sampling distributions are approximately normal, it is known that often they aren't approximately normal enough, and instead they follow a t-distribution shape.</p>
<p>An important detail about the t-distribution is that there are actually several t-distributions. There are different t-distributions for different degrees of freedom. These differ slightly in how heavy the tails of the distribution are. The degrees of freedom are usually related to the sample size minus one or two (depending upon the test being employed - see sections xxx.xxx and xxx.xxx). The higher the degrees of freedom, the more closely the t-distribution looks like a normal distribution. You can see this in the image below. The standard normal distribution is in red, and the t-distribution is in black. Each panel shows a different t-distribution. As the degrees of freedom increase, the t-distribution essentially becomes the normal distribution. At lower degrees of freedom, there is a lot more difference between the t-distribution and the normal distribution in the tails.</p>
<div class="figure">
<img src="img/t.png" />

</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="descriptives.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="confidence-intervals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
