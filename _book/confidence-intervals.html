<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Confidence Intervals | PSY317L Guides</title>
  <meta name="description" content="8 Confidence Intervals | PSY317L Guides" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Confidence Intervals | PSY317L Guides" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Confidence Intervals | PSY317L Guides" />
  
  
  

<meta name="author" content="James P. Curley &amp; Tyler M. Milewski" />


<meta name="date" content="2020-06-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="distributions.html"/>
<link rel="next" href="one-sample-inferential-statistics.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome to PSY317!</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-this-book-includes-and-what-it-doesnt"><i class="fa fa-check"></i><b>1.1</b> What this book includes and what it doesn't</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#how-to-use-this-guide"><i class="fa fa-check"></i><b>1.2</b> How to use this guide</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.4</b> References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r"><i class="fa fa-check"></i><b>1.5</b> Other places to find help about R</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r-and-statistics"><i class="fa fa-check"></i><b>1.6</b> Other places to find help about R and Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#downloading-r"><i class="fa fa-check"></i><b>2.1</b> Downloading R</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#downloading-rstudio"><i class="fa fa-check"></i><b>2.2</b> Downloading RStudio</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#using-rcloud-instead-of-rstudio"><i class="fa fa-check"></i><b>2.3</b> Using RCloud instead of RStudio</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#the-rstudio-environment"><i class="fa fa-check"></i><b>2.5</b> The RStudio Environment</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#the-command-prompt"><i class="fa fa-check"></i><b>2.6</b> The Command Prompt</a></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#what-is-an-rscript-file"><i class="fa fa-check"></i><b>2.7</b> What is an RScript File?</a></li>
<li class="chapter" data-level="2.8" data-path="introduction.html"><a href="introduction.html#what-are-packages"><i class="fa fa-check"></i><b>2.8</b> What are Packages</a></li>
<li class="chapter" data-level="2.9" data-path="introduction.html"><a href="introduction.html#project-folders-and-working-directories"><i class="fa fa-check"></i><b>2.9</b> Project Folders and Working Directories</a></li>
<li class="chapter" data-level="2.10" data-path="introduction.html"><a href="introduction.html#where-to-get-help-for-r-stuff"><i class="fa fa-check"></i><b>2.10</b> Where to Get Help for R stuff</a></li>
<li class="chapter" data-level="2.11" data-path="introduction.html"><a href="introduction.html#quitting-r"><i class="fa fa-check"></i><b>2.11</b> Quitting R</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic-syntax.html"><a href="basic-syntax.html"><i class="fa fa-check"></i><b>3</b> Basic Syntax</a><ul>
<li class="chapter" data-level="3.1" data-path="basic-syntax.html"><a href="basic-syntax.html#boring-mathematical-stuff"><i class="fa fa-check"></i><b>3.1</b> Boring mathematical stuff</a></li>
<li class="chapter" data-level="3.2" data-path="basic-syntax.html"><a href="basic-syntax.html#assignment"><i class="fa fa-check"></i><b>3.2</b> assignment</a></li>
<li class="chapter" data-level="3.3" data-path="basic-syntax.html"><a href="basic-syntax.html#vectors"><i class="fa fa-check"></i><b>3.3</b> vectors</a></li>
<li class="chapter" data-level="3.4" data-path="basic-syntax.html"><a href="basic-syntax.html#characters"><i class="fa fa-check"></i><b>3.4</b> Characters</a></li>
<li class="chapter" data-level="3.5" data-path="basic-syntax.html"><a href="basic-syntax.html#naming-of-objects"><i class="fa fa-check"></i><b>3.5</b> Naming of objects</a></li>
<li class="chapter" data-level="3.6" data-path="basic-syntax.html"><a href="basic-syntax.html#logical-operators"><i class="fa fa-check"></i><b>3.6</b> Logical Operators</a></li>
<li class="chapter" data-level="3.7" data-path="basic-syntax.html"><a href="basic-syntax.html#some-things-that-are-useful-to-know."><i class="fa fa-check"></i><b>3.7</b> Some things that are useful to know.</a><ul>
<li class="chapter" data-level="3.7.1" data-path="basic-syntax.html"><a href="basic-syntax.html#tab-is-your-friend"><i class="fa fa-check"></i><b>3.7.1</b> Tab is your friend</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="basic-syntax.html"><a href="basic-syntax.html#error-messages"><i class="fa fa-check"></i><b>3.8</b> Error Messages</a></li>
<li class="chapter" data-level="3.9" data-path="basic-syntax.html"><a href="basic-syntax.html#functions"><i class="fa fa-check"></i><b>3.9</b> Functions</a></li>
<li class="chapter" data-level="3.10" data-path="basic-syntax.html"><a href="basic-syntax.html#chaining-syntax"><i class="fa fa-check"></i><b>3.10</b> Chaining Syntax</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Introduction to Data Carpentry</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#introduction-to-dataframes"><i class="fa fa-check"></i><b>4.1</b> Introduction to Dataframes</a></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#tidyverse"><i class="fa fa-check"></i><b>4.2</b> tidyverse</a></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#filter"><i class="fa fa-check"></i><b>4.3</b> filter()</a></li>
<li class="chapter" data-level="4.4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#select"><i class="fa fa-check"></i><b>4.4</b> select()</a></li>
<li class="chapter" data-level="4.5" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#mutate"><i class="fa fa-check"></i><b>4.5</b> mutate()</a></li>
<li class="chapter" data-level="4.6" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#arrange"><i class="fa fa-check"></i><b>4.6</b> arrange()</a></li>
<li class="chapter" data-level="4.7" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#wide-vs-long-data"><i class="fa fa-check"></i><b>4.7</b> Wide vs Long Data</a></li>
<li class="chapter" data-level="4.8" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#joins"><i class="fa fa-check"></i><b>4.8</b> Joins</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>5</b> Data Visualization</a><ul>
<li class="chapter" data-level="5.1" data-path="data-visualization.html"><a href="data-visualization.html#intro-to-ggplot2"><i class="fa fa-check"></i><b>5.1</b> Intro to ggplot2</a></li>
<li class="chapter" data-level="5.2" data-path="data-visualization.html"><a href="data-visualization.html#histogram"><i class="fa fa-check"></i><b>5.2</b> Histogram</a></li>
<li class="chapter" data-level="5.3" data-path="data-visualization.html"><a href="data-visualization.html#scatter"><i class="fa fa-check"></i><b>5.3</b> Scatter</a></li>
<li class="chapter" data-level="5.4" data-path="data-visualization.html"><a href="data-visualization.html#line"><i class="fa fa-check"></i><b>5.4</b> Line</a></li>
<li class="chapter" data-level="5.5" data-path="data-visualization.html"><a href="data-visualization.html#boxplot"><i class="fa fa-check"></i><b>5.5</b> Boxplot</a></li>
<li class="chapter" data-level="5.6" data-path="data-visualization.html"><a href="data-visualization.html#bar-graphs"><i class="fa fa-check"></i><b>5.6</b> Bar Graphs</a></li>
<li class="chapter" data-level="5.7" data-path="data-visualization.html"><a href="data-visualization.html#all-the-extras"><i class="fa fa-check"></i><b>5.7</b> All the extras</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>6</b> Descriptives</a><ul>
<li class="chapter" data-level="6.1" data-path="descriptives.html"><a href="descriptives.html#sample-vs-population"><i class="fa fa-check"></i><b>6.1</b> Sample vs Population</a></li>
<li class="chapter" data-level="6.2" data-path="descriptives.html"><a href="descriptives.html#basic-descriptives"><i class="fa fa-check"></i><b>6.2</b> Basic Descriptives</a></li>
<li class="chapter" data-level="6.3" data-path="descriptives.html"><a href="descriptives.html#mean-median-and-mode"><i class="fa fa-check"></i><b>6.3</b> Mean, Median, and Mode</a></li>
<li class="chapter" data-level="6.4" data-path="descriptives.html"><a href="descriptives.html#standard-deviation"><i class="fa fa-check"></i><b>6.4</b> Standard Deviation</a><ul>
<li class="chapter" data-level="6.4.1" data-path="descriptives.html"><a href="descriptives.html#average-deviation"><i class="fa fa-check"></i><b>6.4.1</b> Average Deviation</a></li>
<li class="chapter" data-level="6.4.2" data-path="descriptives.html"><a href="descriptives.html#standard-deviation-1"><i class="fa fa-check"></i><b>6.4.2</b> Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="descriptives.html"><a href="descriptives.html#standard-error"><i class="fa fa-check"></i><b>6.5</b> Standard Error</a></li>
<li class="chapter" data-level="6.6" data-path="descriptives.html"><a href="descriptives.html#median-and-inter-quartile-ranges"><i class="fa fa-check"></i><b>6.6</b> Median and Inter-quartile Ranges</a><ul>
<li class="chapter" data-level="6.6.1" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>6.6.1</b> Median</a></li>
<li class="chapter" data-level="6.6.2" data-path="descriptives.html"><a href="descriptives.html#iqrs"><i class="fa fa-check"></i><b>6.6.2</b> IQRs</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="descriptives.html"><a href="descriptives.html#descriptives-for-groups"><i class="fa fa-check"></i><b>6.7</b> Descriptives for Groups</a></li>
<li class="chapter" data-level="6.8" data-path="descriptives.html"><a href="descriptives.html#comparing-population-and-sample-means"><i class="fa fa-check"></i><b>6.8</b> Comparing population and sample means</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>7</b> Distributions</a><ul>
<li class="chapter" data-level="7.1" data-path="distributions.html"><a href="distributions.html#what-is-a-distribution"><i class="fa fa-check"></i><b>7.1</b> What is a distribution ?</a><ul>
<li class="chapter" data-level="7.1.1" data-path="distributions.html"><a href="distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>7.1.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="7.1.2" data-path="distributions.html"><a href="distributions.html#bimodal-distribution"><i class="fa fa-check"></i><b>7.1.2</b> Bimodal Distribution</a></li>
<li class="chapter" data-level="7.1.3" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>7.1.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="7.1.4" data-path="distributions.html"><a href="distributions.html#standard-normal-distribution"><i class="fa fa-check"></i><b>7.1.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="7.1.5" data-path="distributions.html"><a href="distributions.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>7.1.5</b> Skewness and Kurtosis</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="distributions.html"><a href="distributions.html#z-scores"><i class="fa fa-check"></i><b>7.2</b> z-scores</a><ul>
<li class="chapter" data-level="7.2.1" data-path="distributions.html"><a href="distributions.html#z-scores-in-samples."><i class="fa fa-check"></i><b>7.2.1</b> z-scores in samples.</a></li>
<li class="chapter" data-level="7.2.2" data-path="distributions.html"><a href="distributions.html#using-z-scores-to-determine-probabilities"><i class="fa fa-check"></i><b>7.2.2</b> Using z-scores to determine probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="distributions.html"><a href="distributions.html#what-is-a-sampling-distribution"><i class="fa fa-check"></i><b>7.3</b> What is a Sampling Distribution ?</a><ul>
<li class="chapter" data-level="7.3.1" data-path="distributions.html"><a href="distributions.html#sample-size-and-the-sampling-distribution"><i class="fa fa-check"></i><b>7.3.1</b> Sample Size and the Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="distributions.html"><a href="distributions.html#central-limit-theorem."><i class="fa fa-check"></i><b>7.4</b> Central Limit Theorem.</a></li>
<li class="chapter" data-level="7.5" data-path="distributions.html"><a href="distributions.html#sampling-distribution-problems."><i class="fa fa-check"></i><b>7.5</b> Sampling distribution problems.</a></li>
<li class="chapter" data-level="7.6" data-path="distributions.html"><a href="distributions.html#the-t-distribution"><i class="fa fa-check"></i><b>7.6</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>8</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="8.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#sample-means-as-estimates."><i class="fa fa-check"></i><b>8.1</b> Sample means as estimates.</a></li>
<li class="chapter" data-level="8.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-confidence-interval-with-z-distribution"><i class="fa fa-check"></i><b>8.2</b> Calculating a confidence interval with z-distribution</a><ul>
<li class="chapter" data-level="8.2.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges"><i class="fa fa-check"></i><b>8.2.1</b> Other Confidence Intervals ranges</a></li>
<li class="chapter" data-level="8.2.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-and-sample-size"><i class="fa fa-check"></i><b>8.2.2</b> Confidence Intervals and Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-with-t-distribution"><i class="fa fa-check"></i><b>8.3</b> Confidence Intervals with t-distribution</a></li>
<li class="chapter" data-level="8.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-t-distribution-confidence-interval"><i class="fa fa-check"></i><b>8.4</b> Calculating a t-distribution Confidence Interval</a><ul>
<li class="chapter" data-level="8.4.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#t-distribution-cis-and-sample-size."><i class="fa fa-check"></i><b>8.4.1</b> t-distribution CIs and sample size.</a></li>
<li class="chapter" data-level="8.4.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges-for-t-distribution"><i class="fa fa-check"></i><b>8.4.2</b> Other Confidence Intervals ranges for t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="confidence-intervals.html"><a href="confidence-intervals.html#comparing-cis-using-the-z--and-t-distributions"><i class="fa fa-check"></i><b>8.5</b> Comparing CIs using the z- and t-distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>9</b> One Sample Inferential Statistics</a><ul>
<li class="chapter" data-level="9.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-z-tests"><i class="fa fa-check"></i><b>9.1</b> One-sample Z-tests</a><ul>
<li class="chapter" data-level="9.1.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#sampling-distribution-recap"><i class="fa fa-check"></i><b>9.1.1</b> Sampling Distribution Recap</a></li>
<li class="chapter" data-level="9.1.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#calculating-p-values-for-z-test"><i class="fa fa-check"></i><b>9.1.2</b> Calculating p-values for z-test</a></li>
<li class="chapter" data-level="9.1.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#using-critical-values"><i class="fa fa-check"></i><b>9.1.3</b> Using critical values</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-t-tests"><i class="fa fa-check"></i><b>9.2</b> One-sample t-tests</a><ul>
<li class="chapter" data-level="9.2.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#critical-values-for-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.2.1</b> Critical values for the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#conducting-one-sample-t-tests-in-r"><i class="fa fa-check"></i><b>9.3</b> Conducting one-sample t-tests in R</a></li>
<li class="chapter" data-level="9.4" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#assumptions-of-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> Assumptions of the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>10</b> Two Sample Inferential Statistics</a><ul>
<li class="chapter" data-level="10.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#comparing-two-samples"><i class="fa fa-check"></i><b>10.1</b> Comparing two Samples</a></li>
<li class="chapter" data-level="10.2" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#independent-samples-t-test"><i class="fa fa-check"></i><b>10.2</b> Independent Samples t-test</a></li>
<li class="chapter" data-level="10.3" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#background-to-students-2-sample-t-test"><i class="fa fa-check"></i><b>10.3</b> Background to Student's 2 Sample t-test</a></li>
<li class="chapter" data-level="10.4" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#sampling-distribution-of-the-difference-in-sample-means"><i class="fa fa-check"></i><b>10.4</b> Sampling Distribution of the Difference in Sample Means</a></li>
<li class="chapter" data-level="10.5" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#pooled-standard-deviation"><i class="fa fa-check"></i><b>10.5</b> Pooled Standard Deviation</a></li>
<li class="chapter" data-level="10.6" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#confidence-interval-for-difference-in-means"><i class="fa fa-check"></i><b>10.6</b> Confidence Interval for Difference in Means</a></li>
<li class="chapter" data-level="10.7" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#conducting-student-t-test"><i class="fa fa-check"></i><b>10.7</b> Conducting Student t-test</a></li>
<li class="chapter" data-level="10.8" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#doing-student-t-test-in-r"><i class="fa fa-check"></i><b>10.8</b> Doing Student t-test in R</a></li>
<li class="chapter" data-level="10.9" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#effect-sizes"><i class="fa fa-check"></i><b>10.9</b> Effect Sizes</a></li>
<li class="chapter" data-level="10.10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#paired-t-tests"><i class="fa fa-check"></i><b>10.10</b> Paired t-tests</a></li>
<li class="chapter" data-level="10.11" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#confidence-intervals-with-paired-data"><i class="fa fa-check"></i><b>10.11</b> Confidence Intervals with Paired Data</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>11</b> Correlation</a><ul>
<li class="chapter" data-level="11.1" data-path="correlation.html"><a href="correlation.html#pearson-correlation"><i class="fa fa-check"></i><b>11.1</b> Pearson Correlation</a></li>
<li class="chapter" data-level="11.2" data-path="correlation.html"><a href="correlation.html#calculating-the-pearson-correlation-in-r"><i class="fa fa-check"></i><b>11.2</b> Calculating the Pearson Correlation in R</a></li>
<li class="chapter" data-level="11.3" data-path="correlation.html"><a href="correlation.html#cross-products"><i class="fa fa-check"></i><b>11.3</b> Cross-products</a></li>
<li class="chapter" data-level="11.4" data-path="correlation.html"><a href="correlation.html#conducting-a-pearson-correlation-test"><i class="fa fa-check"></i><b>11.4</b> Conducting a Pearson Correlation Test</a></li>
<li class="chapter" data-level="11.5" data-path="correlation.html"><a href="correlation.html#assumptions-of-pearsons-correlation"><i class="fa fa-check"></i><b>11.5</b> Assumptions of Pearson's Correlation</a></li>
<li class="chapter" data-level="11.6" data-path="correlation.html"><a href="correlation.html#confidence-intervals-for-r"><i class="fa fa-check"></i><b>11.6</b> Confidence Intervals for R</a></li>
<li class="chapter" data-level="11.7" data-path="correlation.html"><a href="correlation.html#partial-correlations"><i class="fa fa-check"></i><b>11.7</b> Partial Correlations</a></li>
<li class="chapter" data-level="11.8" data-path="correlation.html"><a href="correlation.html#non-parametric-correlations"><i class="fa fa-check"></i><b>11.8</b> Non-parametric Correlations</a></li>
<li class="chapter" data-level="11.9" data-path="correlation.html"><a href="correlation.html#point-biserial-correlation"><i class="fa fa-check"></i><b>11.9</b> Point-Biserial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>12</b> Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="regression.html"><a href="regression.html#introduction-to-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Introduction to Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="regression.html"><a href="regression.html#a-and-b"><i class="fa fa-check"></i><b>12.2</b> a and b</a><ul>
<li class="chapter" data-level="12.2.1" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-in-r"><i class="fa fa-check"></i><b>12.2.1</b> How to calculate a and b in R</a></li>
<li class="chapter" data-level="12.2.2" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-by-hand"><i class="fa fa-check"></i><b>12.2.2</b> How to calculate a and b 'by hand'</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="regression.html"><a href="regression.html#residuals"><i class="fa fa-check"></i><b>12.3</b> Residuals</a><ul>
<li class="chapter" data-level="12.3.1" data-path="regression.html"><a href="regression.html#how-to-calculate-the-residuals"><i class="fa fa-check"></i><b>12.3.1</b> How to calculate the residuals</a></li>
<li class="chapter" data-level="12.3.2" data-path="regression.html"><a href="regression.html#visualizing-the-residuals"><i class="fa fa-check"></i><b>12.3.2</b> Visualizing the Residuals</a></li>
<li class="chapter" data-level="12.3.3" data-path="regression.html"><a href="regression.html#comparing-our-trendline-to-other-trendlines"><i class="fa fa-check"></i><b>12.3.3</b> Comparing our trendline to other trendlines</a></li>
<li class="chapter" data-level="12.3.4" data-path="regression.html"><a href="regression.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>12.3.4</b> Coefficient of Determination R2</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="regression.html"><a href="regression.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4</b> Standard Error of the Estimate</a><ul>
<li class="chapter" data-level="12.4.1" data-path="regression.html"><a href="regression.html#what-to-do-with-the-standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4.1</b> What to do with the Standard Error of the Estimate ?</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="regression.html"><a href="regression.html#goodness-of-fit-test---f-ratio"><i class="fa fa-check"></i><b>12.5</b> Goodness of Fit Test - F-ratio</a></li>
<li class="chapter" data-level="12.6" data-path="regression.html"><a href="regression.html#assumptions-of-linear-regression"><i class="fa fa-check"></i><b>12.6</b> Assumptions of Linear Regression</a><ul>
<li class="chapter" data-level="12.6.1" data-path="regression.html"><a href="regression.html#normality-of-residuals"><i class="fa fa-check"></i><b>12.6.1</b> Normality of Residuals</a></li>
<li class="chapter" data-level="12.6.2" data-path="regression.html"><a href="regression.html#linearity----"><i class="fa fa-check"></i><b>12.6.2</b> 2. Linearity ---</a></li>
<li class="chapter" data-level="12.6.3" data-path="regression.html"><a href="regression.html#homogeneity-of-variance-homoscedasticity"><i class="fa fa-check"></i><b>12.6.3</b> 3. Homogeneity of Variance / Homoscedasticity</a></li>
<li class="chapter" data-level="12.6.4" data-path="regression.html"><a href="regression.html#no-colinearity"><i class="fa fa-check"></i><b>12.6.4</b> No Colinearity</a></li>
<li class="chapter" data-level="12.6.5" data-path="regression.html"><a href="regression.html#unusual-datapoints"><i class="fa fa-check"></i><b>12.6.5</b> Unusual Datapoints</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="regression.html"><a href="regression.html#examining-individual-predictor-estimates"><i class="fa fa-check"></i><b>12.7</b> Examining individual predictor estimates</a><ul>
<li class="chapter" data-level="12.7.1" data-path="regression.html"><a href="regression.html#confidence-interval-of-b."><i class="fa fa-check"></i><b>12.7.1</b> 95% confidence interval of 'b'.</a></li>
<li class="chapter" data-level="12.7.2" data-path="regression.html"><a href="regression.html#standard-error-of-b"><i class="fa fa-check"></i><b>12.7.2</b> Standard Error of b</a></li>
<li class="chapter" data-level="12.7.3" data-path="regression.html"><a href="regression.html#calculating-95-confidence-interval-of-b-by-hand"><i class="fa fa-check"></i><b>12.7.3</b> Calculating 95% confidence interval of 'b' by hand</a></li>
<li class="chapter" data-level="12.7.4" data-path="regression.html"><a href="regression.html#signifcance-testing-b"><i class="fa fa-check"></i><b>12.7.4</b> Signifcance Testing b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="permutation-testing.html"><a href="permutation-testing.html"><i class="fa fa-check"></i><b>13</b> Permutation Testing</a><ul>
<li class="chapter" data-level="13.1" data-path="permutation-testing.html"><a href="permutation-testing.html#t-test-permutation"><i class="fa fa-check"></i><b>13.1</b> t-test Permutation</a></li>
<li class="chapter" data-level="13.2" data-path="permutation-testing.html"><a href="permutation-testing.html#correlation-coefficient-permutation-tests"><i class="fa fa-check"></i><b>13.2</b> Correlation Coefficient Permutation Tests</a></li>
<li class="chapter" data-level="13.3" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-test-for-a-paired-t-test"><i class="fa fa-check"></i><b>13.3</b> Permutation test for a Paired t-test</a></li>
<li class="chapter" data-level="13.4" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-tests-in-packages"><i class="fa fa-check"></i><b>13.4</b> Permutation tests in Packages</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PSY317L Guides</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="confidence-intervals" class="section level1">
<h1><span class="header-section-number">8</span> Confidence Intervals</h1>
<p>When we collect a sample, we typically calculate a sample mean <span class="math inline">\(\overline{x}\)</span>. We use this as our estimate of the real population mean <span class="math inline">\(\mu\)</span>, as this is unknown to us typically. If we were to collect another sample, we would most likely get a sample mean <span class="math inline">\(\overline{x}\)</span> that is slightly different to the first one, but would also be an estimate of the population mean <span class="math inline">\(\mu\)</span>.</p>
<p>Given that we are not completely sure of what the population mean <span class="math inline">\(\mu\)</span> is, or how close our sample mean <span class="math inline">\(\overline{x}\)</span> is to that population mean, one thing that we like to do is to put confidence limits around our sample mean estimate. The confidence interval gives us a range of values which likely contains our population mean. In essence, a confidence interval can be considered to be a margin of error around our sample mean estimate.</p>
<p>In this course, we use two separate approaches to calculate confidence intervals around a sample mean <span class="math inline">\(\overline{x}\)</span>. The first method uses the <span class="math inline">\(z\)</span>-distribution to generate the confidence interval. The second method uses the <span class="math inline">\(t\)</span>-distribution. In practice, we almost always use the <span class="math inline">\(t\)</span>-distribution when doing this. In fact, the only time we really use the <span class="math inline">\(z\)</span>-distribution is when teaching introductory stats. The reason for this, is that learning how to make a confidence interval using the <span class="math inline">\(z\)</span>-distribution is a good stepping stone to using the <span class="math inline">\(t\)</span>-distribution. Technically, we can use the <span class="math inline">\(z\)</span>-distribution to calculate the confidence interval when we know the population standard deviation <span class="math inline">\(\sigma\)</span> and our sample size is relatively large. However, we almost never know <span class="math inline">\(\sigma\)</span>, and so that's why in practice we use the <span class="math inline">\(t\)</span>-distribution.</p>
<p>We'll start this chapter by talking about the relationship between the sampling distribution and confidence intervals. Then we'll describe how to use both the <span class="math inline">\(z\)</span>- and <span class="math inline">\(t-\)</span>distributions to generate confidence intervals.</p>
<div id="sample-means-as-estimates." class="section level2">
<h2><span class="header-section-number">8.1</span> Sample means as estimates.</h2>
<p>Let us imagine we have a population of butterflies and we're interested in their wingspan. The population is normally distributed with a population mean <span class="math inline">\(\mu\)</span> is 7.8cm, with a population standard deviation <span class="math inline">\(\sigma\)</span> is 0.3cm. This is what this population distribution looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dt">mean =</span> <span class="fl">7.8</span>, <span class="dt">sd =</span> <span class="fl">0.3</span>)
<span class="kw">library</span>(tidyverse)
p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">vals=</span>x),<span class="kw">aes</span>(vals))+
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;purple&quot;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>, <span class="dt">binwidth =</span> <span class="fl">0.05</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">0.7</span>, <span class="dt">fill =</span> <span class="st">&quot;mistyrose&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Wingspan cm&quot;</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">7.8</span>, <span class="dt">color=</span><span class="st">&#39;black&#39;</span>,<span class="dt">lwd=</span><span class="dv">1</span>)
p1</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-291-1.png" width="672" /></p>
<p>Now, let's collect samples of size <span class="math inline">\(n=15\)</span>. Here's one sample, and it's sample mean <span class="math inline">\(\overline{x}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
samp1 &lt;-<span class="st"> </span><span class="kw">sample</span>(x, <span class="dt">size =</span> <span class="dv">15</span>, <span class="dt">replace =</span> T)
samp1</code></pre></div>
<pre><code>##  [1] 7.532948 8.233244 8.014278 7.446299 8.294182 7.660900 8.045752 7.251308
##  [9] 7.267707 7.742027 8.243312 8.039407 8.367953 8.022449 7.930565</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp1)</code></pre></div>
<pre><code>## [1] 7.872822</code></pre>
<p>Our observed sample mean is <span class="math inline">\(\overline{x}=7.72\)</span> which is pretty close to the population mean of <span class="math inline">\(\mu=7.8\)</span>. But if we were to collect another sample, then that sample mean will be slightly different. Let's do it again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp2 &lt;-<span class="st"> </span><span class="kw">sample</span>(x, <span class="dt">size =</span> <span class="dv">15</span>, <span class="dt">replace =</span> T)
samp2</code></pre></div>
<pre><code>##  [1] 8.050309 7.831114 7.672871 7.681791 7.198823 8.339807 7.520375 8.258590
##  [9] 7.671240 7.189658 8.489073 7.950898 7.670181 7.716136 7.326798</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp2)</code></pre></div>
<pre><code>## [1] 7.771178</code></pre>
<p>This time our sample mean (our estimate) is <span class="math inline">\(\overline{x}=7.95\)</span>.</p>
<p>If we did this thousands of times, then we'd get our sampling distribution of sample means (see section xxx.xxx). This is what our sampling distribution for sample sizes of <span class="math inline">\(n=15\)</span> looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#get sample means for sampling distribution</span>
results&lt;-<span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>,<span class="dv">100000</span>)
for(i in <span class="dv">1</span>:<span class="dv">100000</span>){
results[[i]]  &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(x, <span class="dv">15</span>, <span class="dt">replace =</span> T))  
}

res &lt;-<span class="st"> </span><span class="kw">unlist</span>(results)

p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(res), <span class="kw">aes</span>(<span class="dt">x =</span> res)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;#4adbe0&quot;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>, <span class="dt">binwidth =</span> <span class="fl">0.01</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">0.7</span>, <span class="dt">fill =</span> <span class="st">&quot;ghostwhite&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Mean&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Frequency&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Sample Means for n=15&quot;</span>) +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(res), <span class="dt">lwd=</span><span class="dv">1</span>)

p2</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-294-1.png" width="672" /></p>
<p>According to Central Limit Theorem, this sampling distribution is approximately normally distributed. The mean of this sampling distribution is <span class="math inline">\(\mu_{\overline{x}}=7.8\)</span> which is the same as the population mean <span class="math inline">\(\mu\)</span>. The standard deviation of this sampling distribution <span class="math inline">\(\sigma{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">0.3</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">15</span>)</code></pre></div>
<pre><code>## [1] 0.07745967</code></pre>
<p>Therefore the standard deviation of this sampling distribution is <span class="math inline">\(\sigma{\overline{x}} = 0.077\)</span>.</p>
<p>Remember, this sampling distribution represents thousands and thousands of potential means from individual samples of size 15 that we could have collected. Each one of them in isolation would be our point estimate of the true population mean <span class="math inline">\(\mu\)</span>. Sometimes we'll be really close to the true population mean, and other times we might bea quite far away. This is why we like to put confidence intervals around our sample means, to give a range of values that likely contain our population mean.</p>
<p>One thing we can do first is to think about this - between which two values on the sampling distribution shown above would 95% of the data lie? That is the same as asking, which two values represent the part where 2.5% of the distribution is in each tail (leaving 95% in the middle). To answer this, we just need to remember that according to Central Limit Theorem that our sampling distribution is normally distributed. Therefore we can use the standard normal curve.</p>
<p>According to the standard normal distribution, the values of <code>z</code> that leave 2.5% in each tail are <span class="math inline">\(z=-1.96\)</span> and <span class="math inline">\(z=1.96\)</span>. That means values that 95% of the distribution lie between 1.96 standard deviations below and above the mean.</p>
<div class="figure">
<img src="img/snd.png" />

</div>
<p>If you didn't want to take our word for it that +1.96 and -1.96 are the values of <code>z</code> that leave 2.5% in each tail, you could also directly calculate it in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="co"># get the values of z that are the boundaries of 2.5% to the left, and 97.5% to the left.</span></code></pre></div>
<pre><code>## [1] -1.959964  1.959964</code></pre>
<p>So if we go back to thinking about our sampling distribution - because we say it is approximately normally distributed, 95% of the distribution will also lie between 1.96 standard deviations below and above the mean. We know that the mean of the sampling distribution is <span class="math inline">\(\mu_{\overline{x}=7.8}\)</span> and the standard deviation of the sampling distribution is <span class="math inline">\(\sigma{\overline{x}} = 0.077\)</span>, as we calculated it above. Therefore, we can use this to calculate which sample mean values in the distribution are 1.96 standard deviations either side of the mean. They are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">7.8</span> +<span class="st">  </span>(<span class="fl">1.96</span> *<span class="st"> </span><span class="fl">0.077</span>)</code></pre></div>
<pre><code>## [1] 7.95092</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">7.8</span> -<span class="st">  </span>(<span class="fl">1.96</span> *<span class="st"> </span><span class="fl">0.077</span>)</code></pre></div>
<pre><code>## [1] 7.64908</code></pre>
<p>So, 95% of our sample means in our sampling distribution lie between 7.65 and 7.95. That area is represented by the shaded red area on our sampling distribution below:</p>
<div class="figure">
<img src="img/snd2.png" />

</div>
<p>What we have just done is the basic principle behind a confidence interval using a <span class="math inline">\(z\)</span>-distribution. Let's look at this in more detail.</p>
</div>
<div id="calculating-a-confidence-interval-with-z-distribution" class="section level2">
<h2><span class="header-section-number">8.2</span> Calculating a confidence interval with z-distribution</h2>
<p>Let's go back to our first sample of size 15 that we collected, with <span class="math inline">\(\overline{x}=7.72\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp1</code></pre></div>
<pre><code>##  [1] 7.532948 8.233244 8.014278 7.446299 8.294182 7.660900 8.045752 7.251308
##  [9] 7.267707 7.742027 8.243312 8.039407 8.367953 8.022449 7.930565</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp1)</code></pre></div>
<pre><code>## [1] 7.872822</code></pre>
<p>What we want to do now is put a confidence interval around 7.72. We want to say that our population mean is equal to <span class="math inline">\(7.72 \pm margin.of.error\)</span> The actual formula for the <span class="math inline">\(z\)</span>-distribution confidence interval is:</p>
<p><span class="math inline">\(CI_{95\%} = \overline{x} \pm z \times \frac{\sigma}{\sqrt{n}}\)</span></p>
<p>In this scenario, we are presuming that we don't know what the population mean <span class="math inline">\(\mu\)</span> is - that's why we're building a confidence interval. Consequently, we also don't precisely know what the mean of the sampling distribution <span class="math inline">\(\mu_{\overline{x}}\)</span> is. What we'll do instead, is to <em>assume</em> that our sample mean <span class="math inline">\(\overline{x}\)</span> is the mean of the sampling distribution <span class="math inline">\(\mu_{\overline{x}}\)</span>. We already know what the standard deviation of the sampling distribution <span class="math inline">\(\sigma{\overline{x}}\)</span> is because we know the population standard deviation <span class="math inline">\(\sigma\)</span> is. So, <span class="math inline">\(\sigma{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span>.</p>
<p>What value of <span class="math inline">\(z\)</span> should we use? The short answer is 1.96 for the same reasons as above. If our sampling distribution is normally distributed, then we want to know the values that are <span class="math inline">\(\pm 1.96\)</span> of the sample mean <span class="math inline">\(\overline{x}\)</span>.</p>
<p>So, let's just do it - this is how we calculate the 95% confidence interval if we have <span class="math inline">\(\overline{x}\)</span>, <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(n\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_bar &lt;-<span class="st"> </span><span class="kw">mean</span>(samp1)  <span class="co"># sample mean = 7.72</span>
x_bar</code></pre></div>
<pre><code>## [1] 7.872822</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">length</span>(samp1)  <span class="co"># sample size =  n = 15</span>
n</code></pre></div>
<pre><code>## [1] 15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sigma &lt;-<span class="st"> </span><span class="fl">0.3</span>  <span class="co"># the pop SD given in the example</span>

sem &lt;-<span class="st"> </span>sigma/<span class="kw">sqrt</span>(n) <span class="co"># standard error of the mean (SD of sampling distribution)</span>
sem</code></pre></div>
<pre><code>## [1] 0.07745967</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span><span class="fl">1.96</span>  <span class="co"># the value of &#39;z&#39; we need to get the middle 95% of the distribution</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># margin of error</span>
z *<span class="st"> </span>sem</code></pre></div>
<pre><code>## [1] 0.1518209</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># upper bound of confidence interval</span>
x_bar +<span class="st"> </span>(z *<span class="st"> </span>sem)</code></pre></div>
<pre><code>## [1] 8.024643</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lower bound of confidence interval</span>
x_bar -<span class="st"> </span>(z *<span class="st"> </span>sem)</code></pre></div>
<pre><code>## [1] 7.721001</code></pre>
<p>We have just calculated our 95% confidence interval! It has a lower bound of 7.568cm and an upper bound of 7.871cm. We can write this confidence interval in two ways:</p>
<p><span class="math inline">\(CI_{95\%} = 7.72 \pm 0.152\)</span></p>
<p><span class="math inline">\(CI_{95\%} = 7.72 [7.568, 7.871]\)</span></p>
<p>Below is a graphical representation of our confidence interval around our sample mean <span class="math inline">\(\overline{x}\)</span>. You can see that the true population mean <span class="math inline">\(\mu\)</span> is within the confidence interval.</p>
<div class="figure">
<img src="img/ci1.png" />

</div>
<p>Remember we collected a second sample that had a sample mean <span class="math inline">\(\overline{x}=7.95\)</span> ?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp2)</code></pre></div>
<pre><code>## [1] 7.771178</code></pre>
<p>We could also create a 95% confidence interval for our estimate of the population mean <span class="math inline">\(\mu\)</span> using this sample mean <span class="math inline">\(\overline{x}\)</span>. We just use the same formula:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># upper bound of confidence interval</span>
<span class="fl">7.95</span> +<span class="st"> </span>(z *<span class="st"> </span>sem)</code></pre></div>
<pre><code>## [1] 8.101821</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lower bound of confidence interval</span>
<span class="fl">7.95</span> -<span class="st"> </span>(z *<span class="st"> </span>sem)</code></pre></div>
<pre><code>## [1] 7.798179</code></pre>
<p><span class="math inline">\(CI_{95\%} = 7.95 \pm 0.152\)</span></p>
<p><span class="math inline">\(CI_{95\%} = 7.95 [7.798, 8.102]\)</span></p>
<p>Let's compare this confidence interval with the first one we created:</p>
<div class="figure">
<img src="img/ci2.png" />

</div>
<p>Note that both include the true population mean of 7.8 in their confidence interval. In the second sample mean - it's only just inside, but it's inside!</p>
<p>What if we collected 20 new samples, and calculated 20 sample means, and made 20 confidence intervals? Well, the chart below shows 20 95% confidence intervals collected from samples of size 15 selected at random from our population of butterflies:</p>
<p>First, notice that the margin of error is equal for all of our confidence intervals around the sample means. This is because we are using the same value of <span class="math inline">\(\sigma\)</span> and same value of <span class="math inline">\(z\)</span> for all of these confidence intervals. Secondly, you'll notice that not all the confidence intervals include the population mean <span class="math inline">\(\mu\)</span>. Two of them - highlighted in green - do not include the population mean. In this sense, our sample mean and associated confidence interval is not doing a terrific job of estimating the population mean.</p>
<p>Actually, it turns out that if you collect enough samples and generate enough sample means, then you <em>will capture</em> the population mean within your confidence interval 95% of the time. So roughly 5 out of every 100 confidence intervals you make from samples will not include the population mean.</p>
<p>Technically, this is the definition of a 95% confidence interval. That is, in 95% of your samples you will include the true population mean. However, when talking about confidence intervals in lay-speak, when we have our one confidence interval around our one sample mean e.g. <span class="math inline">\(CI_{95\%} = 7.72 [7.568, 7.871]\)</span>, we often say <em>&quot;there's a 95% chance that the true population mean is between 7.568 and 7.871</em>. This is technically lazy shorthand although it does kind of help us understand the point of a confidence interval. But, please remember, the real definition is that in 95% of samples we'll include the true population mean in our samples.</p>
<p><strong>Assumptions</strong> We should also briefly just remark on what the assumptions are when generating these <span class="math inline">\(z\)</span>-distribution based confidence intervals. We are assuming that our data are normally distributed and that our sample is randomly drawn from the population, and that all data points are independent of each other.</p>
<div id="other-confidence-intervals-ranges" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Other Confidence Intervals ranges</h3>
<p>We can actually construct confidence intervals for any % value. Most commonly people make 95% confidence intervals, but other common ones include 80%, 90% and 99% confidence intervals. These have the same interepretation as the 95% CI. For instance, a 99% confidence interval means that if you were to take 100 samples from a population and calculate 99% confidence intervals for each, only 1 out of a 100 on average would not include the true population mean <span class="math inline">\(\mu\)</span>.</p>
<p>The formulas for each of these confidence intervals when using the <span class="math inline">\(z\)</span>-distribution are as follows:</p>
<p><span class="math inline">\(CI_{80\%} = \overline{x} \pm 1.28 \times \frac{\sigma}{\sqrt{n}}\)</span></p>
<p><span class="math inline">\(CI_{90\%} = \overline{x} \pm 1.64 \times \frac{\sigma}{\sqrt{n}}\)</span></p>
<p><span class="math inline">\(CI_{95\%} = \overline{x} \pm 1.96 \times \frac{\sigma}{\sqrt{n}}\)</span></p>
<p><span class="math inline">\(CI_{99\%} = \overline{x} \pm 2.58 \times \frac{\sigma}{\sqrt{n}}\)</span></p>
<p>Where did each of these different numbers come from for <span class="math inline">\(z\)</span> ? Well, if we wish to make a 99% CI, we need to know what values of <span class="math inline">\(z\)</span> are the boundaries that leave 99% of the distribution inside them on the standard normal curve. We exclude 0.5% in each tail. Likewise, for the 80%CI, we want to know the values of <span class="math inline">\(z\)</span> that leave 5% in each tail and 10% in the middle. We can calculate these values in R like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(.<span class="dv">9</span>)<span class="co"># for 80% CI</span></code></pre></div>
<pre><code>## [1] 1.281552</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(.<span class="dv">95</span>)<span class="co"># for 90% CI</span></code></pre></div>
<pre><code>## [1] 1.644854</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(.<span class="dv">975</span>)<span class="co"># for 95% CI</span></code></pre></div>
<pre><code>## [1] 1.959964</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(.<span class="dv">995</span>)<span class="co"># for 99% CI</span></code></pre></div>
<pre><code>## [1] 2.575829</code></pre>
<div class="figure">
<img src="img/ci4.png" />

</div>
<p>So, if we were to calculate the 80% confidence interval for our first sample, we would do:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#margin of error</span>
<span class="fl">1.281552</span> *<span class="st"> </span>sem</code></pre></div>
<pre><code>## [1] 0.09926859</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># upper bound of confidence interval</span>
x_bar +<span class="st"> </span>(<span class="fl">1.281552</span> *<span class="st"> </span>sem)</code></pre></div>
<pre><code>## [1] 7.972091</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lower bound of confidence interval</span>
x_bar -<span class="st"> </span>(<span class="fl">1.281552</span> *<span class="st"> </span>sem)</code></pre></div>
<pre><code>## [1] 7.773553</code></pre>
<p>Our 80% confidence interval is:</p>
<p><span class="math inline">\(CI_{80\%} = 7.72 \pm 0.099\)</span></p>
<p><span class="math inline">\(CI_{80\%} = 7.72 [7.620, 7.819]\)</span></p>
<p>Looking at each of these confidence intervals for our first sample mean, we get the following. Clearly, confidence intervals widen with higher percentages:</p>
<div class="figure">
<img src="img/ci5.png" />

</div>
<p>Let's look at this same figure, but this time for our second sample that has an sample mean <span class="math inline">\(\overline{x}=7.95\)</span>:</p>
<div class="figure">
<img src="img/ci6.png" />

</div>
<p>As you can see here, this time the 90% and 80% CIs do not inclue the true population mean <span class="math inline">\(\mu\)</span>. We increase the chances of including the true population mean <span class="math inline">\(\mu\)</span> inside our confidence interval by increasing the level of our confidence interval. A 99% confidence interval has an increased probability of including the confidence interval compared to a 95% confidence interval and so on.</p>
</div>
<div id="confidence-intervals-and-sample-size" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Confidence Intervals and Sample Size</h3>
<p>The other variable inside the confidence interval formula that we should think about is the sample size <span class="math inline">\(n\)</span>. Let's look at the formula again:</p>
<p><span class="math inline">\(CI = \overline{x} \pm z \times \frac{\sigma}{\sqrt{n}}\)</span></p>
<p>What happens when we get different sized sample sizes? For instance, look at the 95% confidence intervals below that all have a sample mean of <span class="math inline">\(\overline{x}=7.72\)</span> but are for different sample sizes:</p>
<div class="figure">
<img src="img/ci7.png" />

</div>
<p>There are two things to note. Firstly, as your sample size increases, for any confidence level (in this situation a 95% CI) the confidence interval is going to shrink. It gets tighter for larger sample sizes. <strong>Increasing sample sizes increases certainty</strong>. This is because the denominator of the confidence interval formula <span class="math inline">\(\sqrt{n}\)</span> gets larger, meaning that the margin of error gets smaller.</p>
<p>The second thing might seem counterintuitive. Why does it look like in the graph above that a sample size of <span class="math inline">\(n=50\)</span> is only just able to have <span class="math inline">\(\mu\)</span> contained within it? It would seem that a larger sample size should do a better job of including <span class="math inline">\(\mu\)</span>. Well, remember, that a 95% CI really means that 95% of your sample means will contain <span class="math inline">\(\mu\)</span>.... so across all these sample sizes you have a 95% chance of having captured <span class="math inline">\(\mu\)</span> inside your CI.</p>
<p>The key thing to remember is that with a bigger sample size you are much more likely to get a sample mean <span class="math inline">\(\overline{x}\)</span> that is close to the population mean <span class="math inline">\(\mu\)</span>. That's because the sampling distribution of sample means is much tighter. In some ways the figure above is a bit misleading as all the sample means are at 7.72. What is more likely to be the case for many samples is that they will be closer to the true population mean. Look at the figure below, that compares samples sizes of 10 with sample sizes of 50. Larger samples lead to CIs that have a sample mean closer to <span class="math inline">\(\mu\)</span> and that are tighter - but still with a 95% chance of having captured <span class="math inline">\(\mu\)</span>.</p>
<div class="figure">
<img src="img/ci8.png" />

</div>
</div>
</div>
<div id="confidence-intervals-with-t-distribution" class="section level2">
<h2><span class="header-section-number">8.3</span> Confidence Intervals with t-distribution</h2>
<p>Hopefully the preceding sections on creating a confidence interval with the <span class="math inline">\(z\)</span>-distribution helped you in understanding some of the theory about confidence intervals in general. Perhaps there is still one thought going through your mind - isn't all of this a bit strange? Why are we trying to estimate the population mean <span class="math inline">\(\mu\)</span> from the sample mean <span class="math inline">\(\overline{x}\)</span> when we also already know the population standard deviation <span class="math inline">\(\sigma\)</span>? How could you know <span class="math inline">\(\sigma\)</span> but not know <span class="math inline">\(\mu\)</span> - that makes no sense, and indeed it doesn't.</p>
<p>It turns out that in the real world, that when we collect a sample of data and get our sample mean <span class="math inline">\(\overline{x}\)</span> and we want to create our confidence interval around it to have some certainty about where the population mean <span class="math inline">\(\mu\)</span> might lie, we also do not know <span class="math inline">\(\sigma\)</span>. We need a backup plan for how to construct confidence intervals. This back up plan is making confidence intervals with the <span class="math inline">\(t\)</span>-distribution.</p>
<p>First, let's look at the formula for making a confidence interval with a <span class="math inline">\(t\)</span>-distribution:</p>
<p><span class="math inline">\(CI = \overline{x} \pm t \times \frac{s}{\sqrt{n}}\)</span></p>
<p>Two things are different about this one compared to the formula for calculating a CI with the <span class="math inline">\(z\)</span>-distribution. First, we are using a <span class="math inline">\(t\)</span> value rather than a <span class="math inline">\(z\)</span> value. Secondly, we are using the sample standard deviation <span class="math inline">\(s\)</span> rather than the population standard deviation <span class="math inline">\(\sigma\)</span>. If we do not know <span class="math inline">\(\sigma\)</span> then our next best option is to use our estimate of the population standard deviation, which is our sample standard deviation <span class="math inline">\(s\)</span>.</p>
<p>We briefly introduced the <span class="math inline">\(t\)</span>-distribution in section xxx.xxx. Why do we need to use it here? Essentially, the key thing is that when we collect many sample means to create our sampling distribution of sample means, it is not always the case that this samplign distribution will be perfectly normally distributed. In fact, this is especially true for smaller sample sizes. If we collect smaller samples and calculate the sample mean of each, it turns out our sampling distribution will be slighly heavier in the tails than a normal distribution. How far away from normal our sampling distribution will be depends on our sample size. For bigger sample sizes, our sampling distribution will look more normal. This is illustrated below:</p>
<div class="figure">
<img src="img/t.png" />

</div>
<p>It's important to remember that the shape of the <span class="math inline">\(t\)</span>-distribution varies for different sample sizes. In fact, we actually state the distribution not in terms of the sample size, but in terms of the <em>degrees of freedom</em>. For instance, for a sample size of 15, we would say that the sampling distribution follows a <span class="math inline">\(t\)</span>-distribution with a shape of degrees of freedom 14. The degrees of freedom is equal to <span class="math inline">\(n-1\)</span> when describing sampling distributions of sample means.</p>
<p>As a result of this issue, if we were to assume that our sampling distribution was normally distributed and used <span class="math inline">\(z=1.96\)</span> to calculate our 95% confidence interval, we would be inaccurately determining where the middle 95% of the distribution was. In fact, for a <span class="math inline">\(t\)</span>-distribution, because the tails are heavier, the value of <span class="math inline">\(t\)</span> that leaves 2.5% in each tail will be a larger value than 1.96. Compare the standard normal curve below to the <span class="math inline">\(t\)</span>-distribution for <span class="math inline">\(df=14\)</span>.</p>
<p>fig here.</p>
<p>As we can see, the value of <span class="math inline">\(t\)</span> that leaves 2.5% in each tail is 2.145 which is higher than 1.96. Consequently, all else being equal, this will increase our margin of error.</p>
</div>
<div id="calculating-a-t-distribution-confidence-interval" class="section level2">
<h2><span class="header-section-number">8.4</span> Calculating a t-distribution Confidence Interval</h2>
<p>Let's look more practically at how we calculate a 95% confidence interval for a <span class="math inline">\(t\)</span>-distribtuion. We'll use sample 1 from above that had a mean of <span class="math inline">\(\overline{x}=7.72\)</span>.</p>
<p>The formula we use is:</p>
<p><span class="math inline">\(CI95\% = \overline{x} \pm t \times \frac{s}{\sqrt{n}}\)</span></p>
<p>First, we should calculate the sample standard deviation, which is <span class="math inline">\(s=0.285\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp1</code></pre></div>
<pre><code>##  [1] 7.532948 8.233244 8.014278 7.446299 8.294182 7.660900 8.045752 7.251308
##  [9] 7.267707 7.742027 8.243312 8.039407 8.367953 8.022449 7.930565</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(samp1)</code></pre></div>
<pre><code>## [1] 0.3688933</code></pre>
<p>Next, we need to calculate <span class="math inline">\(t\)</span>. This value will be the value that leaves 2.5% in the tails of a <span class="math inline">\(t\)</span>-distribution for degrees of freedom = 14 (<span class="math inline">\(n-1 = 14\)</span>). We can calculate that in R using the function <code>qt()</code>. We enter <code>0.975</code> to ask it to return the value of <span class="math inline">\(t\)</span> that leaves 2.5% in the upper tail, and then we enter <code>df=14</code> to ensure we are using the correct <span class="math inline">\(t\)</span>-distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="dt">p =</span> <span class="fl">0.975</span>, <span class="dt">df =</span> <span class="dv">14</span>)</code></pre></div>
<pre><code>## [1] 2.144787</code></pre>
<p>This shows us that our value of <span class="math inline">\(t=2.145\)</span>.</p>
<p>We can now create our estimate of the standard error (the standard deviation of the sampling distribution of sample means), and our confidence intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#standard error</span>
sem1 &lt;-<span class="st"> </span><span class="kw">sd</span>(samp1)/<span class="kw">sqrt</span>(<span class="dv">15</span>)
sem1</code></pre></div>
<pre><code>## [1] 0.09524785</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># upper bound of confidence interval</span>
<span class="fl">7.72</span> +<span class="st"> </span>(<span class="fl">2.144787</span> *<span class="st"> </span>sem1)</code></pre></div>
<pre><code>## [1] 7.924286</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lower bound of confidence interval</span>
<span class="fl">7.72</span> -<span class="st"> </span>(<span class="fl">2.144787</span> *<span class="st"> </span>sem1)</code></pre></div>
<pre><code>## [1] 7.515714</code></pre>
<p>Our confidence interval is therefore:</p>
<p><span class="math inline">\(CI95\% = 7.72[7.56,7.88]\)</span></p>
<p>Likewise, for sample 2, which had <span class="math inline">\(\overline{x}=7.95\)</span> our 95% confidence interval would be:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#standard error</span>
sem2 &lt;-<span class="st"> </span><span class="kw">sd</span>(samp2)/<span class="kw">sqrt</span>(<span class="dv">15</span>)
sem2</code></pre></div>
<pre><code>## [1] 0.1013502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># upper bound of confidence interval</span>
<span class="fl">7.95</span> +<span class="st"> </span>(<span class="fl">2.144787</span> *<span class="st"> </span>sem2)</code></pre></div>
<pre><code>## [1] 8.167375</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lower bound of confidence interval</span>
<span class="fl">7.95</span> -<span class="st"> </span>(<span class="fl">2.144787</span> *<span class="st"> </span>sem2)</code></pre></div>
<pre><code>## [1] 7.732625</code></pre>
<p>So, our confidence interval for this sample is therefore:</p>
<p><span class="math inline">\(CI95\% = 7.95[7.796,8.104]\)</span></p>
<p>We can graphically compare this to the 95% confidence interval we calculated using the <span class="math inline">\(z\)</span>-distribution. We've plotted the <span class="math inline">\(t\)</span> confidence intervals in purple and the <span class="math inline">\(z\)</span> confidence intervals in blue. The bottom two confidence intervals are those for sample 1, and the top two are those for sample 2.</p>
<div class="figure">
<img src="img/ci9.png" />

</div>
<p>Notice that the confidence intervals based on the <span class="math inline">\(t\)</span>-distribution are a little wider than those based on the <span class="math inline">\(z\)</span>-distribution. This is because we are using a higher value of <span class="math inline">\(t\)</span> than of <span class="math inline">\(z\)</span> in the equation. This is because we are assuming our sampling distribution is following the <span class="math inline">\(t\)</span> shape rather than the classic <span class="math inline">\(z\)</span> shape, and as the tails are heavier in a <span class="math inline">\(t\)</span>-distribution, the value of <span class="math inline">\(t\)</span> that leaves 2.5% in the tail is further away from 0.</p>
<p>There is also one other detail that is a little hard to see from only two samples above. That is that the size of the confidence intervals constructed using the <span class="math inline">\(z\)</span>-distribution are fixed - i.e. they are always the same size. This is because the value of <span class="math inline">\(z\)</span> is fixed (1.96 in this case) and the value of <span class="math inline">\(\sigma\)</span> is fixed - it's always the same population standard deviation, which doesn't change. However, for confidence intervals made using the <span class="math inline">\(t\)</span>-distribution, the size of these may change from sample to sample. This is because the sample standard deviation changes from sample to sample, meaning that not all confidence intervals will be the same length.</p>
<p>We can illustrate this below. Here are 20 95% confidence intervals made using either the <span class="math inline">\(z-\)</span> or <span class="math inline">\(t\)</span>-distribution for 20 different samples of sample size <span class="math inline">\(n=15\)</span>.</p>
<div class="figure">
<img src="img/ci10.png" />

</div>
<p>You can see that the <span class="math inline">\(z\)</span>-distribution CIs are all equal in length, whereas the <span class="math inline">\(t\)</span>-distribution ones vary from sample to sample. This is because of the use of the sample standard deviation <span class="math inline">\(s\)</span> in the formula. Most of the time, because of the higher <span class="math inline">\(t\)</span> value in the formula than the <span class="math inline">\(z\)</span> value, it leads to the CIs being wider for those calculated with the <span class="math inline">\(t\)</span>-distribution. This sometimes has important implications. For intance, notice the 9th sample down from the top. Using the <span class="math inline">\(z\)</span>-distribution, this CI does not capture the true population mean <span class="math inline">\(\mu\)</span>, but using the <span class="math inline">\(t\)</span>-distribution does capture it. However, the CIs are not always bigger when using the <span class="math inline">\(t\)</span>-distribution. Sometimes, the sample may just have very little variation in it meaning that the sample standard deviation <span class="math inline">\(s\)</span> is very small. This could lead to a smaller margin of error - as seen with the 19th and 20th samples from the top in the figure.</p>
<div id="t-distribution-cis-and-sample-size." class="section level3">
<h3><span class="header-section-number">8.4.1</span> t-distribution CIs and sample size.</h3>
<p>With the <span class="math inline">\(z\)</span>-distribution based confidence intervals, when we increased the sample size <span class="math inline">\(n\)</span>, the margin of error always decreased because both <span class="math inline">\(z\)</span> and <span class="math inline">\(\sigma\)</span> are fixed in the formula. For instance, for a 95% CI with a population standard deviation <span class="math inline">\(\sigma=10\)</span> and sample size <span class="math inline">\(n=10\)</span> or <span class="math inline">\(n=30\)</span>, the margin of error using the <span class="math inline">\(z\)</span>-distribution in the CI would be for each sample size:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">1.96</span> *<span class="st"> </span>(<span class="dv">10</span>/<span class="kw">sqrt</span>(<span class="dv">10</span>))</code></pre></div>
<pre><code>## [1] 6.198064</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">1.96</span> *<span class="st"> </span>(<span class="dv">10</span>/<span class="kw">sqrt</span>(<span class="dv">30</span>))</code></pre></div>
<pre><code>## [1] 3.578454</code></pre>
<p>Clearly, increaseing the sample size reduces the margin of error. The situation is not as consistent when constructing confidence intervals with the <span class="math inline">\(t\)</span>-distribution, although the general pattern remains true.</p>
<p>When we collect samples of different sample sizes, two things change in the <span class="math inline">\(t\)</span>-distribution confidence interval formula. Firstly, the value of <span class="math inline">\(t\)</span> used is dependent upon the degrees of freedom. As sample sizes increase, the <span class="math inline">\(t\)</span>- distribution becomes more normal shaped and less heavy in the tails. If, for example, we are interested in making 95% Confidence Intervals, then the value of <span class="math inline">\(t\)</span> that leaves 2.5% in each tail (and 95% of the distribution in the middle) is going to get closer to 1.96 (and negative -1.96) as the sample size increases. This is illustrated in the figure below:</p>
<div class="figure">
<img src="img/ci12.png" />

</div>
<p>Each of these <span class="math inline">\(t\)</span> values can be calculated, by finding the value of <span class="math inline">\(t\)</span> on the <span class="math inline">\(t\)</span>-distribution for the respective degrees of freedom that leaves 2.5% in the upper tail:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df =</span> <span class="dv">9</span>)</code></pre></div>
<pre><code>## [1] 2.262157</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df =</span> <span class="dv">19</span>)</code></pre></div>
<pre><code>## [1] 2.093024</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df =</span> <span class="dv">29</span>)</code></pre></div>
<pre><code>## [1] 2.04523</code></pre>
<p>So, as sample size increases, the value of <span class="math inline">\(t\)</span> decreases for a given confidence interval. This would seem to suggest that this would decrease the margin of error for the confidence interval. This is for the most part true, but not always. Remember the <span class="math inline">\(t\)</span> value is multiplied by the estimated standard deviation of the sampling distribution (the standard error) which is <span class="math inline">\(\frac{s}{\sqrt{n}}\)</span>. Now again, it looks like increasing <span class="math inline">\(n\)</span> would lead to a larger denominator and a smaller overall margin of error. This is also true. But, because we are using the sample standard deviation <span class="math inline">\(s\)</span> in the formula to estimate the standard error, then <span class="math inline">\(s\)</span> is going to vary from one sample to another. This means that for any given sample, we may actually end up with a tighter confidence interval even if we increase our sample size. However, the main point remains - generally increasing your sample size, will lead to a tighter confidence interval for a given CI range.</p>
<p>The final thing that is worth mentioning is a repeat of what is discussed above in the <span class="math inline">\(z\)</span>-distribution section. Increasing sample sizes also leads to sample means that will be, on average, much closer to the true population mean <span class="math inline">\(\mu\)</span> than you get when using smaller sample sizes.</p>
</div>
<div id="other-confidence-intervals-ranges-for-t-distribution" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Other Confidence Intervals ranges for t-distribution</h3>
<p>Like with the confidence intervals made with the <span class="math inline">\(z\)</span>-distribution, we can create confidence intervals for any range with the <span class="math inline">\(t\)</span>-distribution. The rationiale is the same. If we were to make an 80% CI around a sample mean, what we are effectively saying is that in 80% of all samples that we could collect, we would capture the true population mean <span class="math inline">\(\mu\)</span>. Practically, we use a different value of <span class="math inline">\(t\)</span> for each CI range. This value of <span class="math inline">\(t\)</span> will be the positive and negative value of the <span class="math inline">\(t\)</span>-distribution for a given degree of freedom that leaves the appropriate percentage in the middle of the distribution. For instance, for an 80% CI for a sample size of 25, which had degrees of freedom 24, the value would be <span class="math inline">\(t=1.32\)</span>.</p>
<p>We calculated this as follows: For an 80% CI, we wish to have 80% of the distribution in the middle (40% either side of our sample mean), leaving 20% in the tails - i.e. 10% in each tail. Therefore, we wish to know the value of <span class="math inline">\(t\)</span> that demarks this boundary. The easiest way to do that is to use the <code>qt()</code> function in R, and ask for the 90%th percentile (the value that leaves 10% in the upper tail) for a <span class="math inline">\(t\)</span> distribution of degrees of freedom = 24. We do that like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.90</span>, <span class="dt">df=</span><span class="dv">24</span>)</code></pre></div>
<pre><code>## [1] 1.317836</code></pre>
<div class="figure">
<img src="img/ci11.png" />

</div>
<p>Thus, if we had a sample mean of <span class="math inline">\(\overline{x}=15.52\)</span>, a sample standard deviation of <span class="math inline">\(s=3.3\)</span> and a sample size of <span class="math inline">\(n=25\)</span>, then our 80% confidence interval of the true population mean <span class="math inline">\(\mu\)</span> would be <span class="math inline">\(CI = 15.52[14.65, 16.39]\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">15.52</span> +<span class="st"> </span>(<span class="fl">1.32</span> *<span class="st"> </span>(<span class="fl">3.3</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>)))</code></pre></div>
<pre><code>## [1] 16.3912</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">15.52</span> -<span class="st"> </span>(<span class="fl">1.32</span> *<span class="st"> </span>(<span class="fl">3.3</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>)))</code></pre></div>
<pre><code>## [1] 14.6488</code></pre>
<p>The value of <span class="math inline">\(t\)</span> used in the confidence interval formula therefore changes based on both your confidence interval size, and your degrees of freedom. Below are some other values of <span class="math inline">\(t\)</span> that would be used for different sample sizes and CI ranges:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 99% CI, n = 20</span>
<span class="kw">qt</span>(.<span class="dv">995</span>, <span class="dt">df =</span> <span class="dv">19</span>)</code></pre></div>
<pre><code>## [1] 2.860935</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 90% CI, n = 12</span>
<span class="kw">qt</span>(.<span class="dv">95</span>, <span class="dt">df =</span> <span class="dv">11</span>)</code></pre></div>
<pre><code>## [1] 1.795885</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 99.9% CI, n = 35</span>
<span class="kw">qt</span>(.<span class="dv">9995</span>, <span class="dt">df =</span> <span class="dv">34</span>)</code></pre></div>
<pre><code>## [1] 3.600716</code></pre>
</div>
</div>
<div id="comparing-cis-using-the-z--and-t-distributions" class="section level2">
<h2><span class="header-section-number">8.5</span> Comparing CIs using the z- and t-distributions</h2>
<p>You might be thinking that using the <span class="math inline">\(t\)</span>-distribution to make 95% confidence intervals seems like a lot of extra legwork to figure out what value of <span class="math inline">\(t\)</span> to use, compared to just using <span class="math inline">\(z=1.96\)</span> when using the <span class="math inline">\(z\)</span>-distribution. This mini section hopefully is an illustration of why you have to do this.</p>
<p>These are the two formulas that we use to generate confidence intervals:</p>
<p><span class="math inline">\(CI_{95\%} = \overline{x} \pm z \times \frac{\sigma}{\sqrt{n}}\)</span></p>
<p><span class="math inline">\(CI_{95\%} = \overline{x} \pm t \times \frac{s}{\sqrt{n}}\)</span></p>
<p>But, what if we did just decide to use <span class="math inline">\(z\)</span> when we don't know the population standard deviation <span class="math inline">\(\sigma\)</span> and we used this formula:</p>
<p><span class="math inline">\(CI_{95\%} = \overline{x} \pm z \times \frac{s}{\sqrt{n}}\)</span></p>
<p>Well, in the figure below, we did just that for 25 sample means collected from samples of size <span class="math inline">\(n=8\)</span> from a population with a mean of <span class="math inline">\(\mu=4\)</span> and standard deviation of <span class="math inline">\(\sigma=1.5\)</span>. As you can see, the first and third columns that are using the appropriate <span class="math inline">\(z\)</span>- and <span class="math inline">\(t\)</span>-distribution formulas have 23/25 confidence interals that include the true population mean. In fact, out of 1000 simulations of these data (i.e. 1000 sample means collected) precisely 95% of confidence intervals included the population mean for both, which is what we would expect.</p>
<div class="figure">
<img src="img/ci13.png" />

</div>
<p>Conversely, the middle column include the confidence intervals calculated using <span class="math inline">\(z=1.96\)</span> and using <span class="math inline">\(s\)</span> as an estimate of the population standard deviation. With this formula, 6/25 confidence intervals fail to include the true population mean. Out of the 1000 simulations of the data, actually 9.X% of CIs failed to capture the true population mean. This shows that the margin of error calculated using this formula is consistently too small. The reason for this is that when we estimate the population standard deviation, we generally are under-estimating the true value. This is another reason why we should be using the <span class="math inline">\(t\)</span>-distribution.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="one-sample-inferential-statistics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
