[
["index.html", "PSY317L Guides 1 Welcome to PSY317! 1.1 What this book includes and what it doesn't 1.2 Acknowledgements 1.3 References 1.4 Other places to find help about R 1.5 Other places to find help about R and Statistics", " PSY317L Guides James P. Curley &amp; Tyler M. Milewski June 28, 2020 1 Welcome to PSY317! The following is a simple guide to R studio. What this guide contains.... That it's mainly a how to implement stuff in R, but it also touches on bits of stats and data theory where applicable 1.1 What this book includes and what it doesn't This is in between a textbook and a study guide. We're trying to build materials that will enable students to quickly find what they're looking for to help them understand these statistical concepts. This book is primarily aimed at the content in PSY317L (and PSY1XXR), but occasionally we describe concepts that we don't directly cover in these courses. This is when we feel that it's worth explaining things in a bit more detail for those students that want to know a bit more about a subject. 1.2 Acknowledgements This guidebook is built upon the work of several others' teaching materials. In particular, Danielle Navarro's book.... also some examples come from Andy Field's intro to statistics using R 1.3 References The following books are all freely avaiable online (like this one!) and are helpful resources wilke - visualizing wickham grolemund - intro R other wickham books? danielle navarro - intro stats 1.4 Other places to find help about R Rstudio - cheatsheets, community pages ??? 1.5 Other places to find help about R and Statistics With statistics stuff, you can never really have enough resources. Although throughout this course we try to explain difficult concepts in terms we hope all students can understand, sometimes you just need to read or hear about these concepts several times before they sink in. Othertimes, it's not until somebody else explains it in a slightly different way that it finally finds a home in your brain. To that end, we recommend looking over some other help tutorials and reading. You certainly don't have to go looking at these materials - but if you feel like hearing a different voice, then these are some that I recommend. Albert Kim stuff? Andrew Field's statistics in R book - probably the next level up from introduction in terms of R. Khan Academy videos.(not on R though) "],
["introduction.html", "2 Introduction 2.1 Introduction 2.2 Downloading R 2.3 Downloading RStudio 2.4 Using RCloud instead of RStudio 2.5 Importing Data", " 2 Introduction 2.1 Introduction This chapter details how to get started using R. The initial steps are downloading and installing R. This may differ slightly based on your machine. 2.2 Downloading R Go to the R website. The first box has options for downloading R for Mac, Windows or Linux. Click on the appropriate link. Then you will want to click on \\(\\color{blue}{\\text{&quot;install R for the first time&quot;}}\\). Then follow the instructions. 2.3 Downloading RStudio blah blah blah 2.4 Using RCloud instead of RStudio blah blah blah 2.5 Importing Data There are different options for importing data. It's possible to import data of all different formats into RStudio. We will primarily use spreadsheet type files that have been saved with the &quot;.csv&quot; suffix. These are called 'comma separated files'. Option 1. Import Tab You can click on the &quot;Import Dataset&quot; tab in the top right of RStudio - it's located just above your global environment. Tyler - perhaps we can insert images here? like this Of course, we should look into how to chaneg image sizes. You then need to click on the appropriate file type that you wish to import. As I mentioned, nearly all our files will be &quot;.csv&quot; files, so you should click on &quot;From CSV&quot;. In other versions of RStudio, this may say &quot;from text (readR)&quot; instead. blah blah blah I can't remember how to insert more space between sections, so I just write to try and force space. Option 2. Writing code. This is the option that we will use in our scripts for this course. You may notice that all the datasets that we wish to use are in a folder called &quot;data&quot;. To read in any of these datasets, what we need to do is use the read_csv() function. This comes from a package contained with the tidyverse package, so we must have imported that library first. We then tell it which dataset to find within the 'data' folder. We use the notation &quot;data/...&quot; to tell it to look inside the data folder. For instance, if we wished to load in the bmi.csv dataset, and assign it the name bmi, we would do it like this - make sure to put quotes around the whole file name and location: library(tidyverse) #load package ## -- Attaching packages --------------------------------------- tidyverse 1.2.1 -- ## v ggplot2 3.2.1 v purrr 0.2.4 ## v tibble 2.1.3 v dplyr 0.8.3 ## v tidyr 1.0.2 v stringr 1.3.1 ## v readr 1.1.1 v forcats 0.3.0 ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.3 ## Warning: package &#39;tibble&#39; was built under R version 3.5.3 ## Warning: package &#39;tidyr&#39; was built under R version 3.5.3 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.3 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() bmi &lt;- read_csv(&quot;data/bmi.csv&quot;) # bring in data ## Parsed with column specification: ## cols( ## id = col_integer(), ## age = col_double(), ## bmi = col_double(), ## chol = col_double(), ## insulin = col_double(), ## dbp = col_double(), ## sbp = col_double(), ## smoke = col_integer(), ## educ = col_character() ## ) head(bmi) # first six rows ## # A tibble: 6 x 9 ## id age bmi chol insulin dbp sbp smoke educ ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 18.3 15.4 47.2 2.78 29.5 49.7 1 D ## 2 2 19.4 14.5 52.1 3.47 31.3 49.0 2 C ## 3 3 21.1 16.0 52.2 4.06 32.4 51.7 1 B ## 4 4 21.3 19.5 53.2 4.48 32.0 NA 2 A ## 5 5 21.1 18.6 55.4 5.34 33.7 53.8 1 D ## 6 6 23.9 19.5 54.3 6.29 35.0 56.0 2 C Tyler.....Perhaps we should either suppress the warnings and messages (which you can do in the r chunk header in markdown)? Or we should explain what it means when it gets churned out. blah blah blah more stuff. "],
["basic-syntax.html", "3 Basic Syntax 3.1 boring mathematical stuff 3.2 assignment 3.3 vectors", " 3 Basic Syntax We kind of need to know at least a tiny bit of syntax before we move forward... keep it very simple. 3.1 boring mathematical stuff R is, first and foremost, a statistical programming language. So, unsurprisingly, you can do a bunch of mathematical operations fairly easily. Let's do some really tedious math: You can add with the + key: 4 + 6 ## [1] 10 You can see that the output returned 10 indicating that that is the answer to 4 + 6. However, we also see in the output a [1] before it. What does this mean? This is an index - which we'll talk about in more detail later- but essentially all it is telling us is that the number that comes directly after it is the first bit of the output. We only have one thing in our output here - the number 10, so it's pretty pointless - but it does have relevance in the future. For now though, you can safely ignore it. Subtracting uses a hyphen or minus key -: 43 - 18 ## [1] 25 Multiplying uses the asterisk *: 5 * 3 ## [1] 15 Dividing uses this slash key /: 34 / 7 ## [1] 4.857143 You can square root like so by typing sqrt() and putting your number to square root in the brackets: sqrt(45) ## [1] 6.708204 Squaring a number requires us to use the cute ^ key followed by a number 2 to indicate that we are raising to the power 2 (which is squaring): 12^2 ## [1] 144 Our first little warning Like all programming languages, R will only do what it's told. One thing that catches people out is that it will read code in order from left to right. So, if you're doing several things at once, you may need to insert brackets to make sure that it does what you want it to. For instance, say you want to multiply 5 by the sum of 7 and 9. You might write that like this: 5 * 7 + 9 ## [1] 44 but 44 is not the answer you're looking for! What it has done is to take 5, then multiply it by 7 (makes 35) and then add 9 to make 44. OK, how about you write it like this instead: 7 + 9 * 5 ## [1] 52 Erm, this time it multiplied 9 by 5 to get 45 and then added 7 to make 52. What is happening is that some mathematical operations are taking precedence over others and R does things in certain order. To explicitly make sure that it does what you want, insert some brackets like this: 5 * (7 + 9) ## [1] 80 Now it knows that what you wanted to do was to add 7 and 9 to get 16 and have that number multiplied by 5 to get 80. This sort of thing is something to look out for, but as you get experienced you'll have a sense for it. Well placed brackets can save you a headache. 3.2 assignment e.g. what the assignment operator is. This symbol &lt;- which is just a &quot;greater than sign&quot; followed by a &quot;hyphen&quot; is called an assignment operator. It basically is equivalent to saying you want to save something. You write what you want to be saved on the right hand side of it, and the name of your newly saved thing on the left of it. We call the 'thing' that you've saved an 'object' in programming speak. For instance, say you wanted to save the number 17, and you wanted to call that saved number an object called x. You'd do it like this: x &lt;- 17 Now, whenever you type, x, R thinks that it is the number 17. x ## [1] 17 We should probably fess up right now and tell you that there is another way that you can assign things in R. On the face of it, it's a much easier way too, but we're going to recommend you don't do it. The only reason that we're bringing it up at this point is that if you look up help on the internet or in some books, you'll see people doing it - so we should mention it. You can assign using the equal key = like this: y = 10 - 2 y ## [1] 8 So, as you can see, we created an object called y that was equal to 10 minus 2, that is 8. Using = seems so much easier than using &lt;- so what is the reason not to do it? Well, the equal sign gets used for a ton of other commands and sometimes it gets a little messy and confusing if too many equal signs fly around. Thefore, we're going to politely ask that whenever you assign things, please use the &lt;- operator, even though it's two bits of punctuation stuck together: hooray &lt;- 17 + 4 # thanks for using this sign hooray ## [1] 21 3.3 vectors Of course, you can save even more complex things as the object. For example, if you wished to save the numbers 5, 6, 7, 8, 9, 10, you have two ways of doing that. Let's just see them in action, and then we'll explain the syntax: v1 &lt;- 5:10 v1 ## [1] 5 6 7 8 9 10 v2 &lt;- c(5,6,7,8,9,10) v2 ## [1] 5 6 7 8 9 10 As you can see both v1 and v2 are our newly saved objects and they both contain the numbers 5 through 10. For v1 we separated the numbers 5 and 10 with a colon :. In R, the : sign can be used to mean &quot;to&quot; when talking about a series of numbers. e.g. 5:10 ## [1] 5 6 7 8 9 10 101:103 ## [1] 101 102 103 The other way we did it with v2 was to use the c() function. This stands for concatenate which is a mouthful. Basically, it's a way of sticking things together. You write c() and then put the stuff you want to stick together inside the brackets, but make sure you separate each thing by a comma ,. For example, c(1,10,100,1000) ## [1] 1 10 100 1000 c(4,6,8,10,8,6,1) ## [1] 4 6 8 10 8 6 1 Another bit of terminology might be worth mentioning right now. Our saved objects x, v1 and v2, as well as being called objects can also be called vectors. A vector is something in R that contains several items. These items are actually called elements. Importantly the items are indexed, which means that they are in order, not all jumbled up. That means that you can directly grab an element by it's position. So, if you wished to get the first element (or item) of the object v1 you'd type like this: v1[1] ## [1] 5 That returns '5' which was our first element we put into the vector. If we wish to get the 3rd element of v1 we'd do this: v1[3] ## [1] 7 And, if we wished to get the first, second and third element of v1 we'd do this: v1[1:3] ## [1] 5 6 7 blah blah blah summary image does the image appear above? I just remembered that if you don't want the image to appear inline, then you leave out the ! from the markdown syntax. summary image Now, one of the great advantages of vectors is that you can do things to all of the numbers inside the vectors at once. For instance, say we wanted to square all the numbers inside of v1, then all we need to do is: v1^2 ## [1] 25 36 49 64 81 100 and notice how it squared the numbers 5 through 10 that are the contents of v1. You can also do things to the entire contents of a vector at once. For instance, if you wanted to add up all the numbers in v1 you could do this by using the function sum() like this: sum(v1) ## [1] 45 and it added 5+6+7+8+9+10 to give one answer! "],
["introduction-to-data-carpentry.html", "4 Introduction to Data Carpentry 4.1 tidyverse 4.2 filter() 4.3 select() 4.4 arrange() 4.5 mutate() 4.6 Wide vs Long Data 4.7 Joins", " 4 Introduction to Data Carpentry Data carpentry gives us the tools to work with large data sets. During this chapter you will learn basic concepts, skills to help you work with functions to help tidy and manage your data. check check 4.1 tidyverse The package tidyverse is what we will be using for most of our data carpentry. tidyverse is a larger package which includes several packages useful for dealing with data such as tidyr, dplyr, ggplot2, and more. All of these packages work together to manipulate, reshape, and visual data. Before using tidyverse you will need to install and load the package. You will only need to install a package once, but will have to load a library on every script you plan to use it. install.package(tidyverse) #install From the tidyverse package rread we can read in our data. library(tidyverse) #load df &lt;- read_csv(&quot;data/bloodwork.csv&quot;) ## Parsed with column specification: ## cols( ## ids = col_character(), ## age = col_integer(), ## sex = col_character(), ## state = col_character(), ## children = col_integer(), ## smoker = col_character(), ## hrate = col_integer(), ## bpsyst = col_integer(), ## cellcount = col_double(), ## immuncount = col_double(), ## immuncount2 = col_double() ## ) df is the name of our new dataframe, which will include all data from the bloodwork.csv file. Before we can work with our data we need to what we have to work with using the functions head() and tail() allows you to see the first and last 6 rows, respectfully. head(df) ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 ## 3 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 4 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 5 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 6 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; tail(df) ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JIB 66 fema~ NY 0 no 62 121 0.097 1.39 ## 2 HBB 67 fema~ NJ 1 yes 74 147 0.288 2.27 ## 3 HDG 68 fema~ NY 3 yes 65 129 0.11 2.65 ## 4 ECD 68 fema~ NJ 2 yes 77 129 0.404 2.02 ## 5 HHJ 69 male CT 2 no 71 121 0.475 0.463 ## 6 CCG 70 male CT 0 yes 80 132 0.078 1.06 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; should we explain the dataset?? idk 4.2 filter() Use %&gt;% and 'filter' to only keep rows where the hrate, heart rate, is over 85. &gt; means greater than. df %&gt;% filter(hrate &gt; 85) ## # A tibble: 0 x 11 ## # ... with 11 variables: ids &lt;chr&gt;, age &lt;int&gt;, sex &lt;chr&gt;, state &lt;chr&gt;, ## # children &lt;int&gt;, smoker &lt;chr&gt;, hrate &lt;int&gt;, bpsyst &lt;int&gt;, cellcount &lt;dbl&gt;, ## # immuncount &lt;dbl&gt;, immuncount2 &lt;dbl&gt; Or use %&gt;% and 'filter' to only keep rows where the sex is equal to female. df %&gt;% filter(sex == &quot;female&quot;) ## # A tibble: 18 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 3 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 4 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 5 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## 6 IEE 26 fema~ NY 2 no 71 118 0.093 5.41 ## 7 BED 28 fema~ CT 0 no 62 104 0.082 1.18 ## 8 BFB 28 fema~ NJ 0 no 68 118 0.197 0.724 ## 9 IEA 29 fema~ CT 0 yes 74 117 0.429 5.15 ## 10 CDC 38 fema~ NJ 0 no 66 133 0.038 8.00 ## 11 FJG 40 fema~ NJ 0 yes 80 109 0.253 5.63 ## 12 DAG 41 fema~ CT 0 yes 70 142 0.339 5.52 ## 13 FHA 53 fema~ CT 2 no 77 125 0.099 0.034 ## 14 JHC 55 fema~ CT 0 no 73 121 0.093 1.32 ## 15 JIB 66 fema~ NY 0 no 62 121 0.097 1.39 ## 16 HBB 67 fema~ NJ 1 yes 74 147 0.288 2.27 ## 17 HDG 68 fema~ NY 3 yes 65 129 0.11 2.65 ## 18 ECD 68 fema~ NJ 2 yes 77 129 0.404 2.02 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; You can create new datasets from filtered data. female &lt;- df %&gt;% filter(sex == &quot;female&quot;) 4.3 select() Use %&gt;% then select to just select the rows you want. df %&gt;% select(ids, sex, smoker,hrate) ## # A tibble: 30 x 4 ## ids sex smoker hrate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 JEC female no 63 ## 2 GBH male yes 73 ## 3 EDH female no 65 ## 4 AAA female no 66 ## 5 AJF female no 67 ## 6 FJC female yes 80 ## 7 IEE female no 71 ## 8 BED female no 62 ## 9 BFB female no 68 ## 10 IEA female yes 74 ## # ... with 20 more rows Or if you just want to get rid of one column you can just use %&gt;% then select(-var) df %&gt;% select(-bpsyst) ## # A tibble: 30 x 10 ## ids age sex state children smoker hrate cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 0.126 0.993 ## 2 GBH 20 male NY 1 yes 73 0.169 1.18 ## 3 EDH 21 fema~ NJ 0 no 65 0.281 4.34 ## 4 AAA 21 fema~ CT 3 no 66 0.244 2.56 ## 5 AJF 24 fema~ NJ 0 no 67 0.092 6.45 ## 6 FJC 25 fema~ NY 1 yes 80 0.014 3.97 ## 7 IEE 26 fema~ NY 2 no 71 0.093 5.41 ## 8 BED 28 fema~ CT 0 no 62 0.082 1.18 ## 9 BFB 28 fema~ NJ 0 no 68 0.197 0.724 ## 10 IEA 29 fema~ CT 0 yes 74 0.429 5.15 ## # ... with 20 more rows, and 1 more variable: immuncount2 &lt;dbl&gt; if you want these selections to be permanent then you need to rewrite selections in new dataframe. df1 &lt;- df %&gt;% select(-bpsyst) 4.4 arrange() blah 4.5 mutate() Create new columns using %&gt;% then mutate df2 &lt;- df %&gt;% mutate(totalimmune = immuncount + immuncount2) Now df2 will have 12 columns, instead of 11. 4.6 Wide vs Long Data blah pivot_wide() pivot_long() 4.7 Joins really important.... maybe one day we'll write about this "],
["data-visualization.html", "5 Data Visualization 5.1 Histogram 5.2 Scatter 5.3 Boxplot 5.4 Adding details", " 5 Data Visualization The package ggplot2 is used to visualize data. Before starting you should load the tidyverse package, which includes ggplot2 and other tools, at the very top of your script. See example below. You need to do it once for every script. library(tidyverse) Read in data covid &lt;- read_csv(&quot;data/countycovid.csv&quot;) ## Parsed with column specification: ## cols( ## day = col_integer(), ## date = col_date(format = &quot;&quot;), ## county = col_character(), ## population = col_integer(), ## total = col_integer() ## ) head(covid) ## # A tibble: 6 x 5 ## day date county population total ## &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 0 2020-03-04 Dallas 2734111 0 ## 2 0 2020-03-04 Harris 4978845 0 ## 3 0 2020-03-04 Travis 1291502 0 ## 4 1 2020-03-05 Dallas 2734111 0 ## 5 1 2020-03-05 Harris 4978845 0 ## 6 1 2020-03-05 Travis 1291502 0 tail(covid) ## # A tibble: 6 x 5 ## day date county population total ## &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 92 2020-06-04 Dallas 2734111 249 ## 2 92 2020-06-04 Harris 4978845 241 ## 3 92 2020-06-04 Travis 1291502 95 ## 4 93 2020-06-05 Dallas 2734111 250 ## 5 93 2020-06-05 Harris 4978845 247 ## 6 93 2020-06-05 Travis 1291502 95 5.1 Histogram 5.2 Scatter 5.3 Boxplot 5.4 Adding details "],
["descriptives.html", "6 Descriptives 6.1 Basic Descriptives 6.2 Mean, Median, and Mode 6.3 Standard Deviation 6.4 Inter-quartile Ranges 6.5 Finding", " 6 Descriptives Descriptive statistics describe basic features of the data in simple summaries such as mean, median, and mode. These statistics used to present quantitative descriptions of data in graphing. You can use functions in base R and tidyverse to get descriptive statistics. ## Parsed with column specification: ## cols( ## month = col_integer(), ## day = col_integer(), ## year = col_integer(), ## temp = col_double() ## ) ## # A tibble: 6 x 4 ## month day year temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 2019 43.3 ## 2 1 2 2019 39.4 ## 3 1 3 2019 41.2 ## 4 1 4 2019 44.1 ## 5 1 5 2019 48.6 ## 6 1 6 2019 48.8 Focus in the temperature column for now ggplot(atx, aes(x= temp)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;, binwidth = 10)+ theme_classic() atx$temp ## [1] 43.3 39.4 41.2 44.1 48.6 48.8 67.5 63.8 57.6 49.7 52.7 57.5 45.8 44.0 48.6 ## [16] 50.5 62.1 57.3 57.7 42.3 44.2 61.5 48.1 42.3 47.0 51.6 55.7 52.4 47.7 45.2 ## [31] 50.0 60.1 62.3 65.9 69.4 70.8 71.8 68.0 38.8 36.9 41.2 53.5 59.4 45.9 59.8 ## [46] 65.3 55.0 55.4 51.6 45.2 47.9 50.9 53.1 61.6 54.9 50.9 58.2 60.8 49.7 46.2 ## [61] 51.6 46.4 35.1 35.8 40.5 58.2 69.7 75.5 66.0 64.0 64.9 70.9 71.4 56.9 52.1 ## [76] 54.4 53.8 56.1 59.1 62.4 59.8 67.6 70.6 73.3 64.6 57.2 64.4 68.6 67.0 51.6 ## [91] 48.3 52.0 61.4 73.4 70.3 71.5 68.1 71.5 64.2 73.0 72.7 64.8 68.4 57.4 63.0 ## [106] 70.7 74.8 69.6 66.4 62.3 68.7 73.4 70.9 67.5 67.9 70.7 69.6 73.3 75.3 78.6 ## [121] 79.4 74.7 69.9 69.8 72.5 72.4 71.0 73.2 78.5 63.9 62.7 68.9 69.2 73.0 75.2 ## [136] 75.5 80.9 81.0 79.6 83.8 80.3 82.2 84.9 84.0 82.6 82.4 80.7 81.7 84.3 79.3 ## [151] 80.4 82.7 83.5 83.2 80.9 80.1 78.7 81.4 83.4 84.3 81.0 76.3 75.5 79.7 80.0 ## [166] 84.2 85.7 79.4 78.0 86.1 87.1 86.8 84.8 86.2 78.6 77.8 78.8 82.3 80.0 82.6 ## [181] 77.7 81.1 82.0 80.5 82.1 85.2 84.3 85.4 85.4 83.2 86.8 85.2 85.8 84.2 86.3 ## [196] 84.9 84.4 86.2 85.9 86.1 86.5 85.1 86.1 84.6 80.0 74.7 76.6 79.0 83.7 83.3 ## [211] 85.5 85.8 84.3 84.7 84.2 82.3 83.7 85.5 85.7 87.2 87.7 86.8 86.0 87.9 87.6 ## [226] 88.1 85.3 89.0 89.2 88.9 88.8 88.7 86.7 87.1 86.9 84.1 84.5 86.4 87.2 86.0 ## [241] 86.9 86.1 84.1 84.9 84.5 84.6 85.8 85.5 87.5 87.4 85.7 85.1 82.0 79.0 81.7 ## [256] 83.3 83.9 83.6 84.8 82.2 83.7 84.9 84.2 85.1 84.1 85.6 83.7 83.3 83.9 82.7 ## [271] 84.5 85.6 84.8 84.8 82.8 82.9 81.6 81.0 80.7 78.0 70.3 75.1 84.8 65.7 58.9 ## [286] 64.3 75.5 83.5 70.8 62.5 61.9 74.7 76.4 76.3 62.9 63.6 73.6 53.6 56.5 57.3 ## [301] 64.4 60.1 49.2 44.7 42.3 49.8 52.2 63.0 63.6 74.6 64.6 46.5 50.8 58.1 64.0 ## [316] 36.3 34.5 42.1 43.5 45.7 51.3 54.4 57.0 69.2 74.0 63.8 52.0 52.9 63.1 70.1 ## [331] 60.5 53.8 53.8 76.7 61.5 48.4 49.2 57.4 56.1 62.5 56.1 55.0 68.7 53.2 44.6 ## [346] 43.6 53.4 53.9 57.2 60.3 44.2 40.0 37.8 47.6 52.0 44.4 46.1 47.5 54.2 61.0 ## [361] 59.5 64.6 57.8 49.5 43.9 6.1 Basic Descriptives length(atx$temp) # length this tells you the &#39;n&#39; ## [1] 365 sum(atx$temp) # sum ## [1] 25107.5 range(atx$temp) # range ## [1] 34.5 89.2 min(atx$temp) # minimum ## [1] 34.5 max(atx$temp) # maximum ## [1] 89.2 var(atx$temp) # variance ## [1] 222.2072 6.2 Mean, Median, and Mode mean(atx$temp) # mean ## [1] 68.78767 sum(atx$temp)/length(atx$temp) # mean ## [1] 68.78767 median(atx$temp) # median ## [1] 70.8 #estimate mode function estimate_mode &lt;- function(x) { d &lt;- density(x) d$x[which.max(d$y)] } estimate_mode(atx$temp) #estimate mode ## [1] 83.69078 6.3 Standard Deviation sd(atx$temp) # sample standard deviation ## [1] 14.90662 #population standard deviation function pop.sd &lt;- function(s) { sqrt(sum((s - mean(s))^2)/length(s)) } pop.sd(atx$temp) # population standard deviation ## [1] 14.88618 6.4 Inter-quartile Ranges quantile(atx$temp, .25) # this is the lower quartile ## 25% ## 56.5 quantile(atx$temp, .75) # this is the upper quartile ## 75% ## 83.3 IQR(atx$temp) # this is the inter-quartile range. ## [1] 26.8 these quartiles may be slightly different to what you'd get by hand. ggplot(atx, aes(y=temp)) + geom_boxplot(color=&#39;black&#39;, fill=&#39;lightseagreen&#39;) nb we get those numbers on the x-axis because there is no 'group' we can get rid of them like this: ggplot(atx, aes(y=temp)) + geom_boxplot(color=&#39;black&#39;, fill=&#39;lightseagreen&#39;) + scale_x_discrete(breaks = NULL) 6.5 Finding "],
["distributions.html", "7 Distributions", " 7 Distributions blah "],
["confidence-intervals.html", "8 Confidence Intervals", " 8 Confidence Intervals blah "],
["inferential-stats.html", "9 Inferential Stats 9.1 Comparing two Samples 9.2 Independent Samples t-test 9.3 Background to Student's 2 Sample t-test 9.4 Sampling Distribution of the Difference in Sample Means 9.5 Pooled Standard Deviation 9.6 Confidence Interval for Difference in Means 9.7 Conducting Student t-test 9.8 Doing Student t-test in R 9.9 Effect Sizes 9.10 Paired t-tests", " 9 Inferential Stats blah 9.1 Comparing two Samples blah compare 2 distributions blah compare 2 distributions blah 9.2 Independent Samples t-test types 9.3 Background to Student's 2 Sample t-test blah 9.4 Sampling Distribution of the Difference in Sample Means blah 9.5 Pooled Standard Deviation blah 9.6 Confidence Interval for Difference in Means blah 9.7 Conducting Student t-test blah 9.8 Doing Student t-test in R blah 9.9 Effect Sizes Just because you observe a “significant” difference in means between two groups doesn’t mean that it’s interesting or relevant…. i.e. being ‘significantly different’ doesn’t tell you how BIG the difference is – i.e. how LARGE the effect size is. The formula for Cohen's \\(\\delta\\) is as follows: \\(\\Huge \\delta = \\frac{\\overline{X}_{1} - \\overline{X}_{2}}{\\hat{\\sigma}_{\\rho}}\\) 9.10 Paired t-tests more sections to add... "],
["correlation.html", "10 Correlation 10.1 Pearson Correlation 10.2 Calculating the Pearson Correlation in R 10.3 Cross-products 10.4 Conducting a Pearson Correlation Test 10.5 Assumptions of Pearson's Correlation 10.6 Confidence Intervals for R 10.7 Partial Correlations 10.8 Non-parametric Correlations 10.9 Point-Biserial Correlation", " 10 Correlation intro lines about what correlation is etc... 10.1 Pearson Correlation Pearson's correlation is measured by r and ranges between -1 and +1. +1 indicates that the variables X and Y are maximally positively correlated, such that as values of X increase so do values of Y. -1 indicates a compleltely negative correlation such that as values of X increase, values of Y decrease. A value of 0 indicates that there is no overall relationship. insert figure of negative 0.6, 0 and positive 0.6 here The below image shows scatterplots, each with a sample size of 30. The trendline is to help demonstrate how correlations of different magnitudes look in terms of their association. Correlations 10.2 Calculating the Pearson Correlation in R To calculate the correlation coefficient in R, it's pretty straightforward. You simply can use the cor() function. For instance, let's correlate... cor(1:10,1:10) # replace with an example ## [1] 1 Before we go further into what we should do with these correlations, and how to signficance test them, let's learn a little bit more about how they come about. 10.3 Cross-products The formula for calculating the Pearson's correlation coefficient for a sample is: \\(r = \\frac{\\sum_{}^{} z_{x}z_{y}}{n - 1}\\) When we have a population, we can use the formula: \\(r = \\frac{\\sum_{}^{} z_{x}z_{y}}{N}\\) Essentially, the steps are to convert all the X and Y scores into their respective z-scores. Then you mutliply these two values together to get the cross-product. After summing up all the cross-products for each data point, we divide this number by n-1 if we're dealing with a sample (we usually are), or N if we're dealing with a population. The sum of the cross-products will therefore be largely positive if positive z-scores are multiple together or if negative z-scores are multiplied together. The sum of the cross-products will be largely negative if negative z-scores are multipled with positive z-scores. The following example should help make this clearer. Look at the following data, its scatterplot and the correlation coefficient. They show that we have a positive correlation of r=0.84. Let's break it down how we got that value. x &lt;- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9) y &lt;- c(2.8, 2.9, 1.6, 5.5, 4.7, 8.1, 3.3, 7.7, 7.1, 5.8) df &lt;- data.frame(x, y) df ## x y ## 1 1.1 2.8 ## 2 1.5 2.9 ## 3 2.1 1.6 ## 4 3.5 5.5 ## 5 3.6 4.7 ## 6 3.5 8.1 ## 7 2.6 3.3 ## 8 5.6 7.7 ## 9 4.4 7.1 ## 10 3.9 5.8 ggplot(df, aes(x = x, y = y)) + geom_point(size=2) cor(x,y) ## [1] 0.8418262 First, let's calculate the means and standard deviation (using sd so a sample standard deviation) of x and y. We need to get these values so we can calculate the z-scores of each. # step 1: Get the mean and sd of x and y mean(x) ## [1] 3.18 sd(x) ## [1] 1.370158 mean(y) ## [1] 4.95 sd(y) ## [1] 2.259916 Now, we can calculate the z-scores, remembering that the formula for that is: \\(z = \\frac{x - \\overline{x}}{s_{x}}\\) # step 2. Calculate z-scores of x, and z-scores of y. df$zx &lt;- (x - mean(x)) / sd(x) # z scores of x df$zy &lt;- (y - mean(y)) / sd(y) # z scores of y df ## x y zx zy ## 1 1.1 2.8 -1.5180729 -0.9513626 ## 2 1.5 2.9 -1.2261358 -0.9071132 ## 3 2.1 1.6 -0.7882302 -1.4823557 ## 4 3.5 5.5 0.2335497 0.2433718 ## 5 3.6 4.7 0.3065340 -0.1106236 ## 6 3.5 8.1 0.2335497 1.3938569 ## 7 2.6 3.3 -0.4233088 -0.7301155 ## 8 5.6 7.7 1.7662195 1.2168592 ## 9 4.4 7.1 0.8904082 0.9513626 ## 10 3.9 5.8 0.5254868 0.3761201 Following this, we simply multiple the z-scores of x and y against each other for every data point: # step 3. Calculate the cross-product: zx * zy df$zxzy &lt;- df$zx * df$zy df ## x y zx zy zxzy ## 1 1.1 2.8 -1.5180729 -0.9513626 1.44423785 ## 2 1.5 2.9 -1.2261358 -0.9071132 1.11224399 ## 3 2.1 1.6 -0.7882302 -1.4823557 1.16843751 ## 4 3.5 5.5 0.2335497 0.2433718 0.05683941 ## 5 3.6 4.7 0.3065340 -0.1106236 -0.03390988 ## 6 3.5 8.1 0.2335497 1.3938569 0.32553483 ## 7 2.6 3.3 -0.4233088 -0.7301155 0.30906432 ## 8 5.6 7.7 1.7662195 1.2168592 2.14924036 ## 9 4.4 7.1 0.8904082 0.9513626 0.84710104 ## 10 3.9 5.8 0.5254868 0.3761201 0.19764615 We now have all of our cross-products. Notice why the majority are positive. This is because we have multiplied positive \\(z_{x}\\) with positive \\(z_{y}\\) or we multiplied negative \\(z_{x}\\) with negative \\(z_{y}\\). This happens because datapoints that tend to be above the mean for x are also above the mean for y, and points that are below the mean of x are also below the mean of y. We can add this up to get the sum of the cross-products. That is the \\(\\sum_{}^{} z_{x}z_{y}\\) in the formula. # step 4. Sum up the cross products. sum(df$zxzy) # 7.58 ## [1] 7.576436 We now divide that by n-1 as we have a sample, to get the correlation coefficient r. That gives us an estimation of the average cross-product. # step 5- calculate &#39;r&#39; by dividing by n-1. (for a sample) sum(df$zxzy) / 9 # our n was 10, so n-1 = 9 ## [1] 0.8418262 sum(df$zxzy) / (nrow(df) - 1) # nrow(df) is more generalizable ## [1] 0.8418262 # r=0.84 Just as a quick second example, here is a work through calculating a negative correlation. Notice the \\(z_{x}\\) and \\(z_{y}\\) scores that are multiplied together. They are largely opposite in terms of signs. This is what leads to a negative sum of cross-products and the negative correlation. Why? Because data points that are above the mean for x are generally below the mean in terms of y and visa-versa. ### Example 2. Negative Correlation. x &lt;- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9) y &lt;- c(10.4, 10.0, 8.4, 8.5, 8.4, 6.3, 7.1, 6.2, 8.1, 10.0) df &lt;- data.frame(x, y) ggplot(df, aes(x = x, y = y)) + geom_point(size=2) cor(df$x,df$y) ## [1] -0.6112965 Here is the code, truncated for space: # Calculate z-scores for each x and each y df$zx &lt;- (x - mean(x)) / sd(x) df$zy &lt;- (y - mean(y)) / sd(y) # Calculate the cross-product: zx * zy df$zxzy &lt;- df$zx * df$zy # let&#39;s look at the dataframe # notice the cross products: df ## x y zx zy zxzy ## 1 1.1 10.4 -1.5180729 1.37762597 -2.09133671 ## 2 1.5 10.0 -1.2261358 1.11012578 -1.36116500 ## 3 2.1 8.4 -0.7882302 0.04012503 -0.03162776 ## 4 3.5 8.5 0.2335497 0.10700008 0.02498983 ## 5 3.6 8.4 0.3065340 0.04012503 0.01229968 ## 6 3.5 6.3 0.2335497 -1.36425096 -0.31862038 ## 7 2.6 7.1 -0.4233088 -0.82925058 0.35102907 ## 8 5.6 6.2 1.7662195 -1.43112601 -2.52768263 ## 9 4.4 8.1 0.8904082 -0.16050011 -0.14291061 ## 10 3.9 10.0 0.5254868 1.11012578 0.58335643 # Sum up the cross products and Calculate &#39;r&#39; by dividing by N-1. sum(df$zxzy) / (nrow(df) - 1) ## [1] -0.6112965 cor(df$x,df$y) ## [1] -0.6112965 10.4 Conducting a Pearson Correlation Test Although cor() gives you the correlation between two continuous variables, to actually run a significance test, you need to use cor.test(). Let's use some BlueJay data to do this. We'll just use data on male birds. library(tidyverse) jays &lt;- read_csv(&quot;data/BlueJays.csv&quot;) ## Parsed with column specification: ## cols( ## BirdID = col_character(), ## KnownSex = col_character(), ## BillDepth = col_double(), ## BillWidth = col_double(), ## BillLength = col_double(), ## Head = col_double(), ## Mass = col_double(), ## Skull = col_double(), ## Sex = col_integer() ## ) jayM &lt;- jays %&gt;% filter(KnownSex == &quot;M&quot;) # we&#39;ll just look at Males nrow(jayM) # 63 observations ## [1] 63 head(jayM) ## # A tibble: 6 x 9 ## BirdID KnownSex BillDepth BillWidth BillLength Head Mass Skull Sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0000-00000 M 8.26 9.21 25.9 56.6 73.3 30.7 1 ## 2 1142-05901 M 8.54 8.76 25.0 56.4 75.1 31.4 1 ## 3 1142-05905 M 8.39 8.78 26.1 57.3 70.2 31.2 1 ## 4 1142-05909 M 8.71 9.84 25.5 57.3 74.9 31.8 1 ## 5 1142-05912 M 8.74 9.28 25.4 57.1 75.1 31.8 1 ## 6 1142-05914 M 8.72 9.94 30 60.7 78.1 30.7 1 Let's say you're interested in examining whether there is an association between Body Mass and Head Size. First we'll make a scatterplot between the Mass and Head columns. We'll also investigate the correlation using cor(). ggplot(jayM, aes(x=Mass, y=Head)) + geom_point(shape = 21, colour = &quot;navy&quot;, fill = &quot;dodgerblue&quot;) + stat_smooth(method=&quot;lm&quot;, se=F) cor(jayM$Mass, jayM$Head) # r = 0.58, a strong positive correlation. ## [1] 0.5773562 To run the significance test, we do the following: cor.test(jayM$Head, jayM$Mass) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 7.282e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3846090 0.7218601 ## sample estimates: ## cor ## 0.5773562 This gives us a lot of information. Firstly, at the bottom it repeats the correlation coefficient cor. At the top, it gives us the value of t which is essentially how surprising it is for us to get the correlation we did assuming we were drawing our sample from a population where there is no correlation. Associated with this t value is the degrees of freedom which is equal to n-2, so in this case that is 63-2 = 61. The p-value is also given. If we are using alpha=0.05 as our significance level, then we can reject the hypothesis that there is no overall correlation in the population between Body Mass and Head size if p&lt;0.05. The default for cor.test() is to do a two-tailed test. This is testing whether your observed correlation r is different from r=0 in either the positive or negative direction. This default version also gives us the confidence interval for the correlation coefficient. Essentially, this gives us the interval in which we have a 95% confidence that the true population r lies (remember we just have data from one sample that theoretically comes from a population). It's also possible however that you had an a priori prediction about the direction of the effect. For instance, you may have predicted that Body Mass would be positively correlated with Head Size. In this case, you could do a one-tailed correlation test, where your alternative hypothesis is that there is a positive correlation and the null is that the correlation coefficient is equal to 0 or less than 0. To do one-tailed tests you need to add the alternative argument. # testing if there is a positive correlation cor.test(jayM$Head, jayM$Mass, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 3.641e-07 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.4187194 1.0000000 ## sample estimates: ## cor ## 0.5773562 # testing if there is a negative correlation cor.test(jayM$Head, jayM$Mass, alternative = &quot;less&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 1 ## alternative hypothesis: true correlation is less than 0 ## 95 percent confidence interval: ## -1.0000000 0.7017994 ## sample estimates: ## cor ## 0.5773562 10.5 Assumptions of Pearson's Correlation The Pearson Correlation Coefficient requires your data to be approximately normally distributed. To do this we have various options how to test for normality. Firstly, we could do a Shapiro-Wilk test, which formally determines whether our data are normal. This is done using shapiro.test(), where we assume our data are from a normal population if the resulting p-value is above 0.05. If the p-value is below 0.05 then we have evidence to reject that our data come from a normal population. With our data above, this would look like this when running the test on each variable: shapiro.test(jayM$Mass) # P &gt; 0.05, therefore cannot reject null that data is not normal ## ## Shapiro-Wilk normality test ## ## data: jayM$Mass ## W = 0.97222, p-value = 0.1647 shapiro.test(jayM$Head) # P &gt; 0.05, therefore cannot reject null that data is not normal ## ## Shapiro-Wilk normality test ## ## data: jayM$Head ## W = 0.96521, p-value = 0.07189 We can also make a QQ-plot for each variable. Essentially what we require from this plot is for the majority of our data to fall on the straight line - especially the datapoints in the middle. Some deviation at the tails is ok. This plot orders our data and plots the observed data against values on the x-axis that we would expect to get if our data was truly from a normal population. qqnorm(jayM$Mass) qqline(jayM$Mass, col = &quot;steelblue&quot;, lwd = 2) qqnorm(jayM$Head) qqline(jayM$Head, col = &quot;steelblue&quot;, lwd = 2) Both of these QQ plots are ok, and indicate normality, as does our Shapiro-Wilk tests. Therefore we would be ok to use a Pearson Correlation test with these data. What should you do though if either of your continuous variables are not approximately normally distributed? In that case, there are other correlation coefficients and associated significance tests that you could run instead. We describe these in more detail in Section x.xxx 10.6 Confidence Intervals for R bit more on this and the theory. 10.7 Partial Correlations why.... the stupid formula.... and how to do in R.... and that there are technically better ways... 10.8 Non-parametric Correlations examples.... when I can be bothered, edit this code chunk down into words + code When at least on of our variables are not normal, then we need to consider alternative approaches to the Pearson correlation for assessing correlations. Let's take this example, where we are interested in seeing if there's an association between saturated fat and cholesterol levels across a bunch of different cheeses: library(tidyverse) cheese &lt;- read_csv(&quot;data/cheese.csv&quot;) ## Parsed with column specification: ## cols( ## type = col_character(), ## sat_fat = col_double(), ## polysat_fat = col_double(), ## monosat_fat = col_double(), ## protein = col_double(), ## carb = col_double(), ## chol = col_integer(), ## fiber = col_double(), ## kcal = col_integer() ## ) head(cheese) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 # let&#39;s make a scatterplot of saturated fat against cholesterol ggplot(cheese, aes(x = sat_fat, y = chol)) + geom_point() + stat_smooth(method=&quot;lm&quot;,se=F) It looks like there is a pretty obvious relationship, but let's check the normality of each variable before progressing. Firstly the Shapiro-Wilk tests suggest that our data do not come from a normal distribution: shapiro.test(cheese$sat_fat) # P &lt; 0.05, therefore data may not be normal ## ## Shapiro-Wilk normality test ## ## data: cheese$sat_fat ## W = 0.85494, p-value = 6.28e-07 shapiro.test(cheese$chol) # P &lt; 0.05, therefore data may not be normal ## ## Shapiro-Wilk normality test ## ## data: cheese$chol ## W = 0.90099, p-value = 2.985e-05 Secondly, we have quite dramatic deviation from the straight line of our datapoints in our QQ plots. This indicates that our dat are likley skewed. qqnorm(cheese$sat_fat) qqline(cheese$sat_fat, col = &quot;steelblue&quot;, lwd = 2) qqnorm(cheese$chol) qqline(cheese$chol, col = &quot;steelblue&quot;, lwd = 2) We could be thorough and check this by plotting histograms of our data: library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine p1 &lt;- ggplot(cheese, aes(x=sat_fat)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;) p2 &lt;- ggplot(cheese, aes(x=chol)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;) grid.arrange(p1,p2,nrow=1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Because our data do not appear to be normal, we cannot do a Pearson correlation coefficient. We should instead use a non-parametric correlation method. There are several of these to choose from. We don't plan to go into the details here of how these methods determine their correlation coefficients or conduct significance test. In brief, these methods generally rank order the datapoints along the x and y axes and then determine how ordered these ranks are with respect to each other. Probably the most commonly used non-parametric correlation test is called the Spearman Rank Correlation test. To run this, we can use cor() to get the correlation or cor.test() to run the signficance test in the same way we did the Pearson test. However, the difference here is that we specify method=&quot;spearman&quot; at the end. cor(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;) ## [1] 0.8677042 cor.test(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;) ## Warning in cor.test.default(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;): ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: cheese$sat_fat and cheese$chol ## S = 8575.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8677042 The correlation coefficient here is 0.87 and is termed rho instead of r. With the significance test, you get a test-statistic S which relates to how well ordered the ranked data are. Also provided is a p-value. As with the Pearson, the default is a 2-tailed test, testing whether the obsvered correlation could have come from a population with a correlation of 0. If the p-value is below 0.05 (using alpha = 0.05 as our criterion), then that is reasonable evidence that there is a significant correlation. You may also notice with Spearman Rank correlations that you are forever getting warnings about computing p-values with ties. Don't worry at all about this - although this is an issue with the test and how it calculates the p-value, it isn't of any real practical concern. If you were interested in conducting a one-tailed correlation test, you could do that in the same way as you did for the Pearson. For instance, if you predicted that cholesterol and saturated fat would have a positive correlation, you could do the following to do a one-tailed test: cor.test(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;, alternative = &quot;greater&quot;) ## Warning in cor.test.default(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;, : ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: cheese$sat_fat and cheese$chol ## S = 8575.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is greater than 0 ## sample estimates: ## rho ## 0.8677042 You may also notice that the output of the Spearman Rank test does not give confidence intervals for the value of rho. This is unfortunate and is one of the drawbacks of doing a non-parametric correlation. Finally, there are several other types of non-parametric correlations you could choose from if you didn't want to do a Spearman Rank correlation. We personally recommend using a method called Kendalls Tau B correlation, which can be done like this: cor.test(cheese$sat_fat, cheese$chol, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: cheese$sat_fat and cheese$chol ## z = 8.8085, p-value &lt; 2.2e-16 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.7102531 This output gives you a tau value which is the correlation coefficient, and a p-value which you can interpret in the same way as the other tests. Ranked Data If at least one of your variables of your data are rank (ordinal) data, then you should use non-parametric correlations. In the following example, the data show the dominance rank, age, body size and testosterone levels for a group of 18 animals. Lower numbers of the ranks, indicate a higher ranking animal. An animal with rank 1 means that it is the most dominant individual. Perhaps with such data you may be interested in seeing if there was an association between dominance rank and testosterone levels. Because your dominance rank measure is ordinal (a rank), then you should pick a non-parametric correlation. test &lt;- read_csv(&quot;data/testosterone.csv&quot;) ## Parsed with column specification: ## cols( ## drank = col_integer(), ## age = col_double(), ## size = col_integer(), ## testosterone = col_double() ## ) head(test) ## # A tibble: 6 x 4 ## drank age size testosterone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 3 13 183 4.8 ## 2 7 9 155 3.9 ## 3 7 5.5 144 3.8 ## 4 1 11.5 201 6.4 ## 5 12 3.5 125 1.8 ## 6 4 10 166 4.3 ggplot(test, aes(x = drank, y = testosterone)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) + xlab(&quot;Dominance Rank&quot;) + ylab(&quot;Testosterone Level&quot;) cor(test$drank, test$testosterone, method = &quot;spearman&quot;) # rho = -0.91 ## [1] -0.9083378 If you had the a priori prediction, that more dominant animals would have higher testosterone, then you could do a one-tailed test. This would mean that you expect there to be a negative correlation - as the rank number gets higher, the levels of testosterone would fall. In this case, you'd use alternative = &quot;less&quot;. cor.test(test$drank, test$testosterone, method = &quot;spearman&quot;, alternative = &quot;less&quot;) # 1- tailed ## Warning in cor.test.default(test$drank, test$testosterone, method = ## &quot;spearman&quot;, : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: test$drank and test$testosterone ## S = 1849.2, p-value = 9.367e-08 ## alternative hypothesis: true rho is less than 0 ## sample estimates: ## rho ## -0.9083378 10.9 Point-Biserial Correlation why, what.... "],
["regression.html", "11 Regression 11.1 Introduction to Linear Regression 11.2 a and b 11.3 Residuals 11.4 Standard Error of the Estimate 11.5 Goodness of Fit Test - F-ratio 11.6 Assumptions of Linear Regression 11.7 Examining individual predictor estimates", " 11 Regression Introduction to what regression is... predicting etc... 11.1 Introduction to Linear Regression The regression method that we are going to start with is called Ordinary Least Squares Regression. Hopefully the reason why it's called &quot;least squares&quot; will be come obvious, although we're not too sure why it's called &quot;ordinary&quot;. Let's illustrate the question at hand with some data: library(tidyverse) # Import Data df &lt;- read_csv(&quot;data/parenthood.csv&quot;) ## Parsed with column specification: ## cols( ## dan.sleep = col_double(), ## baby.sleep = col_double(), ## dan.grump = col_integer(), ## day = col_integer() ## ) nrow(df) ## [1] 100 head(df) ## # A tibble: 6 x 4 ## dan.sleep baby.sleep dan.grump day ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 7.59 10.2 56 1 ## 2 7.91 11.7 60 2 ## 3 5.14 7.92 82 3 ## 4 7.71 9.61 55 4 ## 5 6.68 9.75 67 5 ## 6 5.99 5.04 72 6 As you can see, what we have are four columns of data. The first column shows the amount of sleep that Dan got in an evening. The second column relates to the amount of sleep a baby got. The third column is a rating of Dan's grumpiness. The last column is a day identifier. Each row represents a different day. Say we're interested in seeing whether there was an association between Dan's sleep and her grumpiness. We could examine this using a scatterplot: # Scatterplot ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) It looks like these variables are clearly associated, with higher levels of sleep being related to lower levels of grumpiness. To get a measure of how big this relationship is, we could run a correlation test. cor.test(df$dan.sleep, df$dan.grump) ## ## Pearson&#39;s product-moment correlation ## ## data: df$dan.sleep and df$dan.grump ## t = -20.854, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9340614 -0.8594714 ## sample estimates: ## cor ## -0.903384 This shows that the variables are highly correlated with r=-0.90. It's also a highly significant relationship. Using stat_smooth() we also applied a best fitting trendline through the data. This line was actually calculated to be in the position that it is in using regression. The line also has the equation: $y&#39; = a + xb$ You may also se this written in other ways such as: $y&#39; = \\beta_{0} + x\\beta_{1}$ but we'll stick with $y&#39; = a + xb$ 11.2 a and b In the equation for a regression line, $y&#39; = a + xb$ \\(y&#39;\\) is equal to the predicted value of \\(y\\). Essentially if you go from any value of \\(x\\) and go up to the trendline, and then across to the y-axis, that is your predicted value of y. The trendline represents the predicted values of \\(y\\) for all values of \\(x\\). In regression terms, we often refer to \\(x\\) as the predictor variable and \\(y\\) as the outcome variable. The value of \\(b\\) in the equation represents the slope of the regression line. If it's a positive number then it means that the regression line is going upwards (akin to a positive correlation, where \\(y\\) increases as \\(x\\) increases.) If it's a negative number, then it means that the regression line is going downwards (akin to a negative correlation, where \\(y\\) decreases as \\(x\\) increases). The value of \\(b\\) can also be considered to be how much \\(y\\) changes for every 1 unit increase in \\(x\\). So a value of \\(b = -1.4\\) would indicate that as \\(x\\) increases by 1, \\(y\\) will decrease by 1.4. The value of \\(a\\) represents the y-intercept. This is the value of y' that you would get if you extended the regression line to cross at \\(x=0\\). We can illustrate that in the graph below. We extended the trendline (dotted black line) from its ends until it passes through where \\(x=0\\) (the dotted red line). Where it crosses this point is at \\(y=125.96\\) which is depicted by the orange dotted line. This makes the y-intercept for this trendline equal to 125.96. # Scatterplot ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) + xlim(-1,10)+ ylim(40,130)+ geom_abline(intercept =125.956 , slope = -8.937, lty=2)+ geom_vline(xintercept=0, color=&#39;red&#39;, lty=2) + geom_segment(x=-1.6,xend=0,y=125.96,yend=125.96, lty=2, color=&#39;orange&#39;) In reality, we do not ever extend the reression lines like this. In fact, a regression line by definition only fits the range of datapoints along the x-axis that we have - it should not extend beyond those points. This is more a theoretical construct to help us understand where the line is on the graph. To think a bit more about a and b let's compare these regression lines and their respective equations. comparing a and b In the top left image, you can see three parallel regression lines. These all have the same slope value (b = 0.704). How they differ is in their y-intercept. The different values of 'a' indicate that they are at different heights. In the top right, we've extended the regression lines in red back to where the x-axis is 0. Where the lines cross here (x=0) is the value of each y-intercept i.e. 'a'. In the bottom right image, you can see that all the plots have the same y-intercept value (8.32). How they differ is in their slope b. Two are positive values of b, with the larger value having a steeper slope. The negative value of b (-0.622) has a slope going downwards. In the bottom right we extend these trendlines back to where x=0, and you can see that all have the same y-intercept. 11.2.1 How to calculate a and b in R To run a regression in R, we use the lm() function. It looks like the following: mod1 &lt;- lm(dan.grump ~ dan.sleep, data=df) # build regression model The first thing after the bracket is the outcome variable which is dan.grump. Then a tilde (~) and then the predictor variable which is dan.sleep. Finally, we tell lm what dataset we're using. The best way to read that statement is: &quot;dan.grump 'is predicted by' dan.sleep&quot;. We're also saving the regression model as mod1 because there's tons of information that comes along with the regression. To have a look at what a and b are, we can just look at our saved object mod1. mod1 ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Coefficients: ## (Intercept) dan.sleep ## 125.956 -8.937 Here the value underneath &quot;Intercept&quot; (125.956) refers to the y-intercept a. The value underneath dan.sleep, the predictor variable, is our value of b the slope. So this would mean that the regression line for these data would be: \\(y&#39; = 125.956 + -8.937b\\) We can also get these values directly by running the following code: mod1$coefficients ## (Intercept) dan.sleep ## 125.956292 -8.936756 Now, we should also mention one other thing about these values. These regression coefficients are 'estimates'. We have one sample of 100 subjects from which we estimated the true population values of 'a' and 'b'. The true population values of 'a' and 'b' are parameters. 11.2.2 How to calculate a and b 'by hand' To think a bit more about these 'a' and 'b' values, we could look at how these values are actually calculated. First, we calculate 'b'. The formula for this requires knowing the sample standard deviation of the X and Y variables, as well as their Pearson correlation. \\(b = r\\frac{s_{Y}}{s_{X}}\\) So for our example, we'd calculate 'b' like this: r &lt;- cor(df$dan.sleep, df$dan.grump) sy &lt;- sd(df$dan.grump) sx &lt;- sd(df$dan.sleep) b &lt;- r * (sy/sx) b ## [1] -8.936756 Next, we can calculate 'a' using the formula: \\(a = \\overline{Y} - b\\overline{X}\\) . This requires us to know the sample mean of X and Y. So for our example, we calculate 'a' like this: my &lt;- mean(df$dan.grump) mx &lt;- mean(df$dan.sleep) a &lt;- my - (b * mx) a ## [1] 125.9563 Therefore, we have an equation for our trendline which is y' = 125.96 + -8.94b, which means that for every 1 unit increase of sleep, Dan's grumpiness decreases by 8.94. 11.3 Residuals Once we fit a trendline to the data it's clear that not all the datapoints fit to the line. In fact, almost none of the datapoints are on the line! This is because the trendline is our prediction of the value of y based on the value of x. Eeach datapoint actually is either larger or smaller in terms of y than the regression line. Sometimes it's a bit bigger or smaller, other times it might be a lot bigger or smaller. Occasionally, the predicted value of y might be on the regression line. Because our trendline isn't a perfect fit for the data, the formula for the regression line could technically be written as: \\(y&#39; = a + bx + \\epsilon\\) \\(\\epsilon\\) refers to the error, or how far each datapoint is from the predicted value. In fact, the difference of each data point from the predicted value is called a raw residual or ordinary residual. We calculate the size of the residual for each datapoint by the following formula: \\(residual = y - y&#39;\\) This essentially is the difference of each data point from the predicted value. 11.3.1 How to calculate the residuals Using our regression line equation of y' = 125.96 + -8.94b, we can manually calculate the raw residuals. Firstly, we calculate the predicted values of y \\(y&#39;\\) for each value of \\(x\\). We can put these back into our original dataframe. X &lt;- df$dan.sleep # the predictor Y &lt;- df$dan.grump # the outcome Y.pred &lt;- 125.96 + (-8.94 * X) Y.pred ## [1] 58.1054 55.2446 80.0084 57.0326 66.2408 72.4094 52.7414 61.6814 59.8040 ## [10] 67.1348 67.9394 69.9062 72.7670 66.5090 68.6546 69.3698 69.6380 50.2382 ## [19] 61.5026 58.6418 54.4400 60.2510 64.6316 55.6916 82.5116 73.4822 50.8640 ## [28] 64.0058 61.5026 63.4694 52.9202 55.7810 69.9062 48.5396 81.4388 70.6214 ## [37] 68.6546 82.6904 63.1118 57.4796 58.8206 55.1552 53.3672 59.1782 54.5294 ## [46] 77.3264 53.0096 57.8372 73.4822 45.5000 51.6686 65.9726 59.5358 73.2140 ## [55] 49.7912 72.0518 60.7874 60.5192 64.4528 70.3532 63.9164 63.2906 61.5920 ## [64] 69.6380 48.0032 56.0492 53.1884 60.9662 66.0620 58.4630 59.9828 56.8538 ## [73] 78.3992 55.6916 69.1910 62.3966 77.2370 56.2280 62.2178 51.3110 64.0058 ## [82] 62.7542 48.5396 80.4554 82.0646 63.1118 63.2012 57.3902 53.0990 73.3928 ## [91] 74.8232 66.4196 64.7210 76.1642 79.8296 78.4886 56.4962 77.8628 63.2012 ## [100] 68.2970 df$Y.pred &lt;- Y.pred head(df) ## # A tibble: 6 x 5 ## dan.sleep baby.sleep dan.grump day Y.pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 ## 2 7.91 11.7 60 2 55.2 ## 3 5.14 7.92 82 3 80.0 ## 4 7.71 9.61 55 4 57.0 ## 5 6.68 9.75 67 5 66.2 ## 6 5.99 5.04 72 6 72.4 Next, getting the raw residual is simply a matter of taking each observed value of \\(y\\) and subracting the predicted value of \\(y\\). # so to get the residual, y - y&#39; df$dan.grump - Y.pred ## [1] -2.1054 4.7554 1.9916 -2.0326 0.7592 -0.4094 0.2586 -1.6814 ## [9] 0.1960 3.8652 4.0606 -4.9062 1.2330 0.4910 -2.6546 -0.3698 ## [17] 3.3620 1.7618 -0.5026 -5.6418 -0.4400 2.7490 9.3684 0.3084 ## [25] -0.5116 -1.4822 8.1360 1.9942 -1.5026 3.5306 -8.9202 -2.7810 ## [33] 6.0938 -7.5396 4.5612 -10.6214 -5.6546 6.3096 -2.1118 -0.4796 ## [41] 0.1794 4.8448 -5.3672 -6.1782 -4.5294 -5.3264 3.9904 2.1628 ## [49] -3.4822 0.5000 6.3314 2.0274 -1.5358 -2.2140 2.2088 1.9482 ## [57] -1.7874 -1.5192 2.5472 -3.3532 -2.9164 0.7094 -0.5920 -8.6380 ## [65] 5.9968 5.9508 -1.1884 3.0338 -1.0620 6.5370 -2.9828 2.1462 ## [73] 0.6008 -2.6916 -2.1910 -1.3966 4.7630 11.7720 4.7822 2.6890 ## [81] -11.0058 -0.7542 1.4604 -0.4554 8.9354 -1.1118 0.7988 -0.3902 ## [89] 0.9010 -1.3928 3.1768 -3.4196 -5.7210 -2.1642 -3.8296 0.5114 ## [97] -5.4962 4.1372 -8.2012 5.7030 df$residuals &lt;- df$dan.grump - Y.pred head(df) ## # A tibble: 6 x 6 ## dan.sleep baby.sleep dan.grump day Y.pred residuals ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 -2.11 ## 2 7.91 11.7 60 2 55.2 4.76 ## 3 5.14 7.92 82 3 80.0 1.99 ## 4 7.71 9.61 55 4 57.0 -2.03 ## 5 6.68 9.75 67 5 66.2 0.759 ## 6 5.99 5.04 72 6 72.4 -0.409 11.3.2 Visualizing the Residuals Now we have a raw residual for all 100 datapoints in our data. The following plot is our same scatterplot, but this time we've also added a little red line connecting each observed datapoint to the regression line. The size of each of these red lines represents the residuals. p1 &lt;- ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_segment(aes(xend = dan.sleep, yend = dan.grump-residuals), alpha = .5, color=&#39;red&#39;) + theme_classic() + ggtitle(&quot;OLSR best fit trendline&quot;) p1 11.3.3 Comparing our trendline to other trendlines An important question to consider is why did we end up with our trendline and not some other trendline? The answer is that ours is the 'best fit', but what does that really mean? In short, it means that the best-fit regression line is the one that has the smallest squared residuals. The squared residuals are calcualted by squaring every residual and then summing these all up. Let's look at this by looking at one possible trendline that we could have used. The one that we'll choose is a trendline that goes horizonatally through the data with a y value that is the mean of Y. mean(df$dan.grump) ## [1] 63.71 So, the mean of the Y variable dan.grump is 63.71. Let's put a trendline through our data that is horizontal at 63.71. The equation for this line would be: \\(y&#39; = 63.71 + 0x\\) Let's visualize this: ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_hline(yintercept = mean(df$dan.grump), color=&#39;blue&#39;) It doesn't look like using the mean value of Y is that good a predictor of each datapoint. We could actually visualize how good.bad it is by calculating the residual of each datapoint from this new trendline. This time each residual is equal to: \\(residual = y - \\overline{y}\\) Let's calculate these residuals, and then graph them on this scatterplot: # we can work out what the &#39;residuals&#39; would be for this trendline: df$Ymean &lt;- mean(df$dan.grump) df$resid_Ymean &lt;- df$dan.grump - df$Ymean #residual from Ymean head(df) ## # A tibble: 6 x 8 ## dan.sleep baby.sleep dan.grump day Y.pred residuals Ymean resid_Ymean ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 -2.11 63.7 -7.71 ## 2 7.91 11.7 60 2 55.2 4.76 63.7 -3.71 ## 3 5.14 7.92 82 3 80.0 1.99 63.7 18.3 ## 4 7.71 9.61 55 4 57.0 -2.03 63.7 -8.71 ## 5 6.68 9.75 67 5 66.2 0.759 63.7 3.29 ## 6 5.99 5.04 72 6 72.4 -0.409 63.7 8.29 ## visualize this: p2 &lt;- ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_hline(yintercept = mean(df$dan.grump), color=&#39;blue&#39;) + geom_segment(aes(xend = dan.sleep, yend = dan.grump-resid_Ymean), alpha = .5, color=&#39;red&#39;) + # geom_point(aes(y = dan.grump+residuals), shape = 1) + theme_classic() + ggtitle(&quot;Residuals to the mean Y&quot;) p2 If we compare both graphs scatterplots side by side, it's pretty clear that our best-fit trendline is doing a much better job of predicting each datapoint's Y value. The horizontal trendline at the mean of Y looks pretty bad for the majority of datapoints - its residuals are much bigger. library(gridExtra) grid.arrange(p1,p2,nrow=1) In fact, let's actually quantify the difference in residuals between these two trendlines. Because we have both positive and neagative residuals, if we just added them together we'd end up with 0. In statistics, one common way to make numbers positive is to square them. As we saw with standard deviation, this also has the advantage of emphasizing large values. What we do to compare our residuals, is to therefore square them. We call the residuals from our trendline the raw residuals. We call the residuals from the horizontal line the total residuals. df$residuals2 &lt;- df$residuals ^ 2 # raw residuals df$resid_Ymean2 &lt;- df$resid_Ymean ^ 2 # total residuals head(df[c(1,3,6,8,9,10)]) # just showing the relevant columns ## # A tibble: 6 x 6 ## dan.sleep dan.grump residuals resid_Ymean residuals2 resid_Ymean2 ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 56 -2.11 -7.71 4.43 59.4 ## 2 7.91 60 4.76 -3.71 22.6 13.8 ## 3 5.14 82 1.99 18.3 3.97 335. ## 4 7.71 55 -2.03 -8.71 4.13 75.9 ## 5 6.68 67 0.759 3.29 0.576 10.8 ## 6 5.99 72 -0.409 8.29 0.168 68.7 You can see the squared raw residual and squared total residual for the first six datapoints. Ideally, we want the raw residuals to be as small a fraction as possible of the total residuals. If we sum up the squared raw residuals and squared total residuals, we can determine this: SS.resid &lt;- sum(df$residuals2) SS.resid #1838.722 ## [1] 1838.75 SS.tot &lt;- sum(df$resid_Ymean2) SS.tot #9998.59 ## [1] 9998.59 As you can clearly see, the summed squared residuals for our best fit regression line are much smaller than the summed squared residuals when using the mean of y as the regression line. 11.3.4 Coefficient of Determination R2 One way to make these summed squares of residuals numbers more interpretable is to convert them to \\(R^{2}\\). The logic goes as following. If the trendline is absolutely useless at predicting the y values, then the trendline would have residuals as high as the total residuals. If the trendline is perfect at predicting the y values, then the residual SS total would be 0. If we look at the sum of the squares of the raw residuals as a ratio of a sum of the squared total residuals then we can work out how well our trendline fits. We calculate this using the formula, \\(R^{2} = 1 - \\frac{SS_{raw}}{SS_{total}}\\) in the following way: 1 - (SS.resid/SS.tot) # 0.816 (this is R2) ## [1] 0.816099 So for our data, \\(R^{2} = 0.816\\). This means that our regression model (and trendline) is a very good fit to the data. \\(R^{2}\\) ranges from 0 to 1. If its value was 1, then that would indicate that the best-fit trendline perfectly fits the data with no raw residuals. If its value was 0, then that would mean that the best-fit trendline is no better at fitting the data than the horizontal line at the mean of Y. Values of \\(R^{2}\\) that get closer to 1 indicate that the model is doing a better job at estimating each value of Y. We say that the model has a better 'fit' to the data. There is a way to directly get the value of \\(R^{2}\\) in R. You may remember earlier in this chapter that we ran our linear model using the function lm(). We saved the output of this regression model as the object mod1. You can use summary() to get lots of information about the regression model. summary(mod1) # the R2 matches ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 The thing to focus on from this output right now, is the value called Multiple R-squared. This gives us the same value of \\(R^{2}\\) that we calculated by hand: \\(R^{2}=0.816\\). The other value of \\(R^{2}\\) is called Adjusted R-squared. This is relevant when you are conducting multiple regression with more than one predictor in the model. We only have one predictor in the model (dan.sleep) so we won't talk more about Adjusted R squared here. Finally, there is a shortcut way in which we can calculate \\(R^{2}\\). It is simply to square the Pearson's correlation coefficient: \\(R^{2} = r^{2}\\). r &lt;- cor(df$dan.sleep, df$dan.grump) r ## [1] -0.903384 r^2 # same R2 as above ## [1] 0.8161027 11.4 Standard Error of the Estimate \\(R^{2}\\) is one value that gives us a sense of how well our regression model is doing. Another method to assess 'model fit' is to examine the Standard Error of the Estimate. Unfortunately, this value has a number of names. You'll see it referred to as the Standard Error of the Regression, Residual Standard Error, Regression Standard Error, \\(S\\), or \\(\\sigma_{est}\\). We prefer to call it the Standard Error of the Estimate or \\(\\sigma_{est}\\). This is calculated as follows: \\(\\sigma_{est} = \\sqrt{\\frac{\\Sigma (y - y&#39;)^{2}}{n-2}}\\) The \\(\\Sigma (y - y&#39;)^{2}\\) part of this equation is the Sum of the raw residuals squared that we already calculated when calculating \\(R^{2}\\). We can therefore calculate \\(\\sigma_{est}\\) quite straightforwardly in R manually. s_est &lt;- sqrt(SS.resid / 98) # n=100 s_est ## [1] 4.3316 Therefore \\(\\sigma_{est} = 4.332\\) for our model. It's acutally possible to see this value in the output of the summary of the model in R. Here, it's called the 'residual standard error': summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 11.4.1 What to do with the Standard Error of the Estimate ? What we are generally looking for with \\(\\sigma_{est}\\) is a number as small as possible. This is because what essentially it is a measure of is an approximate estimate of the average raw residual. The question is, how small is small? This is difficult to answer. One reason is because \\(\\sigma_{est}\\) is actually in the original units of the Y-axis (the outcome variable). Thefore it's not as simple as saying that it should be close to 0, because the Y-axis may be in units that are very large. In other ways, having this value in the original units of Y can be quite helpful as it is easy to envisage what the size of the average residual is. However, a good rule of thumb is that approximately 95% of the observations (raw datapoints) should fall within plus or minus 2 times the standard error of the estimates from the regression line. We can illustrate this with the following plot: ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() + geom_abline(intercept = 125.956+s_est+s_est, slope = -8.937, color = &#39;red&#39;, lty=2)+ geom_abline(intercept = 125.956-s_est-s_est, slope = -8.937, color = &#39;red&#39;, lty=2) + ggtitle(&quot;Regression with 2 x Standard Error of the Estimate&quot;) This is our original scatterplot again. The blue line is still the best fitting trendline. The two dashed red lines are two times \\(\\sigma_{est}\\) above and below the blue line respectively. That is they are 2 * 4.332 = 8.644 units of grumpiness above or below the trendline. If we count up the number of datapoints that our outside of the dotted red lines, we can see that there are six datapoints out of our 100 datapoints that are just outside, although some of htese are very close indeed to the red line. This is probably ok as we're only expecting 95% of datapoints on average to be inside the red lines. 11.5 Goodness of Fit Test - F-ratio \\(R^{2}\\) and \\(\\sigma_{est}\\) are two calculations that help us determine if our regression model (and trendline) is indeed a good fit to the data. A more formal method is to run a statistical test called the Goodness of Fit Test. To do this we calculate an F-ratio that essentially examines how well our trendline (and model) fit the data compared to the null model which is the model where we use the mean of Y as our prediction. As a reminder, these scatterplots show the difference in performance of our fitted model (left) compared to the null model (right). grid.arrange(p1,p2,nrow=1) The F-ratio is essentially a method of determining the proportion of residual variance compared to total variance. We can calculate it using the following formula: $F = $ Here, SSM refers to the sum of squares for the model (the model sum of squares). This is equal to: \\(SSM = SST - SSR\\) That is, it's the difference between the total sum of squares (the sum of the squared residuals for the null model) minus the residual sum of squares (the sum of the squared residuals for the fitted model). The d.f.SSM refers to the degrees of freedom for the model sum of squares, which is equal to the number of predictors in the model. We only have one predictor (dan.sleep), so that means the degrees of freedom are 1. The d.f.SSR refers to the degrees of freedom for the raw residuals. This is equal to the number of observations minus the number of predictors minus 1. Therefore it is equal to 100 - 1 - 1 = 98 as we had 100 datapoints (or obsevations). Let's calculate this in R. Firstly, we'll calculate the model sum of squares: SS.mod &lt;- SS.tot - SS.resid SS.mod #8159.868 ## [1] 8159.84 Next we divide the model sum of squares and the residual sum of squares by their respective degrees of freedom to get the mean sum of squares for each. \\(F\\) is then calculated by dividing the former by the latter: MS.resid &lt;- SS.resid / 98 MS.resid #18.76 ## [1] 18.76276 MS.mod &lt;- SS.mod / 1 MS.mod #8159.868 ## [1] 8159.84 Fval &lt;- MS.mod / MS.resid Fval #434.9 ## [1] 434.8955 So \\(F = 434.9\\) which is a large value. Larger values indicate that we were less likely to get a difference between the sum of squares for our fitted and null models by chance alone. Essentially, the observed value of F is compared to the sampling distribution for F if the null hypothesis is true. The null is that our model (trendline) is no better than random in fitting the datapoints Whether our \\(F\\) value is sufficiently large can be looked up in an F-table (not recommended), or you can simply let R do the work for you. If you use summary() on your saved regression model, it will give you the \\(F\\) value as well as a p-value to go along with it. summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 You can see from the model output that the F-statistic is given on the bottom row along with the degrees of freedom for the model (1) and the residuals (98). The p-value here is 0.0000000000000022 which is basically 0. Typically, if the p-value is less than 0.05 we will say that our fitted model is a better fit to the data than the null model. 11.6 Assumptions of Linear Regression Even if your model is a good fit to the data, there are still several things you need to check before progressing to examining whether your predictor is 'significantly' (a better term is probably 'meaningfully') predicting the outcome variable. The last series of things we need to do is to check whether we have violated the assumptions of linear regression. In short, here are some of the assumptions that we need to adhere to: Normality - Specifically the residuals are normally distributed. Linearity - We need to be examining linear relationships between predictors and outcomes Homogeneity of Variance (homoscedasticity) Uncorrelated Predictors (only relevant if doing more than one predictor) No overly influential datapoints Let's discuss each of these in turn. 11.6.1 Normality of Residuals One assumption of linear regression is that our residuals are approximately normally distributed. We can check this in several ways. But first, we should probably own up and mention that there are several types of residuals. The residuals we have been dealing with from the regression model have been the raw residuals. We got these by simply subtracting the predicted value of y from the observed value of y \\(residual = y - y&#39;\\). However, statisticians like to modify these residuals. One modification they make is to turn these residuals into what are called standardized residuals. These are the raw residuals divided by the standard deviation of the residuals. You can directly access the standardized residuals in R by using the rstandard() function with your model output: df$std_resids &lt;- rstandard(mod1) head(df[c(1,3,6,11)]) # just showing the relevant columns ## # A tibble: 6 x 4 ## dan.sleep dan.grump residuals std_resids ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 56 -2.11 -0.494 ## 2 7.91 60 4.76 1.10 ## 3 5.14 82 1.99 0.467 ## 4 7.71 55 -2.03 -0.478 ## 5 6.68 67 0.759 0.172 ## 6 5.99 72 -0.409 -0.0991 We can demonstrate that the standardized residuals really are highly correlated with the raw residuals by plotting them on a scatterplot: ggplot(df, aes(x=residuals, y=std_resids)) + geom_point() + theme_classic()+ xlab(&quot;Raw Residuals&quot;)+ ylab(&quot;Standardized Residuals&quot;) They are effectively the same - just transformed. One advantage of standardized residuals is that they can help us look for unusual datapoints with large residuals. While raw residuals are always in the original units of the y-axis, standardized residuals are, well, standardized. Standardized residuals that are greater than 2 or less than -2 are quite large, those that are larger than 3 or less than 3 are very large indeed and should be looked at in more detail. Let's get back to checking the normality of these residuals. First up, we could plot a histogram and see if we think it's approximately normal: #a) histogram plot ggplot(df, aes(x=std_resids)) + geom_histogram(color=&#39;white&#39;) # possibly ok ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This looks possibly ok. It's not too skewed, but it can be harded from a histogram with just 100 datapoints to get a sense of the shape. A second approach would be to use a Shapiro-Wilk test which more formally test whether the data are approximately normally distributed: #b) Shapiro-Wilk test shapiro.test(df$std_resids) # shapiro test says normal. ## ## Shapiro-Wilk normality test ## ## data: df$std_resids ## W = 0.99201, p-value = 0.8221 Given that the p-value here is above our cut-off of 0.05, this suggests that we have no evidence to reject the hypothesis that our data came from a normal distribution. An easier way of saying this is, our residuals are likey approximately normally distributed. The other method we can employ to check normality is to use a QQ plot: #c) QQ plot qqnorm(df$std_resids) qqline(df$std_resids, col = &quot;steelblue&quot;, lwd = 2) # it&#39;s ok A discussion of precisely what these QQ plots are is beyond the scope here. However, in general terms, what we are plotting is the residual against the theoretical value of each residual that we would expect if our data were normally distributed. In practical terms, what we're looking for is that the bulk of our data fit along the blue line. It's ok to have a bit of wobble at the extremes - that just means that our data distribution probably has slightly fat tails. It's also possible to generate a version of the above plot quickly, directly from the saved linear model object. plot( x = mod1, which = 2 ) # fast way of getting same plot 11.6.2 2. Linearity --- The second major assumption of linear regression is that the relationship between our predictor and outcome is linear! So, data that look like the following would not have a linear relationship. comparing a and b One simple approach is to examine the scatterplot of your predictor (X) and outcome (Y) variables. We already did this, and our data looked pretty linear! Another approach is to examine the relationship between your observed Y values and the predicted Y values \\(y&#39;\\). This should also be a linear relationship. Although we calculated the \\(y&#39;\\) values earlier using the formula for the regression line, we can actually grab them directly from the model object with the fitted.values() command. df$Y.fitted &lt;- fitted.values(mod1) # plot this relationship ggplot(df, aes(x = Y.fitted, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) As you can see from this plot, our fitted (predicted) values of Y on the x-axis have a strong linear relationship with our observed values of Y on the y-axis. 11.6.3 3. Homogeneity of Variance / Homoscedasticity The third assumption that we need to check is homoscedasticity (also sometimes referred to as homogeneity of variance). What this really means is that the model should be equally good at predicting Y's across all values of X. Our regression model shouldn't be better at predicting values of Y for e.g. small values of X but not for large values of X. Practically, what this means, is that the size of the residuals should be equal across all values of X. If we plot the values of X (or the predicted/fitted values of Y) on the x-axis against the residuals (in this case standardized residuals) on the Y-axis, then there should be no overall pattern. We should be equally likely to get small or large residuals for any value of X (or predicted/fitted value of Y). This would mean that any trendline on this graph should be a horizontal line. # if true, this should be a straight line ggplot(df, aes(x = Y.fitted, y = std_resids)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) As you can see, our plot is basically a random scatterplot and there is no overall pattern. That is good, it means we have homoscedasticity. If we did not have homoscedasticity, then we'd see a pattern in this scatterplot - such as the residuals getting larger or smaller for larger fitted (predicted) values of Y. You can access a version of this plot directly from the model object like this: plot(mod1, which = 3) There are some more formal tests of homoscedasticity but we don't need to worry about them. 11.6.4 No Colinearity This assumption of linear regression only applies when we have more than one predictor in the model. In our model, we do only have one predictor (dan.sleep) so we don't need to worry about it. If we had added in another variable into the model, e.g. the amount of hours of baby sleep, then we'd have a second predictor. That means, we're trying to predict dan.grump based on both dan.sleep and baby.sleep. In this case, dan.sleep and baby.sleep should not be correlated with each other. 11.6.5 Unusual Datapoints There are a number of ways that datapoints could be unusual. We will discuss data points that are: outliers have high leverage have high influence Generally linear regression models should not be overly affected by individual data points. Usually the category that we most need to be concerned about are points with high influence. 11.6.5.1 i. Outliers Outliers are datapoints that are typically highly unusual in terms of outcome Y but not in terms of the predictor X. These will be datapoints that have high residuals. Let's look at an example dataset. We have a predictor 'x' and an outcome 'y'. With 'y1' we have the same outcome variables, but have removed the value for one datapoint: x &lt;- c(4.1,4.2,5,5.5,6,6.1,6.15,6.4,7,7.2,7.7,8,9.7) y &lt;- c(3.9,4.3,6,5.9,6.1,11.9,6.3,5.8,7.9,6.4,7.8,8,9.1) y1 &lt;- c(3.9,4.3,6,5.9,6.1,NA,6.3,5.8,7.9,6.4,7.8,8,9.1) ddf &lt;- data.frame(x,y,y1) ddf ## x y y1 ## 1 4.10 3.9 3.9 ## 2 4.20 4.3 4.3 ## 3 5.00 6.0 6.0 ## 4 5.50 5.9 5.9 ## 5 6.00 6.1 6.1 ## 6 6.10 11.9 NA ## 7 6.15 6.3 6.3 ## 8 6.40 5.8 5.8 ## 9 7.00 7.9 7.9 ## 10 7.20 6.4 6.4 ## 11 7.70 7.8 7.8 ## 12 8.00 8.0 8.0 ## 13 9.70 9.1 9.1 And here, we're plotting the best fitting regression line in blue. The dotted red line represents the best fitting trendline that we would have if we removed the datapoints that has the y value of 11.9. The dashed black line shows the distance from this datapoint to the trendline we would have if it was removed. ## (Intercept) x ## 0.8247787 0.8785270 As you can see from this small example, outliers are datapoints that have very large residuals from the trendline. They have an unusually large Y. Notice though that the slope of the trendline hasn't change too much at all. It is slighly shifted down after you remove that outlier from the calculations, but overall the coefficient of 'b' is similar to before. This type of outlier is not necessarily a big deal. 11.6.5.2 ii. High Leverage Datapoints that have high leverage are those that have a high influence on the regression line's trajectory, but don't necessarily affect the angle of the slope. They are typically unsual in terms of their X value, but not necessarily in terms of Y, meaning that they don't have to have a high residual. We can measure the leverage of datapoints using the function hatvalues() on the model object. For the scatterplot above, the hat values of each datapoint can be calculated as follows: mod.out &lt;- lm(y~x, data = ddf) ddf$hat_val &lt;- hatvalues(mod.out) ddf ## x y y1 hat_val ## 1 4.10 3.9 3.9 0.25535302 ## 2 4.20 4.3 4.3 0.24009985 ## 3 5.00 6.0 6.0 0.14260536 ## 4 5.50 5.9 5.9 0.10381722 ## 5 6.00 6.1 6.1 0.08206442 ## 6 6.10 11.9 NA 0.07975810 ## 7 6.15 6.3 6.3 0.07886047 ## 8 6.40 5.8 5.8 0.07692761 ## 9 7.00 7.9 7.9 0.08966480 ## 10 7.20 6.4 6.4 0.09936183 ## 11 7.70 7.8 7.8 0.13552914 ## 12 8.00 8.0 8.0 0.16540649 ## 13 9.70 9.1 9.1 0.45055168 As you can see, the 6th datapoint was the outlier but it does not have a large leverage as it's not overly influencing the trajectory of the regression line. Datapoint 13 on the other hand does have a higher leverage. Roughly speaking, a large hatvalue is one which is 2-3 times the average hat value. We can check the mean of the hat values like this: mean(hatvalues(mod.out)) ## [1] 0.1538462 So clearly, the 13th datapoint does have a high leverage. 11.6.5.3 iii. High Influence The third type of unusual datapoint are ones that you need to be most wary of. These are datapoints that have high influence. Essentially a high influence datapoint is a high leverage datapoint that is also an outlier. Let's look at these example data. We are looking at a predictor variable 'x' against an outcome variable 'y2'. 'y3' is the same data as 'y2' with one datapoint's value removed. x &lt;- c(4.1,4.2,5,5.5,6,6.15,6.4,7,7.2,7.7,8,9.7) y2 &lt;- c(3.9,4.3,6,5.9,6.1,6.3,5.8,7.9,6.4,7.8,8,6.1) y3 &lt;- c(3.9,4.3,6,5.9,6.1,6.3,5.8,7.9,6.4,7.8,8,NA) ddf1 &lt;- data.frame(x,y2,y3) ddf1 ## x y2 y3 ## 1 4.10 3.9 3.9 ## 2 4.20 4.3 4.3 ## 3 5.00 6.0 6.0 ## 4 5.50 5.9 5.9 ## 5 6.00 6.1 6.1 ## 6 6.15 6.3 6.3 ## 7 6.40 5.8 5.8 ## 8 7.00 7.9 7.9 ## 9 7.20 6.4 6.4 ## 10 7.70 7.8 7.8 ## 11 8.00 8.0 8.0 ## 12 9.70 6.1 NA Let's plot these data - I'm sparing you the code for this plot: What you can see here is the scatterplot of 'x' against 'y2'. The blue trendline is the best fit line to the regression of 'x' against 'y2'. The red dotted line is the regression line when we remove the datapoint at x=9.7. This datapoint has both high leverage and is an outlier (has an overly large residual). As a consequence of this, when you remove this datapoint it significantly shifts the angle of the slope of the regression line. That means that the value of b changes quite considerably. Let's add the hat values (measuring leverage) to the data. We can also add what is called Cook's Distance which is a measure of the influence of each datapoint. mod.out2 &lt;- lm(y2~x, data = ddf1) ddf1$hat_val &lt;- hatvalues(mod.out2) ddf1$cooks_d &lt;-cooks.distance(mod.out2) ddf1 ## x y2 y3 hat_val cooks_d ## 1 4.10 3.9 3.9 0.26609280 0.2940686663 ## 2 4.20 4.3 4.3 0.25062833 0.1201644267 ## 3 5.00 6.0 6.0 0.15151904 0.0347793264 ## 4 5.50 5.9 5.9 0.11178988 0.0026090653 ## 5 6.00 6.1 6.1 0.08914853 0.0007585906 ## 6 6.15 6.3 6.3 0.08568825 0.0029898530 ## 7 6.40 5.8 5.8 0.08333867 0.0085341275 ## 8 7.00 7.9 7.9 0.09512926 0.1169635836 ## 9 7.20 6.4 6.4 0.10452756 0.0038328585 ## 10 7.70 7.8 7.8 0.13998476 0.0808111497 ## 11 8.00 8.0 8.0 0.16946124 0.1138881520 ## 12 9.70 6.1 NA 0.45269169 2.8757588661 As you can see, datapoint 12 (x=9.7) has high leverage and it's also an outlier. Consequently it has a very large Cook's Distance. Typically a Cook's distance &gt; 1 requires more consideration about how to proceed with your model. You can also quickly get a plot of the Cook's Distances of all datapoints from a regression model like this: plot(mod.out2, which = 4) 11.6.5.4 Checking Influence: Let's look for influential datapoints in our Sleep vs Grumpiness Data. At the beginning of this section, we saved the linear model as mod. We can look at the Cook's Distances with the cooks.distance() function, or we could just plot them: plot(mod1, which = 4) This plot shows each of our 100 datapoints in the order they appear in the original dataframe along the x-axis. Clearly, looking at the y-axis, all our datapoints have Cook's distances of far below 1, so we are fine to assume that we have no overly influential datapoints. 11.7 Examining individual predictor estimates After checking if our model is a good fit and making sure that we did not violate any of the assumptions of the linear regression, we can finally move forward with what we came here to do. That is, we can check whether our predictor is meaningfully useful in predicting our outcome variable. This means, is our observed value of b sufficiently different from 0? To do this, we use two strategies. First, we can generate a 95% confidence interval around our estimate of b. That is the method that we prefer. Secondly, you can run a significance test. 11.7.1 95% confidence interval of 'b'. First, let's do this the easy way. Then we'll work out how we got the 95% confidence interval. You can simply find this by running the confint() function on our regression model object: # easy way - gives us the confidence interval: coefficients(mod1) ## (Intercept) dan.sleep ## 125.956292 -8.936756 confint(object = mod1, level = .95) ## 2.5 % 97.5 % ## (Intercept) 119.971000 131.94158 ## dan.sleep -9.787161 -8.08635 This tells us that the lower bound of the 95% confidence interval of b is -9.79, and the upper bound is -8.09, with our estimate of b being -8.94. This means that if we were to sample 100 days randomly over and over again and measure Dan's sleep and grumpiness on these days, in 95% of the samples we collect we would have the true population parameter value of b. In lay terms, we can suggest that there is approximately a 95% likelihood that this true population value of b is between -9.79 and -8.09. The only way to really get the true population value would have been to measure Dan's sleep and grumpiness for every day that Dan has been alive. In big picture terms, what we can draw from this 95% confidence interval is that the relationship between sleep and grumpiness is highly negative and clearly strong. There really seems to be close to no chance that 0 could be the value of b - it is not inside the confidence interval. In fact, we could generate a ridiculously confidence confidence interval, such as a 99.9999% confidence interval: confint(object = mod1, level = .999999) ## 0.00005 % 99.99995 % ## (Intercept) 110.21037 141.702208 ## dan.sleep -11.17398 -6.699536 ... and 0 is still nowhere near being included. How is this 95% confidence interval calculated? To do this we need to think about those sampling distributions again, and the standard error of b. 11.7.2 Standard Error of b As we've discussed earlier, our one single estimate of b is just one possible estimate. We estimated this value based on our one single sample of 100 days. However, if we had sampled a different sample of 100 days, we would have got a slighlty (or maybe greatly) different estimate of b. If we repeated this procedure thousands of times, then we'd have a sampling distribution of b's. This sampling distribution will be t-distribution shaped and has degrees of freedom = number of observations - the number of predictors (1) - 1. Therefore it is t-distribution shaped with d.f.=98. If we know the standard deviation of this sampling distribution (which is known as the standard error of b - \\(s_{b}\\)), then we can create confidence intervals. However, it turns out that calculating \\(s_{b}\\) is a bit annoying. Fortunately, there is a quick way to find it out by looking at the summary of the model output: summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 We can see here that \\(s_{b} = 0.4285\\) If you're interested, then this is the formula to calculate \\(s_{b}\\): \\(s_{b} = \\frac{\\sigma_{est}}{\\sqrt{\\Sigma (x - \\overline{x})}}\\) Here's how we calculate it by hand according to this formula: x_dif &lt;- df$dan.sleep - mean(df$dan.sleep) # difference of each x from mean of x ssx &lt;- sqrt(sum(x_dif^2)) # square root of the sum of these differences squared sb &lt;- s_est/ssx #s_est was calculated earlier sb # this is the standard error of b ## [1] 0.4285351 11.7.3 Calculating 95% confidence interval of 'b' by hand We can use the following formula to calculate the 95% CI for b: \\(CI_{95} = b +/- t * s_{b}\\) As with all confidence intervals, what we do is to presume that our estimate of b is the mean of the sampling distribution. Then knowing that the distribution is t-shaped with d.f.=98, we need to find the value of t that will leave 2.5% of the distribution in each tail. We can look that up in R using qt: tval &lt;- qt(.975, df = 98) tval ## [1] 1.984467 We've now calculated everyting we need to for the Confidence Interval b + (tval * sb) #upper bound ## [1] -8.086342 b - (tval * sb) # lower bound ## [1] -9.78717 As we can see, these match up with the output when using confint(): confint(mod1) ## 2.5 % 97.5 % ## (Intercept) 119.971000 131.94158 ## dan.sleep -9.787161 -8.08635 11.7.4 Signifcance Testing b We can also use this sampling distribution to apply a signifiance test. Our null hypothesis will be that the population value of b is 0. Our alternative hypothesis will be that b is not equal to 0. \\(H_{0}: b=0\\). \\(H_{1}: b\\neq0\\) Given we know the standard deviation of this sampling distribution, \\(s_{b}\\), we can calculate how far away our observed sample value of \\(b\\) is in terms of how many standard deviations from the mean of 0 it is. We call this value a t-statistic and it is calculated like this: \\(t = \\frac{b}{s_{b}}\\) Once we have calculated our t-statistic, then we can determine given the shape of the distribution and the degrees of freedom, what proportion of the distribution is more extreme than our observed value. tobs &lt;- b/sb tobs #-20.854 ## [1] -20.8542 Our observed value is therefore \\(t = -20.854\\). Let's look at this graphically: comparing a and b This shows the t-distribution for d.f. = 98. It looks very strange because we've extended the axes to -21 and +21. We did this so that we could include a dotted blue line for our observed t-value of -20.854. Notice that by the time the t-distribution of d.f.=98 gets to close to a value of t=-3 then there is almost nothing left in the tail of the distribution. Just for completeness, we can calculate what proportion of times we observe a t-value of more extreme (i.e. less than) -20.854 using pt(). pt(-20.854, df=98) ## [1] 4.094577e-38 This essentially gives us a one-tailed p-value of p = 0.00000000000000000000000000000000000004094577, which is very small. To get a 2-tailed p-value we just double this value. Essentially what we can conclude here is that our one value of b is extremely unlikely to have come from a population where b=0, and thus we reject our null hypothesis and accept the alternative. The preceding information was provided to help you think about what's really going on when we calculate the t-statistic. However, tt turns out there is actually a shortcut way of calculating t using n and the Pearson's correlation coefficient r. It's the following formula: \\(t = \\frac{r \\times \\sqrt{n-2}}{\\sqrt{1 - r^{2}}}\\) r &lt;- cor(df$dan.sleep, df$dan.grump) n &lt;- nrow(df) (r * sqrt(n-2)) / (sqrt(1-r^2)) # -20.85 ## [1] -20.8544 As you can see from the above code - it works! "],
["permutation-testing.html", "12 Permutation Testing 12.1 t-test Permutation 12.2 Correlation Coefficient Permutation Tests", " 12 Permutation Testing An example of permutation testing and the theory behind it. 12.1 t-test Permutation Let's look at our two independent samples of exam scores: library(tidyverse) anastasia &lt;- c(65, 74, 73, 83, 76, 65, 86, 70, 80, 55, 78, 78, 90, 77, 68) bernadette &lt;- c(72, 66, 71, 66, 76, 69, 79, 73, 62, 69, 68, 60, 73, 68, 67, 74, 56, 74) # put into a dataframe: dd &lt;- data.frame(values = c(anastasia, bernadette), group = c(rep(&quot;Anastasia&quot;,15), rep(&quot;Bernadette&quot;, 18)) ) dd ## values group ## 1 65 Anastasia ## 2 74 Anastasia ## 3 73 Anastasia ## 4 83 Anastasia ## 5 76 Anastasia ## 6 65 Anastasia ## 7 86 Anastasia ## 8 70 Anastasia ## 9 80 Anastasia ## 10 55 Anastasia ## 11 78 Anastasia ## 12 78 Anastasia ## 13 90 Anastasia ## 14 77 Anastasia ## 15 68 Anastasia ## 16 72 Bernadette ## 17 66 Bernadette ## 18 71 Bernadette ## 19 66 Bernadette ## 20 76 Bernadette ## 21 69 Bernadette ## 22 79 Bernadette ## 23 73 Bernadette ## 24 62 Bernadette ## 25 69 Bernadette ## 26 68 Bernadette ## 27 60 Bernadette ## 28 73 Bernadette ## 29 68 Bernadette ## 30 67 Bernadette ## 31 74 Bernadette ## 32 56 Bernadette ## 33 74 Bernadette We can plot these data as boxplots to get a sense of the within group variation as well as the observed differences between the groups: ggplot(dd, aes(x = group, y = values, fill = group)) + geom_boxplot(alpha=.3, outlier.shape = NA) + geom_jitter(width=.1, size=2) + theme_classic() + scale_fill_manual(values = c(&quot;firebrick&quot;, &quot;dodgerblue&quot;)) Now, from our two independent samples, we can directly observe what the difference in sample means is. This is just calculated by subtracting one sample mean from the other: meandif &lt;- mean(anastasia) - mean(bernadette) # 5.48 meandif ## [1] 5.477778 So, from our samples, we observed a difference in grades of 5.48 between the groups. Typically, we would run an independent t-test to test whether these two samples came from theoretical populations that differ in their means: t.test(anastasia, bernadette, var.equal = T) ## ## Two Sample t-test ## ## data: anastasia and bernadette ## t = 2.1154, df = 31, p-value = 0.04253 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1965873 10.7589683 ## sample estimates: ## mean of x mean of y ## 74.53333 69.05556 This Student's t-test (notice var.equal=T) suggests that this is a significant difference, meaning that the groups do differ in their population means. However, this test relies on several assumptions (see section xx.x.x). Instead, we could apply a permutation test that is free of assumptions. Essentially what we are going to do is ask how surprising it was to get a difference of 5.48 given our real data. Put another way, if we shuffled the data into different groups of 15 and 18 (the respective sample sizes of Anastasia and Bernadette), would we get a difference in sample means of greater or lower than 5.48? If we did this thousands of times, how many times would we get differences in sample means above 5.48? Let's apply this theory to just one permutation. First, we combine all the data: set.seed(1) # just to keep the random number generator the same for all of us allscores &lt;- c(anastasia, bernadette) allscores ## [1] 65 74 73 83 76 65 86 70 80 55 78 78 90 77 68 72 66 71 66 76 69 79 73 62 69 ## [26] 68 60 73 68 67 74 56 74 Next, we shuffle them into new groups of 15 and 18.: x &lt;- split(sample(allscores), rep(1:2, c(15,18))) x ## $`1` ## [1] 80 78 71 73 65 68 67 74 72 74 76 83 68 70 69 ## ## $`2` ## [1] 74 90 69 68 78 66 73 76 62 56 79 65 60 73 55 77 66 86 We have two brand new samples that contain all of the scores from our original data, but they've just been shuffled around. We could look at what the difference in sample means is between these two new samples: x[[1]] # this is our shuffled sample of size 15 ## [1] 80 78 71 73 65 68 67 74 72 74 76 83 68 70 69 x[[2]] # this is our shuffled sample of size 18 ## [1] 74 90 69 68 78 66 73 76 62 56 79 65 60 73 55 77 66 86 mean(x[[1]]) # mean of the new sample of size 15 ## [1] 72.53333 mean(x[[2]]) # mean of the new sample of size 18 ## [1] 70.72222 # what&#39;s the difference in their means? mean(x[[1]]) - mean(x[[2]]) ## [1] 1.811111 The difference in sample means is 1.81, which is a lot smaller than our original difference in sample means. Let's do this same process 10,000 times! Don't worry too much about the details of the code. What we are doing is the above process, just putting it in a loop and asking it to do it 10,000 times. We save all the results in an object called results. results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ x &lt;- split(sample(allscores), rep(1:2, c(15,18))) results[[i]]&lt;-mean(x[[1]]) - mean(x[[2]]) } head(unlist(results)) # these are all our mean differences from 10,000 shuffles of the data. We&#39;re just looking at the first 6. ## [1] -1.8555556 -2.5888889 4.0111111 -3.9333333 0.2222222 3.5222222 We can actually make a histogram showing the distribution of these differences in sample means. df &lt;- data.frame(difs = unlist(results)) ggplot(df, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;green&quot;, alpha=.4) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = 5.48) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This histogram shows that for some of our 10,000 shuffles, we actually got some differences between our two samples of higher than 5.48 (the dotted blue line), but the vast majority of shuffles led to samples that had mean differences lower than 5.48. In fact, several shuffles led to samples where the sample of size 18 (Bernadette in the original data) had a sample mean that was higher than the sample of size 15 (Anastasia in the original data). We can directly calculate how many times out of 10,000 shuffles we got a difference in sample means that was greater than 5.48 sum(unlist(results) &gt; 5.48) # 202 times out of 10000 ## [1] 215 To convert this to a p-value, we simply divide this value by the number of shuffles we ran - which was 10,000. sum(unlist(results) &gt; 5.48) /10000 # which is 0.0202 proportion of the time ## [1] 0.0215 So our p-value is p=0.0215 which is similar to a one-tailed p-value. 12.1.1 Example 2: Let's take a look at a second example. Here, we have various subjects rating their anxiety levels. They do this after either taking a new anxiolytic drug or a placebo. The subjects in each group are independent of each other. The placebo group has 19 subjects and the drug group has 21 subjects. The data: placebo &lt;- c(15, 16, 19, 19, 17, 20, 18, 14, 18, 20, 20, 20, 13, 11, 16, 19, 19, 16, 10) drug &lt;- c(15, 15, 16, 13, 11, 19, 17, 17, 11, 14, 10, 18, 19, 14, 13, 16, 16, 17, 14, 10, 14) length(placebo) #19 ## [1] 19 length(drug) #21 ## [1] 21 If we were interested in doing a Student's t-test, we might want to check whether the data are approximately normal. We could perform Shapiro-Wilk tests to do this: shapiro.test(drug) # approximately normal as p&gt;.05 ## ## Shapiro-Wilk normality test ## ## data: drug ## W = 0.95184, p-value = 0.3688 shapiro.test(placebo) # not enough evidence to be normal as p&lt;.05 ## ## Shapiro-Wilk normality test ## ## data: placebo ## W = 0.88372, p-value = 0.02494 From this we find that the placebo group is not approximately normally distributed (p value of the Shapiro-Wilk test is &lt;.05). We could do a non-parametric test such as Wilcoxon Ranked Sum test (see xxx.xxx), but an alternative strategy is to perform a permutation test. Let's first plot the data, and then look at our observed difference in anxiety scores between our two independent samples: # put into dataframe - long format df &lt;- data.frame(anxiety = c(placebo, drug), group = c(rep(&quot;placebo&quot;, length(placebo)), rep(&quot;drug&quot;, length(drug)) ) ) head(df) ## anxiety group ## 1 15 placebo ## 2 16 placebo ## 3 19 placebo ## 4 19 placebo ## 5 17 placebo ## 6 20 placebo #boxplots ggplot(df, aes(x=group, y=anxiety, fill=group)) + geom_boxplot(outlier.shape = NA, alpha=.4) + geom_jitter(width=.1) + theme_classic() + scale_fill_manual(values=c(&quot;orange&quot;, &quot;brown&quot;)) mean(placebo) - mean(drug) #2.128 ## [1] 2.12782 So our observed difference in sample means is 2.128. In the permutation test, what we'll do is shuffle all the scores randomly between the two groups, creating new samples of the same size (19 and 21). Then we'll see what difference in sample means we get from those shuffled groups. We'll also do this 10,000 times. allvalues &lt;- c(placebo, drug) results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ x &lt;- split(sample(allvalues), rep(1:2, c(19,21))) results[[i]]&lt;-mean(x[[1]]) - mean(x[[2]]) } head(unlist(results)) # these are the first six of all our mean differences from 10,000 shuffles of the data. ## [1] -0.8796992 -0.7794486 -1.2807018 -0.4786967 2.5288221 1.1253133 Let's plot the distribution of these data to see what proportion of times our shuffled groups got samples that were greater than 2.128. df0 &lt;- data.frame(difs = unlist(results)) ggplot(df0, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, alpha=.4) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = 2.128) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. It looks like very few times did we get two samples that had differences in sample means that were greater than 2.128. We can calculate exactly how many times, and express this as the proportion of times we got a difference in sample means greater than 2.128: sum(unlist(results) &gt; 2.128) # 109 times out of 10000 ## [1] 113 sum(unlist(results) &gt; 2.128) /10000 # which is 0.0109 proportion of the time ## [1] 0.0113 So, in this case we can say that the probability of getting a difference in sample means between the drug and placebo groups that was larger than our observed difference of 2.128 was p = 0.0109. This is very strong evidence that the observed difference is significantly greater than we'd expect by chance. 12.2 Correlation Coefficient Permutation Tests You can apply the logic of permutation tests to almost any statistical test. Let's look at an example for Pearson correlations. In these data, we are looking at 15 subjects who are completing a task. We measured the time they spent on the task and their high scores. library(tidyverse) df &lt;- read_csv(&quot;data/timescore.csv&quot;) ## Parsed with column specification: ## cols( ## subject = col_character(), ## time = col_double(), ## score = col_double() ## ) head(df) ## # A tibble: 6 x 3 ## subject time score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1A 5.5 3 ## 2 2B 2.4 6.9 ## 3 3C 8.8 17.9 ## 4 4D 7 10.5 ## 5 5E 9.3 12.2 ## 6 6F 2.5 3.5 If we make a scatterplot of the data, we can see that those who spent longer on the task tended to get higher scores: # scatterplot ggplot(df, aes(x = time, y = score)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) Using a standard approach, we could find the correlation of these two variables and run a signficance test using cor.test(). We can see that there is a moderate Pearson's r of r=0.55 which is statistically significant (p=0.031). # regular significance test cor.test(df$time,df$score) #r=0.55, p=0.031 ## ## Pearson&#39;s product-moment correlation ## ## data: df$time and df$score ## t = 2.4258, df = 13, p-value = 0.03057 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.0643515 0.8324385 ## sample estimates: ## cor ## 0.5582129 We could take an alternative tack, and decide to do a permutation test. The idea here is again, how surprising is it to get a correlation of 0.55 with these data? Were there other ways of ordering the x and y variables to get higher correlation coefficients? Let's look at our y axis variable, the score: set.seed(1) # just doing this so all our results look same df$score # actual data in order ## [1] 3.0 6.9 17.9 10.5 12.2 3.5 11.0 7.6 8.4 13.4 10.1 9.0 10.1 17.7 6.8 This is the original order of the data. If we use sample() we can shuffle the data: sample(df$score) # actual data but order shuffled ## [1] 10.5 3.5 7.6 10.1 17.9 8.4 13.4 17.7 12.2 3.0 6.9 10.1 9.0 6.8 11.0 Let's shuffle the score again, but this time store it in the original dataframe: df$shuffle1 &lt;- sample(df$score) #create a new column with shuffled data df ## # A tibble: 15 x 4 ## subject time score shuffle1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1A 5.5 3 7.6 ## 2 2B 2.4 6.9 10.1 ## 3 3C 8.8 17.9 10.1 ## 4 4D 7 10.5 12.2 ## 5 5E 9.3 12.2 8.4 ## 6 6F 2.5 3.5 13.4 ## 7 7G 4.8 11 6.9 ## 8 8H 4.1 7.6 3.5 ## 9 9I 5 8.4 3 ## 10 10J 2.9 13.4 17.7 ## 11 11K 6.4 10.1 6.8 ## 12 12L 7.7 9 11 ## 13 13M 9.3 10.1 9 ## 14 14N 8.3 17.7 17.9 ## 15 15O 5.1 6.8 10.5 If we plot this shuffled y (score) against the original x (time), we now get this scatterplot, which basically shows no relationship: # this is what that new column looks like: ggplot(df, aes(x = time, y = shuffle1)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) And the correlation for this new scatterplot is really close to 0! r = 0.0005: cor.test(df$time, df$shuffle1) # now relationship is a bit negative ## ## Pearson&#39;s product-moment correlation ## ## data: df$time and df$shuffle1 ## t = 0.0016429, df = 13, p-value = 0.9987 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.5119267 0.5125988 ## sample estimates: ## cor ## 0.0004556502 We could shuffle the score variable even more times, and directly calculate the r value aginst the time variable for each shuffle using cor(). # we can do this many times cor(df$time, sample(df$score)) # r = 0.35 ## [1] 0.3023584 cor(df$time, sample(df$score)) # r = 0.04 ## [1] -0.05905503 cor(df$time, sample(df$score)) # r = -0.06 ## [1] -0.4665168 cor(df$time, sample(df$score)) # r = 0.15 ## [1] -0.435933 As you can see, the more shuffles we do, we get varied values of r. What we really should do is perform 10,000 (or another really high number) shuffles of the score variable and re-calculate r against the time variable for all 10,000 of these shuffles. Don't worry about the code below, but that's exactly what we're doing. We're saving the r values from the 10,000 shuffles in the object called results. results &lt;- vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- cor(df$time, sample(df$score)) } head(unlist(results)) # this are the correlations for the first 6 of 10,000 shuffles ## [1] 0.274190962 0.005288304 -0.114492469 -0.280528642 0.235874922 ## [6] 0.061278049 We can plot the results in a histogram, and also put a vertical line at 0.56 which was our original observed correlation between time and score from the raw unshuffled data. results.df &lt;- data.frame(x = unlist(results)) ggplot(results.df, aes(x)) + geom_histogram(color=&quot;darkgreen&quot;,fill=&quot;lightseagreen&quot;) + geom_vline(xintercept = 0.56, lwd=1, lty=2) + xlab(&quot;r&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. As you can see, there were a few shuffles (or permutations) that we got an r value of greater than 0.56, but not that many. In fact, we can directly calculate how many: sum(unlist(results) &gt; 0.56) #163 were greater. ## [1] 163 It turns out that 163 times out of 10,000 shuffles we got a r value of greater than 0.56. WE could calculate this as a proportion by dividing by 10,000: sum(unlist(results) &gt; 0.56) / 10000 #0.0163 ## [1] 0.0163 We can use this value as our p-value. Because it is relatively low, we could argue that we were very unlikely by chance alone to have got a r value of 0.56 from our data. This suggests that the correlation between time and score is significant. The advantages of running a permutation test is that it is free of the assumptions of normality for the Pearson's r correlation signifiance test. It's also a cool method, and pretty intuitive. "]
]
