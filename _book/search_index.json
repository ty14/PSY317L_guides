[
["index.html", "PSY317L Guides 1 Welcome to PSY317! 1.1 What this book includes and what it doesn't 1.2 How to use this guide 1.3 Acknowledgements 1.4 References 1.5 Other places to find help about R 1.6 Other places to find help about R and Statistics", " PSY317L Guides James P. Curley &amp; Tyler M. Milewski June 28, 2020 1 Welcome to PSY317! The following is a simple guide to R studio. What this guide contains.... That it's mainly a how to implement stuff in R, but it also touches on bits of stats and data theory where applicable 1.1 What this book includes and what it doesn't This is in between a textbook and a study guide. We're trying to build materials that will enable students to quickly find what they're looking for to help them understand these statistical concepts. This book is primarily aimed at the content in PSY317L (and PSY1XXR), but occasionally we describe concepts that we don't directly cover in these courses. This is when we feel that it's worth explaining things in a bit more detail for those students that want to know a bit more about a subject. 1.2 How to use this guide This guide covers the material presented in the video segments. In some parts, we go beyond what is presented. This is to supplement the material and to help you understand the content or to round out some subjects a little more. You will not be tested directly on information that is in this guide if it did not appear in the video segments. Quizzes and tests are only based on the videos. This guidebook is exactly that - an extra reference to help you understand that material and a place to look for help or answers. 1.3 Acknowledgements This guidebook is built upon the work of several others' teaching materials. In particular, Danielle Navarro's book.... also some examples come from Andy Field's intro to statistics using R 1.4 References The following books are all freely avaiable online (like this one!) and are helpful resources wilke - visualizing wickham grolemund - intro R other wickham books? danielle navarro - intro stats 1.5 Other places to find help about R Rstudio - cheatsheets, community pages ??? 1.6 Other places to find help about R and Statistics With statistics stuff, you can never really have enough resources. Although throughout this course we try to explain difficult concepts in terms we hope all students can understand, sometimes you just need to read or hear about these concepts several times before they sink in. Othertimes, it's not until somebody else explains it in a slightly different way that it finally finds a home in your brain. To that end, we recommend looking over some other help tutorials and reading. You certainly don't have to go looking at these materials - but if you feel like hearing a different voice, then these are some that I recommend. Albert Kim stuff? Andrew Field's statistics in R book - probably the next level up from introduction in terms of R. Khan Academy videos.(not on R though) "],
["introduction.html", "2 Introduction 2.1 Downloading R 2.2 Downloading RStudio 2.3 Using RCloud instead of RStudio 2.4 Importing Data 2.5 The RStudio Environment 2.6 The Command Prompt 2.7 What is an RScript File? 2.8 What are Packages 2.9 Project Folders and Working Directories 2.10 Where to Get Help for R stuff 2.11 Quitting R", " 2 Introduction How to get started with R and Rstudio. The initial steps are downloading and installing R. This may differ slightly based on your machine. 2.1 Downloading R Go to the R website. The first box has options for downloading R for Mac, Windows or Linux. Click on the appropriate link. Then you will want to click on \\(\\color{blue}{\\text{&quot;install R for the first time&quot;}}\\). Then follow the instructions. 2.2 Downloading RStudio Go to the Rstudio website. Click DOWNLOAD under RStudio Desktop, open source license FREE. Then click on the appropriate installer for your computer (Mac, Windows, or Linux). Run the installer and follow the instructions. You will always use Rstudio, when working with data. 2.3 Using RCloud instead of RStudio blah blah blah DO we really need this? RCloud 2.4 Importing Data There are different options for importing data. It's possible to import data of all different formats into RStudio. We will primarily use spreadsheet type files that have been saved with the &quot;.csv&quot; suffix. These are called 'comma separated files'. Option 1. Import Tab You can click on the &quot;Import Dataset&quot; tab in the top right of RStudio - it's located just above your global environment. Depending on your RStudio version, you will be asked to select from a dropdown menu. When importing .csv files, you want to select From CSV... if your menu looks like this: If your menu looks like the underneath, then you'll want to select From Text (readr): Option 2. Writing code. This is the option that we will use in our scripts for this course. You may notice that all the datasets that we wish to use are in a folder called &quot;data&quot;. To read in any of these datasets, what we need to do is use the read_csv() function. This comes from a package contained with the tidyverse package, so we must have imported that library first. We then tell it which dataset to find within the 'data' folder. We use the notation &quot;data/...&quot; to tell it to look inside the data folder. For instance, if we wished to load in the bmi.csv dataset, and assign it the name bmi, we would do it like this - make sure to put quotes around the whole file name and location: library(tidyverse) #load package ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.3 ## Warning: package &#39;tibble&#39; was built under R version 3.5.3 ## Warning: package &#39;tidyr&#39; was built under R version 3.5.3 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.3 bmi &lt;- read_csv(&quot;data/bmi.csv&quot;) # bring in data head(bmi) # first six rows ## # A tibble: 6 x 9 ## id age bmi chol insulin dbp sbp smoke educ ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 18.3 15.4 47.2 2.78 29.5 49.7 1 D ## 2 2 19.4 14.5 52.1 3.47 31.3 49.0 2 C ## 3 3 21.1 16.0 52.2 4.06 32.4 51.7 1 B ## 4 4 21.3 19.5 53.2 4.48 32.0 NA 2 A ## 5 5 21.1 18.6 55.4 5.34 33.7 53.8 1 D ## 6 6 23.9 19.5 54.3 6.29 35.0 56.0 2 C tail(bmi) # last six rows ## # A tibble: 6 x 9 ## id age bmi chol insulin dbp sbp smoke educ ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 15 34.8 30.4 63.6 15.3 42.7 64.8 1 B ## 2 16 33.6 29.9 65.5 NA 46.4 65.6 2 A ## 3 17 33.4 NA 65.2 18.2 46.2 64.6 1 D ## 4 18 35.1 32.5 68.6 19.7 45.8 66.1 2 C ## 5 19 35.0 33.4 NA 21.1 NA 68.4 1 B ## 6 20 37.5 34.1 68.0 22.1 49.3 68.6 2 A If you want to see more on what the data looks like the following functions can help. nrow(bmi) # how many rows in dataset ## [1] 20 ncol(bmi) # how many columns in dataset ## [1] 9 colnames(bmi) # column names ## [1] &quot;id&quot; &quot;age&quot; &quot;bmi&quot; &quot;chol&quot; &quot;insulin&quot; &quot;dbp&quot; &quot;sbp&quot; ## [8] &quot;smoke&quot; &quot;educ&quot; If you just want to view your entire dataset, there is a function for that. View(bmi) You can also see the struture of your data, whether each variable is a number, character, factor, etc. This will become extremely important when trying to graph and analysis different types of data. str(bmi) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 20 obs. of 9 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ age : num 18.3 19.4 21.1 21.3 21.1 ... ## $ bmi : num 15.4 14.5 16 19.5 18.6 ... ## $ chol : num 47.2 52.1 52.2 53.2 55.4 ... ## $ insulin: num 2.78 3.47 4.06 4.48 5.34 ... ## $ dbp : num 29.5 31.3 32.4 32 33.7 ... ## $ sbp : num 49.7 49 51.7 NA 53.8 ... ## $ smoke : int 1 2 1 2 1 2 1 2 1 2 ... ## $ educ : chr &quot;D&quot; &quot;C&quot; &quot;B&quot; &quot;A&quot; ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 9 ## .. ..$ id : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ age : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ bmi : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ chol : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ insulin: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ dbp : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ sbp : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ smoke : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ educ : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; Here you can see that all variables are numeric, except for education with is a character. If you need to change the struture of a variable say to a factor to group data in a certain way you would do the following: bmi$smoke &lt;- as.factor(bmi$smoke) The $ basically allows you to call certain columns in a datframe. The code bmi$smoke is allowing us to only change that column, or variable to a factor without changing anything else in the dataframe. You can also change variables to characters with as.character() and to numbers with as.number(). 2.5 The RStudio Environment RStudio Environment You Rstudio enviornment is separated into 4 main panes. On your machine these panes by be organized differently. You can also change the order of these panes by going your tools tab, then clicking Global preferences. First, in the top left, you have a source pane - or script. This is where you will be writing most of your code. You will have to run the code, before seeing the output in the console. These pane is important because this is where you can write and save code for future use. Second, on the bottom left, there is the Console. All of the script's output will be assess and found in the console. However, objects in the console will not be saved. You can also, type right in the console for fast calculations, like this: 4+5 ## [1] 9 Third, you have a Gobal Environment and History in the top right. Your gobal environment will display all the names of objects you defined in your script. Here is an example of a Gobal Environment where you can see dataframes, matrices, values, and functions: Global Environment If you click the play arrow next to the dataframe, you can view the entire dataframe. This is the save as running the function View(df). Another, important button is the broom icon, if you click this it will clear your gobal enviornment and you will have to rerun code to get any dataframes, function, etc. back. In the History tab, you can see all the code that was once ran through your console. This tab can be important, because as I said before information in the console is never saved and can be deleted easily. Nevertheless, you should get use to saving your code in R scripts, so you will probably not use this tab to often. The fourth pane, is the most compacted and has tabs for files, plots, packages, and help. The file tab, allows you access all files on your hard drive. The Plot tab, will be filled with plots generated from your code. There are puttons right under the tab to Zoom and Export plots. There is also a button which looks like a broom, this will clear the plots. The Packages tab, has a User Library which is just a list of all the packages you already have install on your machine. You can also install a package, by hitting the install button and then search for your wanted package. The update button will update all packages you already have installed. The Help tab, will give you information to help understand different functions and commands in R. You can also call for help with a function right from the console like this: ?t.test 2.6 The Command Prompt There are several ways to run the code in your script file. You can highlight your code, then hit 'Run' in the top right corner of the script file. You can click anywhere on the code, then hit 'Run' You can highlight or click on the code you want to run, then use the keyboard shortcut with is “Command + Return” on Mac, or “Control + Enter” on PC 2.7 What is an RScript File? console vs script. what a # does in a script .... Saving Scripts etc. 2.8 What are Packages include the code they should run at the beginning of this course to have all packages installed. 2.9 Project Folders and Working Directories how projects and folders are managed. The top level of the the project folder is the one that contains all the folders and looks like this: This is where R will automatically look for files when you ask it to find stuff..... and how to refer to folders... e.g. data/cheese.csv - we write it like this as it's the cheese.csv file that exists in the folder data. If you ever need to check where R is looking for things, run getwd() and it will tell you where it thinks you are currently working from. 2.10 Where to Get Help for R stuff help guides.... how can you use them.? ? stackoverflow rstudio cheatsheets navarro textbook wilke textbook our guidebook 2.11 Quitting R how to quit from R successful, you don't have to hit that save object thing all the time.... "],
["basic-syntax.html", "3 Basic Syntax 3.1 Boring mathematical stuff 3.2 assignment 3.3 vectors 3.4 Characters 3.5 Naming of objects 3.6 Logical Operators 3.7 Some things that are useful to know. 3.8 Error Messages 3.9 Functions 3.10 Chaining Syntax", " 3 Basic Syntax Before we move forward it is important to know a little bit of R syntax. This chapter contains lots of information about how R is organized and works. Don't feel like you have to learn all of this now. This chapter is here for reference and for help. Also, these nuts and bolts of programming languages are things that you pick up over time - they are pretty hard to remember at first and you constantly do have to check and copy/paste things. That's ok. Another very helpful resource on basic syntax in R is Chapter 3 of Danielle' Navarro's book which we highly recommend. 3.1 Boring mathematical stuff R is, first and foremost, a statistical programming language. So, unsurprisingly, you can do a bunch of mathematical operations fairly easily. Let's do some really tedious math: You can add with the + key: 4 + 6 ## [1] 10 You can see that the output returned 10 indicating that that is the answer to 4 + 6. However, we also see in the output a [1] before it. What does this mean? This is an index - which we'll talk about in more detail later- but essentially all it is telling us is that the number that comes directly after it is the first bit of the output. We only have one thing in our output here - the number 10, so it's pretty pointless - but it does have relevance in the future. For now though, you can safely ignore it. Subtracting uses a hyphen or minus key -: 43 - 18 ## [1] 25 Multiplying uses the asterisk *: 5 * 3 ## [1] 15 Dividing uses this slash key /: 34 / 7 ## [1] 4.857143 You can square root like so by typing sqrt() and putting your number to square root in the brackets: sqrt(45) ## [1] 6.708204 Squaring a number requires us to use the cute ^ key followed by a number 2 to indicate that we are raising to the power 2 (which is squaring): 12^2 ## [1] 144 We can also get the logarithm of numbers: log(12) ## [1] 2.484907 Our first little warning Like all programming languages, R will only do what it's told. One thing that catches people out is that it will read code in order from left to right. So, if you're doing several things at once, you may need to insert brackets to make sure that it does what you want it to. For instance, say you want to multiply 5 by the sum of 7 and 9. You might write that like this: 5 * 7 + 9 ## [1] 44 but 44 is not the answer you're looking for! What it has done is to take 5, then multiply it by 7 (makes 35) and then add 9 to make 44. OK, how about you write it like this instead: 7 + 9 * 5 ## [1] 52 Erm, this time it multiplied 9 by 5 to get 45 and then added 7 to make 52. What is happening is that some mathematical operations are taking precedence over others and R does things in certain order. To explicitly make sure that it does what you want, insert some brackets like this: 5 * (7 + 9) ## [1] 80 Now it knows that what you wanted to do was to add 7 and 9 to get 16 and have that number multiplied by 5 to get 80. This sort of thing is something to look out for, but as you get experienced you'll have a sense for it. Well placed brackets can save you a headache. 3.2 assignment e.g. what the assignment operator is. This symbol &lt;- which is just a &quot;greater than sign&quot; followed by a &quot;hyphen&quot; is called an assignment operator. It basically is equivalent to saying you want to save something. You write what you want to be saved on the right hand side of it, and the name of your newly saved thing on the left of it. We call the 'thing' that you've saved an 'object' in programming speak. For instance, say you wanted to save the number 17, and you wanted to call that saved number an object called x. You'd do it like this: x &lt;- 17 Now, whenever you type, x, R thinks that it is the number 17. x ## [1] 17 We should probably fess up right now and tell you that there is another way that you can assign things in R. On the face of it, it's a much easier way too, but we're going to recommend you don't do it. The only reason that we're bringing it up at this point is that if you look up help on the internet or in some books, you'll see people doing it - so we should mention it. You can assign using the equal key = like this: y = 10 - 2 y ## [1] 8 So, as you can see, we created an object called y that was equal to 10 minus 2, that is 8. Using = seems so much easier than using &lt;- so what is the reason not to do it? Well, the equal sign gets used for a ton of other commands and sometimes it gets a little messy and confusing if too many equal signs fly around. Thefore, we're going to politely ask that whenever you assign things, please use the &lt;- operator, even though it's two bits of punctuation stuck together: hooray &lt;- 17 + 4 # thanks for using this sign hooray ## [1] 21 3.3 vectors Of course, you can save even more complex things as the object. For example, if you wished to save the numbers 5, 6, 7, 8, 9, 10, you have two ways of doing that. Let's just see them in action, and then we'll explain the syntax: v1 &lt;- 5:10 v1 ## [1] 5 6 7 8 9 10 v2 &lt;- c(5,6,7,8,9,10) v2 ## [1] 5 6 7 8 9 10 As you can see both v1 and v2 are our newly saved objects and they both contain the numbers 5 through 10. For v1 we separated the numbers 5 and 10 with a colon :. In R, the : sign can be used to mean &quot;to&quot; when talking about a series of numbers. e.g. 5:10 ## [1] 5 6 7 8 9 10 101:103 ## [1] 101 102 103 The other way we did it with v2 was to use the c() function. This stands for concatenate which is a mouthful. Basically, it's a way of sticking things together. You write c() and then put the stuff you want to stick together inside the brackets, but make sure you separate each thing by a comma ,. For example, c(1,10,100,1000) ## [1] 1 10 100 1000 c(4,6,8,10,8,6,1) ## [1] 4 6 8 10 8 6 1 Another bit of terminology might be worth mentioning right now. Our saved objects x, v1 and v2, as well as being called objects can also be called vectors. A vector is something in R that contains several items. These items are actually called elements. Importantly the items are indexed, which means that they are in order, not all jumbled up. That means that you can directly grab an element by it's position. So, if you wished to get the first element (or item) of the object v1 you'd type like this: v1[1] ## [1] 5 That returns '5' which was our first element we put into the vector. If we wish to get the 3rd element of v1 we'd do this: v1[3] ## [1] 7 And, if we wished to get the first, second and third element of v1 we'd do this: v1[1:3] ## [1] 5 6 7 We can also get non-consecutive elements of vectors. If we wanted to get the first, fourth and sixth elements of v1 we'd do this: v1[c(1,4,6)] ## [1] 5 8 10 Notice here that you can't do this: v1[1,4,6] ## Error in v1[1, 4, 6]: incorrect number of dimensions You have to put the indexes of the elements you want inside c() and separate each with a comma, if they are non-consecutive. Be careful to make sure you're referring to elements that actually exist. The vector v1 has six elements in total, so if you ask it to give the seventh element, then you get the following: v1[7] ## [1] NA It just says NA. This is R for &quot;nothing&quot; or &quot;doesn't exist&quot; or &quot;missing&quot;. You'll also see NaN in some situations to represent missing data. That's quite a lot about vectors, their elements, and how to index them. Hopefully the below image will help you remember this terminology: Working with Vectors Now, one of the great advantages of vectors is that you can do things to all of the numbers inside the vectors at once. For instance, say we wanted to square all the numbers inside of v1, then all we need to do is: v1^2 ## [1] 25 36 49 64 81 100 and notice how it squared the numbers 5 through 10 that are the contents of v1. You can also do things to the entire contents of a vector at once. For instance, if you wanted to add up all the numbers in v1 you could do this by using the function sum() like this: sum(v1) ## [1] 45 and it added 5+6+7+8+9+10 to give one answer! You can also use the names of the vectors together. For example, if you do v1 + v2 let's see what happens: v1 ## [1] 5 6 7 8 9 10 v2 ## [1] 5 6 7 8 9 10 v1 + v2 ## [1] 10 12 14 16 18 20 You can see that it added the first element of each vector to each other, then the second element of each vector to each other and so on. This only works though because v1 and v2 are the same length. If they had unequal number of elements, R would get mad and not do anything. 3.4 Characters The vectors that we've been looking at so far are all numeric meaning that they contain numbers. Another type of information that R uses is text - or characters. Sometimes called strings. Let's make a vector of characters: the_week &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;) the_week ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; &quot;Saturday&quot; ## [7] &quot;Sunday&quot; Here we have an object called the_week that has 7 elements. Each element is separated by a comma. This is pretty similar to when we enter numbers except for situation when we can use the : shortcut such as using 5:8 to mean 5, 6, 7, 8. We also stick everything together into a vector using c() as we did with numbers. The major difference is that to make sure R knows that each of these words is a character or text string, we have put quote marks around each word. If we didn't do that, it would be looking for objects called Monday, Tuesday etc. Actually, let's see an example of using words to create a vector, when the words are actually objects: aa &lt;- 5 bb &lt;- c(9, 11) cc &lt;- 10 dd &lt;- c(&quot;james&quot;, &quot;tyler&quot;) ee &lt;- &quot;hedgehog&quot; aa ## [1] 5 bb ## [1] 9 11 cc ## [1] 10 dd ## [1] &quot;james&quot; &quot;tyler&quot; ee ## [1] &quot;hedgehog&quot; example &lt;- c(aa, bb, cc, dd, ee) example ## [1] &quot;5&quot; &quot;9&quot; &quot;11&quot; &quot;10&quot; &quot;james&quot; &quot;tyler&quot; &quot;hedgehog&quot; Here, each of aa to ee is assigned to be an object and they contain information. When we write c(aa, bb, cc, dd, ee) we don't need to use commas as we're referring to the objects and not to words called aa, ee etc. Also notice that we were able to make vectors that mixed together numbers and characters. Also, some of our original vectors like bb and dd contained more than one element. When we put them all together with c(aa, bb, cc, dd, ee) to create the example object, R just stuck them all together in the order we wrote. It's pretty unusual though to mix numeric and character vectors together. Let's get back to talking about vectors tha contain characters only. Some functions can work on both character vectors and numeric vectors. One example is length() which counts the number of elements in a vector: v1 ## [1] 5 6 7 8 9 10 the_week ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; &quot;Saturday&quot; ## [7] &quot;Sunday&quot; length(v1) ## [1] 6 length(the_week) ## [1] 7 However, some functions only make sense when working with one type or the other. For instance sum() to add up the elements of a vector, only works with numeric data: sum(v1) ## [1] 45 If you try it with character data, then it will give you an error: sum(the_week) ## Error in sum(the_week): invalid &#39;type&#39; (character) of argument Others work with characters. For example, nchar() calculates the number of letters in a piece of character text. Applying it to a vector will tell you how many letters are in each of the days of the week: nchar(the_week) ## [1] 6 7 9 8 6 8 6 If you try this with numeric vectors, it gives you a funny output of how many characters are in each number! nchar(v1) ## [1] 1 1 1 1 1 2 Regardless of whether you are using numerical or character vectors, indexing works in the same way. e.g. To get the 3rd element of each vector: v1[3] ## [1] 7 the_week[3] ## [1] &quot;Wednesday&quot; e.g. to get the 5th and 5th elements of each vector: v1[5:6] ## [1] 9 10 the_week[5:6] ## [1] &quot;Friday&quot; &quot;Saturday&quot; e.g. to get the first, fourth, and sixth element of each vector: v1[c(1,4,6)] ## [1] 5 8 10 the_week[c(1,4,6)] ## [1] &quot;Monday&quot; &quot;Thursday&quot; &quot;Saturday&quot; 3.5 Naming of objects One of the hardest things in R is deciding what to name things. You run out of ideas very fast! There's only so many times you can call things x or df or v1 before you get bored. The best names tend to be short and memorable. If you were saving a vector of color names for example, you might want to call it mycolors: mycolors &lt;- c(&quot;mistyrose&quot;, &quot;dodgerblue&quot;, &quot;pink&quot;) There are some important rules for the naming of objects. These are some DOs and some important DON'Ts: you can use characters or numbers in object names, or both do not use spaces try to use lower case ideally do not use any puncutation in names for objects, dataframes or column names except for the period . or underscore _. Everything else is forbidden. if you do use an underscore _ it cannot go at the beginning, e.g. _mynumber &lt;- 17 ## Error: &lt;text&gt;:1:1: unexpected input ## 1: _ ## ^ you can use a period . at the beginning of an object name, but please don't. periods . and underscores _ are best used in the middle of object names to help read the name, e.g. prime_numbers &lt;- c(1,3,5,7,11,13) prime_numbers ## [1] 1 3 5 7 11 13 but even better, use short object names! primes &lt;- c(1,3,5,7,11,13) primes ## [1] 1 3 5 7 11 13 you should avoid using certain words, as they have other meanings in R and calling something else by that name could confuse things. Examples of words to avoid: if, else, repeat, library, break, while, stop, function, for, in, next, TRUE, FALSE, NULL, Inf, NaN, NA, data, etc. That still leaves a lot of words you can use. Usually when you start to write one of these words in R, you'll notice that they change color in the script (to which color depends upon your color settings) - this indicates that these words are 'special'. for the same reason, there are several letters that I would avoid using, because they have special meaning: e.g. c, t, T, F. The last two are short for TRUE and FALSE. We've already seen c() in action. t() means to transpose data. You might find that we've called some objects of numbers t in this course - sorry in advance if we broke this 'rule'. 3.6 Logical Operators The R language contains several bits of punctuation that can be used for what's termed 'logical operators'. These will lead to an output of either TRUE or FALSE being printed to the console. It's easiest to work out what they do by just seeing them in action: &quot;is equals to&quot; ## [1] &quot;is equals to&quot; 2+2 == 4 ## [1] TRUE 2+2 == 5 ## [1] FALSE The double equals sign == checks if the left hand side is equal to the right hands side. In the top example, 2+2 does equal 4 so we get a TRUE, while in the bottom example, 2+5 does not equal 5 so we get a FALSE. You don't have to check one thing at a time. You could check all elements of a vector at once: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x == 10 ## [1] FALSE TRUE FALSE TRUE FALSE FALSE TRUE Here it is saying which elements of x are equal to 10 or not. If you want to ask whether something is not equal to something, then you use != like this: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x != 10 ## [1] TRUE FALSE TRUE FALSE TRUE TRUE FALSE Other logical operators are &lt;, &lt;=, &gt; and &gt;= and can be used in the usual mathematical way: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x &lt; 10 ## [1] TRUE FALSE FALSE FALSE TRUE FALSE FALSE x &lt;= 10 ## [1] TRUE TRUE FALSE TRUE TRUE FALSE TRUE x &gt; 10 ## [1] FALSE FALSE TRUE FALSE FALSE TRUE FALSE x &gt;= 10 ## [1] FALSE TRUE TRUE TRUE FALSE TRUE TRUE You can also add up the logical output. Say you wanted to know how many of your vector of x were equal to 10. You can use sum() to add up. With the TRUE AND FALSE output, sum() will count up the number of TRUEs you have. (It considerds TRUE to be 1's and FALSE to be 0's) x &lt;- c(5, 10, 15, 10, 1, 11, 10) x == 10 ## [1] FALSE TRUE FALSE TRUE FALSE FALSE TRUE sum(x==10) ## [1] 3 Indexing Vectors with Logical Operators This is all a bit abstract and vague as to why we should care about logical operators. One common situation we use them is when we want to subset vectors. Remember, we use the square brackets [] to index a vector like this: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x[5] ## [1] 1 x[1:2] ## [1] 5 10 Above we first got the 5th element of x and then we got the first and second elements of x. We can also grab things from vectors using logic. Effectively keeping those elements that give the output TRUE in response to the logic statement. Here are examples: x &lt;- c(5, 10, 15, 10, 1, 11, 10) x[x==10] ## [1] 10 10 10 x[x!=10] ## [1] 5 15 1 11 x[x&gt;10] ## [1] 15 11 Another funky piece of syntax that comes in useful from time to time is %in%. You can basically use it only keep those elements that match those that exist inside another element. stuff &lt;- c(1, 5, 15) x[x %in% stuff] ## [1] 5 15 1 3.7 Some things that are useful to know. whitespace R doesn't, on the whole, care that much about whitespace - this is space that is between code. It just ignores it. For instance, all of the following give the same result (what's happening here is that a and b are vectors that each have three elements. Telling a+b will make it add the first element of a to the first element of b and then the 2nd element of a to the 2nd element of b and so on: a &lt;- c(1,2,3) b &lt;- c(4,5,6) a + b ## [1] 5 7 9 a&lt;- c( 1, 2, 3) b &lt;- c(4,5,6 ) a + b ## [1] 5 7 9 a &lt;- c(1 ,2, 3) b &lt;- c( 4,5 ,6) a + b ## [1] 5 7 9 Hopefully, you get the point. You can also leave whitespace across lines and R will, for the most part, jump to the code ahead and not worry about the whitespace. This example looks ugly because the commas, numbers, and brackets are all on different lines, but R reads it ok: x &lt;- c( 4, 5, 3, 6 , 14 ) x ## [1] 4 5 3 6 14 Although this is possible. Please, please, don't do it! It's ugly to read, and what's more, there are situations in which it can cause you problems. Indeed, there are some important place where whitespace needs to be adhered to. Firstly, when using the assignment operator &lt;-. R cannot stand it if you put a space in between the &lt; and the -. e.g. eg &lt; - 5:10 ## Error in eval(expr, envir, enclos): object &#39;eg&#39; not found eg ## Error in eval(expr, envir, enclos): object &#39;eg&#39; not found It is stating here that object 'eg' not found because it thinks you're trying to ask it if eg is less than -5:10. If eg already existed, then you need to be extra careful here. We won't go into what could happen in that situation, but it could be bad! So, golden rule, no spaces with &lt;- ! Secondly, in future sections on carpentry and visualization, you'll use some syntax that pipes or chains together a series of commands. These operators are + or %&gt;%. When using this syntax, you need to finish each line (expect the last line) with one of these pipe syntaxes. If you do not do that, you'll get an error message, because R won't jump to the later lines when it is chaining things together. Although this might be a bit early in your R journey to mention - it is such a frequent error, that we just wanted to reference it here. Hopefully, this will be a useful reference aid if you see this error. Here's a simple example of what this looks like. x &lt;- c(3, 5, 6, 7, 4, 6, 5, 10, 4, 5, 7) x %&gt;% unique ## Error: &lt;text&gt;:4:1: unexpected SPECIAL ## 3: x ## 4: %&gt;% ## ^ Here we get an error message saying Error: unexpected SPECIAL in &quot;%&gt;%&quot;. What has happened is that R read the lines where we assigned the numbers to x and then on the line that just had x it printed out the numbers. Then when it got to %&gt;% unique it didn't know what this was referring to. When using chain syntax such as %&gt;% you cannot start a new line with it. Instead you should write: x &lt;- c(3, 5, 6, 7, 4, 6, 5, 10, 4, 5, 7) x %&gt;% unique ## [1] 3 5 6 7 4 10 Now, it has processed the code appropriately and the output is what we want. Basically, we took our vector of x and extracted all the unique numbers. A golden rule should be - be neat with your code !!! and this will avoid a lot of errors and problems. Other common errors Close your brackets Missing commas or brackets might be the most common errors! You will get an error message though, and it usually will tell you what you did wrong. Also, when you start to type ( RStudio will automatically autocomplete to write () which helps you to not forget to close your bracket. a &lt;- c(4, 4, 2, 7 ## Error: &lt;text&gt;:2:0: unexpected end of input ## 1: a &lt;- c(4, 4, 2, 7 ## ^ When you forget to close your brackets, but you run your code anyway, you'll notice that in your console you get a flashing bar and the cursor still has a plus sign like this : This is telling you that it's waiting for something else to finish the code. It expected you to close your brackets! If you do this and you want to reset to get back to the &gt; cursor sign (which means it's ready for action), then just click next to the plus on the flashing | and hit Esc key. This should reset it to the &gt; cursor, and look like this: Don't forget commas Commas need to be watched out for too. They are usually used to split separate items up, such as numbers in a vector or arguments in a function (see section xxx.xxx). Missing them out can cause issues! For instance, here we are trying to make the vector b have the numbers 3, 6, 9 &amp; 10. But the way we write it, R might think we mean 3, 6 &amp; 910 because we forgot the comma. Remember, R often ignores spaces. Fortunately, R catches it because it 'knows' numbers need to be paid attention to - and suggests that you've missed out a comma. Well, technically it tells you Error: unexpected numeric constant in .... which doesn't help much - but it's because we forgot the comma. b &lt;- c(3, 6, 9 10) ## Error: &lt;text&gt;:1:16: unexpected numeric constant ## 1: b &lt;- c(3, 6, 9 10 ## ^ RStudio can help you as you write code. During most of this course, you will be following code already written by us. However, in the try for yourself examples, you'll have to fill in blanks. Whenever there is an issue with code and it won't run, RStudio flags this for you as soon as it realizes. It will give you a big red circle with an X inside of it. These appear in your script. You can see an example below for our problems with missing brackets and commas. Another thing that often happens with commas, is that people forget to put something in between them. Here's this issue: x &lt;- c(5, 8, ) ## Error in c(5, 8, ): argument 3 is empty You get warned that one of your arguments are empty. If you scroll away from your code, you'll get the X in the red circle telling you something is wrong. If your cursor is still by your code, you may get a yellow warning triangle telling you to beware, similar to this: Check your quote marks Quote marks &quot;hello&quot; or 'hello' are often used in R when we want to make sure something is a character (i.e. text). Most commonly used when we're inputting data that include words, or when we're stating a color, or when we're adding a title or a label to a graph. An error that we often see is that quote marks are either missing or not closed. Up to this point in the book, we haven't yet covered the situations in which this is most likely to occur - so don't worry about following this code just yet. We're actually trying to make a scatterplot, that we discuss more in section xxx.xxx. Instead, just focus on the error message: library(tidyverse) ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(color = purple) ## Error in layer(data = data, mapping = mapping, stat = stat, geom = GeomPoint, : object &#39;purple&#39; not found Note that it says object 'purple' not found. That's because, even though we are trying to make the points of the scatterplot purple, because we didn't put it in quotes, it's looking for the object called purple. It thinks there is something called purple that contains the information it needs. The actual code should include purple in quotes, to make sure it processes the word 'purple' literally: library(tidyverse) ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point(color = &quot;purple&quot;) For the most part in R, you can choose whether you use double quotes &quot; or single quotes ' when using quotes. Just make sure you match them, and close them. For example, the following examples are correct: days &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;) days ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; days &lt;- c(&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;) days ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; You can also do this, although it looks a bit odd: days &lt;- c(&#39;Monday&#39;, &quot;Tuesday&quot;, &#39;Wednesday&#39;) days ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; Here are some examples of what happens if you don't close your quotes: days &lt;- c(&quot;Monday, &quot;Tuesday&quot;, &quot;Wednesday&quot;) ## Error: &lt;text&gt;:1:21: unexpected symbol ## 1: days &lt;- c(&quot;Monday, &quot;Tuesday ## ^ Often you'll get the error message Error: unexpected symbol... in such situations. This is usually a clue to look for a punctuation error. Another think you'll notice when you make this error, is that the colors of your code don't look right. Notice that the colors of each day aren't consistent: In general, we'd recommend that when you need to use quotes, that the double quotes &quot;hello&quot; are the better option. There are two reasons for this. First, if you start to type those in R then the second quote mark magically appears. RStudio knows you're using quotes and already produces the closing quote. This doesn't happen (for some reason) when you use the single quote 'hello'. Secondly, often we use quote marks because we're adding a title or a label to something. Some example code to add a title to a graph might be: p + ggtitle(&quot;Tyler&#39;s first graph&quot;) Notice in this title that one of the words requires an apostrophe '. If you had used single quotes to wrap around Tyler's first graph then this would have caused confusion as to where you wanted to end the text string. This only arises when using single quotes - so we say that double quotes are safer! Typos are bad! R can only run the code that you tell it to run..... (well this is not 100% true... but wait until the end of the paragraph for that). If you don't precisely tell it what you want, it will get confused or angry. If you typo, then this can cause problems. Let's do an example. Say we want to make two vectors called apples and oranges and we want to add them togeher. This works: apples &lt;- c(4, 7, 10) oranges &lt;- c(3, 1, 2) apples + oranges ## [1] 7 8 12 but this will not work: apples &lt;- c(4, 7, 10) oranges &lt;- c(3, 1, 2) appless + oranges ## Error in eval(expr, envir, enclos): object &#39;appless&#39; not found You'll get the error saying object 'blah' not found. This is the most common error that we see. Basically, it cannot find an object (be it a vector or a dataframe) if it's not spelled correctly. This also applies to column names of dataframes and so on. R is also case sensisitive. The following doesn't work: apples &lt;- c(4, 7, 10) oranges &lt;- c(3, 1, 2) apples + ORANGES ## Error in eval(expr, envir, enclos): object &#39;ORANGES&#39; not found It will not automatically realize you meant oranges. However, there is a bit of help that RStudio will give you. Look what happens when we write ORANGES in our code script: It automatically popped up oranges as a suggestion. The little colored icon to the left of it indicates that it is in an object. So - RStudio can be your friend! Actually, when you first start to type the first letter of any object, you can hit the Tab key and various options will pop up for you to choose from. You can then either use your up and down arrows or your mouse to click on the thing you want. In the example below, you can see it includes our oranges object and ton of other stuff that we don't need right now that also beings with o. This is super helpful when you've forgotten what you called something: We said that it's not 100% true that R cannot 'guess' what you mean. There are a few exceptions. One example, is that sometimes R can guess the name of a column in a dataset if you only type half of it or the first few letters. However, many programmers think this is a terrible thing to be able to do and so perhaps we should just forget about it. We would highly recommend always writing everything precisely. 3.7.1 Tab is your friend As seen from the example above, the Tab key can be used to get help as you're writing code. This is a super useful little trick as it can be hard to remember the names of some of the commands (functions) in R. Earlier in this section we saw that it was possible to calculate ths sum of a vector with a function called sum(). However, what if you forgot the name of this but you just had a vague recollection that it began with an s ? You could start to type s and then hit the Tab key. Scroll through the list that pops up by your code until you find the one that you think is correct! You'll also notice that you get a big yellow help box that appears to the side. This gives you a bit more information about what the thing you've highlighted does - and this may help jog your memory as to what you were looking for. After you have clicked on the thing you want- it will automatically include it in your script- and you're good to go: 3.8 Error Messages We'll be honest here and say that often times when you get an error message, they aren't massively helpful. Or, at least this was the case until recently. There still are many error messages that don't really help you too much in learning what you did wrong, but if you make errors when using the tidyverse (see section xxx.xxx) such as when making visualization - these error messages are much more helpful now than they once were. Making errors when coding is part of the learning curve though. It's like learning to play the piano. There is almost no way you can do it just by watching somebody else. You have to practice and be ok with making small mistakes. At first they catch you out, but over time you'll realize when you're about to mess up and you'll avoid it. Or, you'll mess up and be faster at working out how to get past it. If people are stuck on particular error messages during this course - please do let us know. We'll try and collate an FAQ of error messages that we can provide answers to - that will hopefully make the learning process a lot less painful. 3.9 Functions Functions are operations that you can conduct in R. There are many built-in functions to R, and several that come in packages that we can install and load in. There is usually (although not always) an object that is the focus of the function. Functions have names and end in brackets. We've already met some of these, e.g.: sum() length() c() sqrt() As we go through the course, we'll see lots more functions and it will become second nature to use them. There is one additional thing that we'll bring up at this point. The things that you put inside functions are called arguments. Sometimes you'll see them called parameters - but that has other meaninings in statistics, so let's use 'arguments'. Some functions have one argument, others have more. Most functions have defaults for these arguments, so if you don't include them, it will still run using it's predefined default. Let's illustrate this by introducing you to the function called round(). This function will round numbers to various decimal places. Let's use the number 17.10771947 as an example. What does round do to this number? round(17.10771947) ## [1] 17 It's default is to round it to 0 decimal places. Inside of round() the number is in the position of the 1st argument. round() can have two arguments - the second one relates to how many decimal places you would like to round to. So, if you want to round to 3 decimal places, you need to do this: round(17.10771047, 3) ## [1] 17.108 Now we have two arguments. In actual fact, those arguments have names, and you can most often work out what the names for the arguments should be from help guides. Technically, what we have just performed is the following: round(x = 17.10771047, digits = 3) ## [1] 17.108 But as you have already worked out, we didn't need to write the names of the arguments. R worked it out already. What if we'd got the arguments, the wrong way round? round(3, 17.10771047) ## [1] 3 Now it defaulted to giving us the first argument to zero decimal places. Because the second argument isn't a whole number (integer) then it just ignored it. What if you keep these in this order, but give them their correct argument names? round(digits = 3, x = 17.10771047) ## [1] 17.108 Now it works as expected again. The moral of this story is that it is best practice to use the argument names! If you hit the Tab key as you type out a function name, then it will give you a list of possible arguments for that function: Arguments of functions aren't always numeric. Sometimes you need to write in words, punctuation or the logical statements TRUE or FALSE to tell the function what you want to do. An example is the function paste which joins together two objects. Here we tell it to paste together three objects with an underscore: obj1 &lt;- &quot;hello&quot; obj2 &lt;- &quot;james&quot; obj3 &lt;- &quot;curley&quot; paste(obj1, obj2, obj3, sep = &quot;_&quot;) ## [1] &quot;hello_james_curley&quot; In functions, punctuation and characters need quote marks around them. TRUE and FALSE should not have punctuation around them. If you want to continue with R then it's also possible to write custom functions that will do what we do whatever we ask them to do - but that is for the future. R works from in to out One thing to remember about R as a coding language is that it will apply functions from in to out. It will start with the innermost function in a statement, and then move outwards. So, if you have the following vector: x &lt;- c(1, 5, 3, 7, 10, 11, 13, 5, 6, 2, 5, 6, 3, 7) x ## [1] 1 5 3 7 10 11 13 5 6 2 5 6 3 7 and you want to get the unique numbers (i.e. get rid of the repeating numbers and only keep one of each of all the numbers), then you could use unique(): unique(x) ## [1] 1 5 3 7 10 11 13 6 2 If you wanted to take the unique numbers, and then square root each of them you could wrap unique(x) in sqrt() like this: sqrt(unique(x)) ## [1] 1.000000 2.236068 1.732051 2.645751 3.162278 3.316625 3.605551 2.449490 ## [9] 1.414214 That code is still just about readable. However, say you want to (for some reason), keep the unique numbers, then take the square root of each number, then add them all up, then round the answer to 2 decimal places. This is what it would like: round(sum(sqrt(unique(x))), digits = 2) ## [1] 21.56 It finds the object you are working with x then the first function outside of that is unique() then the next one outside of unique(x) is sqrt(), then the next one outside of sqrt(unique(x)) is sum() and the next one outside of sum(sqrt(unique(x))) is round(). That's quite hard to follow! Another approach would be to do this in steps and assign objects at each step, like this: x ## [1] 1 5 3 7 10 11 13 5 6 2 5 6 3 7 x1 &lt;- unique(x) x1 ## [1] 1 5 3 7 10 11 13 6 2 x2 &lt;- sqrt(x1) x2 ## [1] 1.000000 2.236068 1.732051 2.645751 3.162278 3.316625 3.605551 2.449490 ## [9] 1.414214 x3 &lt;- sum(x2) x3 ## [1] 21.56203 x4 &lt;- round(x3, digits = 2) x4 ## [1] 21.56 Another alternative is to chain the syntax together (see next section). 3.10 Chaining Syntax The code in the previous section was pretty ugly! Here it is: round(sum(sqrt(unique(x))), digits = 2) ## [1] 21.56 We could rewrite it however by chaining together the outputs of each function using the pipe or chaining piece of syntax that looks like this %&gt;%. This is how it would look if we chained each function together: x %&gt;% unique %&gt;% sqrt %&gt;% sum %&gt;% round(2) ## [1] 21.56 Let's break down what's happening bit by bit. Essentially you can read the %&gt;% as and next do this. This first bit just keeps the numbers that are unique in the vector, removing duplicates: x %&gt;% unique ## [1] 1 5 3 7 10 11 13 6 2 Next we square root each of those unique numbers: x %&gt;% unique %&gt;% sqrt ## [1] 1.000000 2.236068 1.732051 2.645751 3.162278 3.316625 3.605551 2.449490 ## [9] 1.414214 Then the next step is to add up those six square rooted numbers: x %&gt;% unique %&gt;% sqrt %&gt;% sum ## [1] 21.56203 Then we round that value to two decimal places: x %&gt;% unique %&gt;% sqrt %&gt;% sum %&gt;% round(2) ## [1] 21.56 It's also worth pointing out that you don't have to start a new line after every %&gt;%, you can have these in the middle of rows. Just don't start rows with them (see errors above)! This also is fine: x %&gt;% unique %&gt;% sqrt %&gt;% sum %&gt;% round(2) ## [1] 21.56 "],
["introduction-to-data-carpentry.html", "4 Introduction to Data Carpentry 4.1 Introduction to Dataframes 4.2 tidyverse 4.3 filter() 4.4 select() 4.5 mutate() 4.6 arrange() 4.7 Wide vs Long Data 4.8 Joins", " 4 Introduction to Data Carpentry Data carpentry gives us the tools to work with large data sets. During this chapter you will learn basic concepts, skills to help you work with functions to help tidy and manage your data. 4.1 Introduction to Dataframes What is a dataframe and how do we get one from our data. A dataframe, in R is like a table that includes your data, each column contains values or characters for one variable. In R dataframes need to contain the following: Columns should all be the same length, with unique column names. These column names should not start with number and hold no spaces. The data stored in datagframes can hold many different structures such as numeric, factor or character type. Let's divide into all the features of a dataframe. First import data. library(tidyverse) df &lt;- read_csv(&quot;data/cheese.csv&quot;) ## Parsed with column specification: ## cols( ## type = col_character(), ## sat_fat = col_double(), ## polysat_fat = col_double(), ## monosat_fat = col_double(), ## protein = col_double(), ## carb = col_double(), ## chol = col_integer(), ## fiber = col_double(), ## kcal = col_integer() ## ) Explore the Data look at top rows head(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 head(df, 4) ## # A tibble: 4 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 head(df, 8) ## # A tibble: 8 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 ## 7 cheshire 19.5 0.87 8.67 23.4 4.78 103 0 387 ## 8 colby 20.2 0.953 9.28 23.8 2.57 95 0 394 look at bottom rows tail(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 past process,~ 3.30 0.18 1.35 25.5 4.3 35 0 170 ## 2 cottage,lowfa~ 0.619 0.039 0.282 10.9 3 3 0 67 ## 3 past process,~ 19.7 0.988 8.93 22.2 1.6 94 0 375 ## 4 swiss,low sod~ 17.7 0.968 7.26 28.4 3.4 92 0 376 ## 5 swiss,low fat 3.30 0.18 1.35 28.4 3.4 35 0 179 ## 6 mozzarella,lo~ 10.9 0.509 4.84 27.5 3.1 54 0 280 tail(df, 10) ## # A tibble: 10 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 past process~ 4.41 0.222 2.00 24.6 3.5 35 0 180 ## 2 american che~ 8.79 0.409 4.10 16.7 11.6 36 0 239 ## 3 parmesan,lo ~ 19.1 0.659 8.72 41.6 3.7 79 0 456 ## 4 cottage,lowf~ 0.632 0.031 0.284 12.4 2.7 4 0 72 ## 5 past process~ 3.30 0.18 1.35 25.5 4.3 35 0 170 ## 6 cottage,lowf~ 0.619 0.039 0.282 10.9 3 3 0 67 ## 7 past process~ 19.7 0.988 8.93 22.2 1.6 94 0 375 ## 8 swiss,low so~ 17.7 0.968 7.26 28.4 3.4 92 0 376 ## 9 swiss,low fat 3.30 0.18 1.35 28.4 3.4 35 0 179 ## 10 mozzarella,l~ 10.9 0.509 4.84 27.5 3.1 54 0 280 dataframe Dimensions nrow(df) #73 ## [1] 73 ncol(df) #9 ## [1] 9 dim(df) #73 9 ## [1] 73 9 length(df) #9 ## [1] 9 Column names in dataframe head(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 colnames(df) ## [1] &quot;type&quot; &quot;sat_fat&quot; &quot;polysat_fat&quot; &quot;monosat_fat&quot; &quot;protein&quot; ## [6] &quot;carb&quot; &quot;chol&quot; &quot;fiber&quot; &quot;kcal&quot; Changing column names colnames(df)[6] &lt;- &quot;carbo&quot; colnames(df) ## [1] &quot;type&quot; &quot;sat_fat&quot; &quot;polysat_fat&quot; &quot;monosat_fat&quot; &quot;protein&quot; ## [6] &quot;carbo&quot; &quot;chol&quot; &quot;fiber&quot; &quot;kcal&quot; Indexing dataframes. The $ indicates which column to call. df$chol ## [1] 75 94 100 72 93 105 103 95 17 13 7 10 4 110 89 89 116 94 114 ## [20] 110 90 89 79 89 64 54 96 74 88 68 123 69 51 31 104 90 92 102 ## [39] 94 94 85 72 94 85 105 79 46 105 105 105 21 100 12 88 55 62 65 ## [58] 11 4 63 18 14 54 35 36 79 4 35 3 94 92 35 54 df$type ## [1] &quot;blue&quot; ## [2] &quot;brick&quot; ## [3] &quot;brie&quot; ## [4] &quot;camembert&quot; ## [5] &quot;caraway&quot; ## [6] &quot;cheddar&quot; ## [7] &quot;cheshire&quot; ## [8] &quot;colby&quot; ## [9] &quot;cottage,crmd,lrg or sml curd&quot; ## [10] &quot;cottage,crmd,w/fruit&quot; ## [11] &quot;cottage,nonfat,uncrmd,dry,lrg or sml curd&quot; ## [12] &quot;cottage,lowfat,2% milkfat&quot; ## [13] &quot;cottage,lowfat,1% milkfat&quot; ## [14] &quot;cream&quot; ## [15] &quot;edam&quot; ## [16] &quot;feta&quot; ## [17] &quot;fontina&quot; ## [18] &quot;gjetost&quot; ## [19] &quot;gouda&quot; ## [20] &quot;gruyere&quot; ## [21] &quot;limburger&quot; ## [22] &quot;monterey&quot; ## [23] &quot;mozzarella,whl milk&quot; ## [24] &quot;mozzarella,whl milk,lo moist&quot; ## [25] &quot;mozzarella,part skim milk&quot; ## [26] &quot;mozzarella,part skim milk,lo moist&quot; ## [27] &quot;muenster&quot; ## [28] &quot;neufchatel&quot; ## [29] &quot;parmesan,grated&quot; ## [30] &quot;parmesan,hard&quot; ## [31] &quot;port de salut&quot; ## [32] &quot;provolone&quot; ## [33] &quot;ricotta,whole milk&quot; ## [34] &quot;ricotta,part skim milk&quot; ## [35] &quot;romano&quot; ## [36] &quot;roquefort&quot; ## [37] &quot;swiss&quot; ## [38] &quot;tilsit&quot; ## [39] &quot;past process,american,w/di na po4&quot; ## [40] &quot;past process,pimento&quot; ## [41] &quot;past process,swiss,w/di na po4&quot; ## [42] &quot;parmesan,shredded&quot; ## [43] &quot;past process,american,wo/di na po4&quot; ## [44] &quot;past process,swiss,wo/di na po4&quot; ## [45] &quot;goat,hard type&quot; ## [46] &quot;goat,semisoft type&quot; ## [47] &quot;goat,soft type&quot; ## [48] &quot;mexican,queso anejo&quot; ## [49] &quot;mexican,queso asadero&quot; ## [50] &quot;mexican,queso chihuahua&quot; ## [51] &quot;lofat,cheddar or colby&quot; ## [52] &quot;low-sodium,cheddar or colby&quot; ## [53] &quot;cream,fat free&quot; ## [54] &quot;parmesan,dry grated,red fat&quot; ## [55] &quot;provolone,red fat&quot; ## [56] &quot;mexican,blend,red fat&quot; ## [57] &quot;monterey,low fat&quot; ## [58] &quot;past process,cheddar or american,fat-free&quot; ## [59] &quot;cottage,lowfat,1% milkfat,lactose red&quot; ## [60] &quot;muenster,low fat&quot; ## [61] &quot;mozzarella,non-fat&quot; ## [62] &quot;cottage,w/veg&quot; ## [63] &quot;cream,low fat&quot; ## [64] &quot;past process,american,lofat&quot; ## [65] &quot;american cheddar,imitn&quot; ## [66] &quot;parmesan,lo na&quot; ## [67] &quot;cottage,lowfat,1% milkfat,no na&quot; ## [68] &quot;past process,swiss,lofat&quot; ## [69] &quot;cottage,lowfat,1% milkfat,w/veg&quot; ## [70] &quot;past process,cheddar or american,lo na&quot; ## [71] &quot;swiss,low sodium&quot; ## [72] &quot;swiss,low fat&quot; ## [73] &quot;mozzarella,lo na&quot; df$kcal ## [1] 353 371 334 300 376 403 387 394 98 97 72 86 72 342 357 264 389 466 356 ## [20] 413 327 373 300 318 254 302 368 253 431 392 352 351 174 138 387 369 380 340 ## [39] 375 375 334 415 375 334 452 364 268 373 356 374 173 398 105 265 274 282 313 ## [58] 148 74 274 149 95 201 180 239 456 72 170 67 375 376 179 280 What if you just want the first 10 rows of kcal: df$kcal[1:10] ## [1] 353 371 334 300 376 403 387 394 98 97 Call just certain rows and columns. Important: rows before comma, columns after comma. Just get the 7th row: df[7,] ## # A tibble: 1 x 9 ## type sat_fat polysat_fat monosat_fat protein carbo chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 cheshire 19.5 0.87 8.67 23.4 4.78 103 0 387 Get the 10th to 14th rows: df[10:14,] ## # A tibble: 5 x 9 ## type sat_fat polysat_fat monosat_fat protein carbo chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 cottage,crmd,~ 2.31 0.124 1.04 10.7 4.61 13 0.200 97 ## 2 cottage,nonfa~ 0.169 0.003 0.079 10.3 6.66 7 0 72 ## 3 cottage,lowfa~ 0.979 0.07 0.443 11.8 3.66 10 0 86 ## 4 cottage,lowfa~ 0.645 0.031 0.291 12.4 2.72 4 0 72 ## 5 cream 19.3 1.44 8.62 5.93 4.07 110 0 342 Get the first and second columns: df[,1:2] ## # A tibble: 73 x 2 ## type sat_fat ## &lt;chr&gt; &lt;dbl&gt; ## 1 blue 18.7 ## 2 brick 18.8 ## 3 brie 17.4 ## 4 camembert 15.3 ## 5 caraway 18.6 ## 6 cheddar 21.1 ## 7 cheshire 19.5 ## 8 colby 20.2 ## 9 cottage,crmd,lrg or sml curd 1.72 ## 10 cottage,crmd,w/fruit 2.31 ## # ... with 63 more rows Get 3rd, 5th, and 9th columns: df[,c(3,5,9)] ## # A tibble: 73 x 3 ## polysat_fat protein kcal ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.8 21.4 353 ## 2 0.784 23.2 371 ## 3 0.826 20.8 334 ## 4 0.724 19.8 300 ## 5 0.83 25.2 376 ## 6 0.942 24.9 403 ## 7 0.87 23.4 387 ## 8 0.953 23.8 394 ## 9 0.123 11.1 98 ## 10 0.124 10.7 97 ## # ... with 63 more rows Get the 20th to 22nd row, and the 1st, 8th and 9th column: df[20:22,c(1,8,9)] ## # A tibble: 3 x 3 ## type fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 gruyere 0 413 ## 2 limburger 0 327 ## 3 monterey 0 373 Creating new column head(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carbo chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 df$food_type &lt;- &quot;cheese&quot; head(df) ## # A tibble: 6 x 10 ## type sat_fat polysat_fat monosat_fat protein carbo chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 came~ 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 cara~ 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 ched~ 21.1 0.942 9.39 24.9 1.28 105 0 403 ## # ... with 1 more variable: food_type &lt;chr&gt; Deleting a new column df$food_type &lt;- NULL head(df) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carbo chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 4.2 tidyverse The package tidyverse is what we will be using for most of our data carpentry. tidyverse is a larger package which includes several packages useful for dealing with data such as tidyr, dplyr, ggplot2, and more. All of these packages work together to manipulate, reshape, and visual data. Functions and syntax to know before beginning the chapter: filter() - subsetting data select() - selecting columns arrange() - sorting a column mutate() - adding a new column %&gt;% means &quot;and next do this&quot; == means &quot;is equal to&quot; != means &quot;is not equal to&quot; | means &quot;or&quot; Before using tidyverse you will need to install and load the package. You will only need to install a package once, but will have to load a library on every script you plan to use it. install.package(tidyverse) #install From the tidyverse package readr we can read in our data. library(tidyverse) #load bw &lt;- read_csv(&quot;data/bloodwork.csv&quot;) bw is the name of our new dataframe, which will include all data from the bloodwork.csv file. Before we can work with our data we need to what we have to work with using the functions head() and tail() allows you to see the first and last 6 rows, respectfully. head(bw) # first 6 lines ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 ## 3 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 4 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 5 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 6 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; tail(bw) #last 6 lines ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JIB 66 fema~ NY 0 no 62 121 0.097 1.39 ## 2 HBB 67 fema~ NJ 1 yes 74 147 0.288 2.27 ## 3 HDG 68 fema~ NY 3 yes 65 129 0.11 2.65 ## 4 ECD 68 fema~ NJ 2 yes 77 129 0.404 2.02 ## 5 HHJ 69 male CT 2 no 71 121 0.475 0.463 ## 6 CCG 70 male CT 0 yes 80 132 0.078 1.06 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; should we explain the dataset?? idk table() summarizes categorical variables. table(bw\\(sex) table(bw\\)children) table(bw\\(sex, bw\\)children) 4.3 filter() Use %&gt;% and 'filter' to only keep rows where the hrate, heart rate, is over 60. &gt; means greater than. bw %&gt;% filter(hrate &gt; 60) ## # A tibble: 30 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 ## 3 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 4 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 5 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 6 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## 7 IEE 26 fema~ NY 2 no 71 118 0.093 5.41 ## 8 BED 28 fema~ CT 0 no 62 104 0.082 1.18 ## 9 BFB 28 fema~ NJ 0 no 68 118 0.197 0.724 ## 10 IEA 29 fema~ CT 0 yes 74 117 0.429 5.15 ## # ... with 20 more rows, and 1 more variable: immuncount2 &lt;dbl&gt; Use %&gt;% and 'filter' to only keep rows where the sex is equal to female. bw %&gt;% filter(sex == &quot;female&quot;) ## # A tibble: 18 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 3 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 4 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 5 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## 6 IEE 26 fema~ NY 2 no 71 118 0.093 5.41 ## 7 BED 28 fema~ CT 0 no 62 104 0.082 1.18 ## 8 BFB 28 fema~ NJ 0 no 68 118 0.197 0.724 ## 9 IEA 29 fema~ CT 0 yes 74 117 0.429 5.15 ## 10 CDC 38 fema~ NJ 0 no 66 133 0.038 8.00 ## 11 FJG 40 fema~ NJ 0 yes 80 109 0.253 5.63 ## 12 DAG 41 fema~ CT 0 yes 70 142 0.339 5.52 ## 13 FHA 53 fema~ CT 2 no 77 125 0.099 0.034 ## 14 JHC 55 fema~ CT 0 no 73 121 0.093 1.32 ## 15 JIB 66 fema~ NY 0 no 62 121 0.097 1.39 ## 16 HBB 67 fema~ NJ 1 yes 74 147 0.288 2.27 ## 17 HDG 68 fema~ NY 3 yes 65 129 0.11 2.65 ## 18 ECD 68 fema~ NJ 2 yes 77 129 0.404 2.02 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; You can filter to only include, certain rows, such as state as &quot;NJ&quot; or &quot;NY&quot; in the bloodwork dataset. bw %&gt;% filter(state == &quot;NJ&quot; | state == &quot;NY&quot;) ## # A tibble: 18 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 ## 3 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 4 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 5 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## 6 IEE 26 fema~ NY 2 no 71 118 0.093 5.41 ## 7 BFB 28 fema~ NJ 0 no 68 118 0.197 0.724 ## 8 ACC 33 male NJ 1 no 63 131 0.065 4.66 ## 9 CDC 38 fema~ NJ 0 no 66 133 0.038 8.00 ## 10 EEB 39 male NJ 1 no 68 104 0.594 3.06 ## 11 FJG 40 fema~ NJ 0 yes 80 109 0.253 5.63 ## 12 AGC 43 male NY 1 yes 77 108 0.072 1.44 ## 13 FGD 50 male NY 0 yes 77 147 0.428 0.037 ## 14 JCI 52 male NY 1 yes 61 115 0.131 0.233 ## 15 JIB 66 fema~ NY 0 no 62 121 0.097 1.39 ## 16 HBB 67 fema~ NJ 1 yes 74 147 0.288 2.27 ## 17 HDG 68 fema~ NY 3 yes 65 129 0.11 2.65 ## 18 ECD 68 fema~ NJ 2 yes 77 129 0.404 2.02 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; You can filter several variables at one time. bw %&gt;% filter( sex == &quot;female&quot;, hrate &gt; 60, children != &quot;0&quot; ) ## # A tibble: 8 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 3 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## 4 IEE 26 fema~ NY 2 no 71 118 0.093 5.41 ## 5 FHA 53 fema~ CT 2 no 77 125 0.099 0.034 ## 6 HBB 67 fema~ NJ 1 yes 74 147 0.288 2.27 ## 7 HDG 68 fema~ NY 3 yes 65 129 0.11 2.65 ## 8 ECD 68 fema~ NJ 2 yes 77 129 0.404 2.02 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; You can create new datasets from filtered data by creating a new object female &lt;- bw %&gt;% filter( sex == &quot;female&quot;, hrate &gt; 60, children != &quot;0&quot; ) 4.4 select() Use %&gt;% then select to just select the rows you want. bw %&gt;% select(ids, sex, smoker,hrate,children) ## # A tibble: 30 x 5 ## ids sex smoker hrate children ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 JEC female no 63 1 ## 2 GBH male yes 73 1 ## 3 EDH female no 65 0 ## 4 AAA female no 66 3 ## 5 AJF female no 67 0 ## 6 FJC female yes 80 1 ## 7 IEE female no 71 2 ## 8 BED female no 62 0 ## 9 BFB female no 68 0 ## 10 IEA female yes 74 0 ## # ... with 20 more rows Or if you just want to get rid of one column you can just use %&gt;% then select(-var) bw %&gt;% select(-children, -bpsyst) ## # A tibble: 30 x 9 ## ids age sex state smoker hrate cellcount immuncount immuncount2 ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ no 63 0.126 0.993 0.921 ## 2 GBH 20 male NY yes 73 0.169 1.18 1.19 ## 3 EDH 21 female NJ no 65 0.281 4.34 3.21 ## 4 AAA 21 female CT no 66 0.244 2.56 4.01 ## 5 AJF 24 female NJ no 67 0.092 6.45 9.13 ## 6 FJC 25 female NY yes 80 0.014 3.97 2.85 ## 7 IEE 26 female NY no 71 0.093 5.41 10.2 ## 8 BED 28 female CT no 62 0.082 1.18 0.788 ## 9 BFB 28 female NJ no 68 0.197 0.724 0.848 ## 10 IEA 29 female CT yes 74 0.429 5.15 4.97 ## # ... with 20 more rows You can rename columns as you go bw %&gt;% select(subject = ids, gender = sex, smoker,hrate,children) ## # A tibble: 30 x 5 ## subject gender smoker hrate children ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 JEC female no 63 1 ## 2 GBH male yes 73 1 ## 3 EDH female no 65 0 ## 4 AAA female no 66 3 ## 5 AJF female no 67 0 ## 6 FJC female yes 80 1 ## 7 IEE female no 71 2 ## 8 BED female no 62 0 ## 9 BFB female no 68 0 ## 10 IEA female yes 74 0 ## # ... with 20 more rows if you want these selections to be permanent then you need to rewrite selections in new dataframe. See the difference between bw and bw1 bw1 &lt;- bw %&gt;% select(subject = ids, gender = sex, smoker,hrate,children) head(bw) ## # A tibble: 6 x 11 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 ## 3 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 4 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 5 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 6 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## # ... with 1 more variable: immuncount2 &lt;dbl&gt; head(bw1) ## # A tibble: 6 x 5 ## subject gender smoker hrate children ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 JEC female no 63 1 ## 2 GBH male yes 73 1 ## 3 EDH female no 65 0 ## 4 AAA female no 66 3 ## 5 AJF female no 67 0 ## 6 FJC female yes 80 1 If you just want to use the column number, you can do that too! The code below selects for column 1-4, 9, 10, but does not save the information as a new object. bw %&gt;% select(1,2,3,4,9,10) ## # A tibble: 30 x 6 ## ids age sex state cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 0.126 0.993 ## 2 GBH 20 male NY 0.169 1.18 ## 3 EDH 21 female NJ 0.281 4.34 ## 4 AAA 21 female CT 0.244 2.56 ## 5 AJF 24 female NJ 0.092 6.45 ## 6 FJC 25 female NY 0.014 3.97 ## 7 IEE 26 female NY 0.093 5.41 ## 8 BED 28 female CT 0.082 1.18 ## 9 BFB 28 female NJ 0.197 0.724 ## 10 IEA 29 female CT 0.429 5.15 ## # ... with 20 more rows bw %&gt;% select(1:4,9,10) ## # A tibble: 30 x 6 ## ids age sex state cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 female NJ 0.126 0.993 ## 2 GBH 20 male NY 0.169 1.18 ## 3 EDH 21 female NJ 0.281 4.34 ## 4 AAA 21 female CT 0.244 2.56 ## 5 AJF 24 female NJ 0.092 6.45 ## 6 FJC 25 female NY 0.014 3.97 ## 7 IEE 26 female NY 0.093 5.41 ## 8 BED 28 female CT 0.082 1.18 ## 9 BFB 28 female NJ 0.197 0.724 ## 10 IEA 29 female CT 0.429 5.15 ## # ... with 20 more rows 4.5 mutate() Create new columns using %&gt;% then mutate bw %&gt;% mutate(totalimmune = immuncount + immuncount2) ## # A tibble: 30 x 12 ## ids age sex state children smoker hrate bpsyst cellcount immuncount ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 JEC 19 fema~ NJ 1 no 63 101 0.126 0.993 ## 2 GBH 20 male NY 1 yes 73 120 0.169 1.18 ## 3 EDH 21 fema~ NJ 0 no 65 100 0.281 4.34 ## 4 AAA 21 fema~ CT 3 no 66 109 0.244 2.56 ## 5 AJF 24 fema~ NJ 0 no 67 108 0.092 6.45 ## 6 FJC 25 fema~ NY 1 yes 80 118 0.014 3.97 ## 7 IEE 26 fema~ NY 2 no 71 118 0.093 5.41 ## 8 BED 28 fema~ CT 0 no 62 104 0.082 1.18 ## 9 BFB 28 fema~ NJ 0 no 68 118 0.197 0.724 ## 10 IEA 29 fema~ CT 0 yes 74 117 0.429 5.15 ## # ... with 20 more rows, and 2 more variables: immuncount2 &lt;dbl&gt;, ## # totalimmune &lt;dbl&gt; Saving new column in a new data frame. Now, bw2 will have 12 columns, instead of 11. bw2 &lt;- bw %&gt;% mutate(totalimmune = immuncount + immuncount2) You can also create new columns without tidyverse bw$totalimmune &lt;- bw$immuncount + bw$immuncount2 Now bw and bw2 are the same. 4.6 arrange() Lets try another example! pga &lt;- read_csv(&quot;data/pga.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_integer(), ## drivepct = col_double(), ## name = col_character(), ## driveavg = col_double(), ## girpct = col_double(), ## goforgreenpct = col_double(), ## sandsavepct = col_double(), ## scramblingpct = col_double(), ## threeputtpct = col_double(), ## oneputtpct = col_double(), ## putts.avg = col_double(), ## birdiepct = col_double(), ## score.avg = col_double(), ## birdiebogieratio = col_double(), ## par3.birdiepct = col_double(), ## par4.birdiepct = col_double(), ## par5.birdiepct = col_double(), ## moneypct = col_double(), ## country = col_character(), ## label = col_character() ## ) ## See spec(...) for full column specifications. head(pga) ## # A tibble: 6 x 53 ## drivepct fairways.hit possible.fairwa~ year name driveavg total.distance ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 77.2 1031 1335 2004 Fred~ 272. 52198 ## 2 77.1 978 1268 2004 Scot~ 278 50314 ## 3 76.2 881 1157 2004 Crai~ 271. 44959 ## 4 75.1 886 1180 2004 Joe ~ 287. 48829 ## 5 74.7 861 1153 2004 Tom ~ 273. 45246 ## 6 74.3 664 894 2004 Jose~ 279. 35479 ## # ... with 46 more variables: total.drives &lt;int&gt;, rounds &lt;int&gt;, girpct &lt;dbl&gt;, ## # greens.hit &lt;int&gt;, total.holes &lt;int&gt;, goforgreenpct &lt;dbl&gt;, ## # goforgreen.attempts &lt;int&gt;, goforgreen.noattempts &lt;int&gt;, sandsavepct &lt;dbl&gt;, ## # total.sandsaves &lt;int&gt;, total.bunkers &lt;int&gt;, scramblingpct &lt;dbl&gt;, ## # parorbetter &lt;int&gt;, missedgir &lt;int&gt;, threeputtpct &lt;dbl&gt;, ## # threeputt.total &lt;int&gt;, oneputtpct &lt;dbl&gt;, oneputt.total &lt;int&gt;, ## # putts.avg &lt;dbl&gt;, total.putts &lt;int&gt;, rounds1 &lt;int&gt;, birdiepct &lt;dbl&gt;, ## # total.birdies &lt;int&gt;, score.avg &lt;dbl&gt;, strokes &lt;int&gt;, ## # birdiebogieratio &lt;dbl&gt;, total.birdies.better &lt;int&gt;, ## # total.bogeys.worse &lt;int&gt;, par3.birdiepct &lt;dbl&gt;, par3.birdies.better &lt;int&gt;, ## # par3.holes &lt;int&gt;, par4.birdiepct &lt;dbl&gt;, par4.birdies.better &lt;int&gt;, ## # par4.holes &lt;int&gt;, par5.birdiepct &lt;dbl&gt;, par5.birdies.better &lt;int&gt;, ## # par5.holes &lt;int&gt;, total.events &lt;int&gt;, top10s &lt;int&gt;, firsts &lt;int&gt;, ## # seconds &lt;int&gt;, thirds &lt;int&gt;, moneypct &lt;dbl&gt;, totalmoney &lt;int&gt;, ## # country &lt;chr&gt;, label &lt;chr&gt; colnames(pga) ## [1] &quot;drivepct&quot; &quot;fairways.hit&quot; &quot;possible.fairways&quot; ## [4] &quot;year&quot; &quot;name&quot; &quot;driveavg&quot; ## [7] &quot;total.distance&quot; &quot;total.drives&quot; &quot;rounds&quot; ## [10] &quot;girpct&quot; &quot;greens.hit&quot; &quot;total.holes&quot; ## [13] &quot;goforgreenpct&quot; &quot;goforgreen.attempts&quot; &quot;goforgreen.noattempts&quot; ## [16] &quot;sandsavepct&quot; &quot;total.sandsaves&quot; &quot;total.bunkers&quot; ## [19] &quot;scramblingpct&quot; &quot;parorbetter&quot; &quot;missedgir&quot; ## [22] &quot;threeputtpct&quot; &quot;threeputt.total&quot; &quot;oneputtpct&quot; ## [25] &quot;oneputt.total&quot; &quot;putts.avg&quot; &quot;total.putts&quot; ## [28] &quot;rounds1&quot; &quot;birdiepct&quot; &quot;total.birdies&quot; ## [31] &quot;score.avg&quot; &quot;strokes&quot; &quot;birdiebogieratio&quot; ## [34] &quot;total.birdies.better&quot; &quot;total.bogeys.worse&quot; &quot;par3.birdiepct&quot; ## [37] &quot;par3.birdies.better&quot; &quot;par3.holes&quot; &quot;par4.birdiepct&quot; ## [40] &quot;par4.birdies.better&quot; &quot;par4.holes&quot; &quot;par5.birdiepct&quot; ## [43] &quot;par5.birdies.better&quot; &quot;par5.holes&quot; &quot;total.events&quot; ## [46] &quot;top10s&quot; &quot;firsts&quot; &quot;seconds&quot; ## [49] &quot;thirds&quot; &quot;moneypct&quot; &quot;totalmoney&quot; ## [52] &quot;country&quot; &quot;label&quot; table(pga$year) ## ## 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 ## 195 201 195 195 195 183 191 185 190 179 176 184 let's pick out name, year, total.holes, total.putts, score.avg columns and save as pga1. pga %&gt;% select(name, year, total.holes, total.putts, score.avg) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Fred Funk 2004 1728 2774 70.4 ## 2 Scott Verplank 2004 1638 2619 69.9 ## 3 Craig Bowden 2004 1494 2401 71.5 ## 4 Joe Durant 2004 1530 2563 70.7 ## 5 Tom Byrum 2004 1494 2380 70.5 ## 6 Jose Coceres 2004 1152 1819 71.0 ## 7 Bart Bryant 2004 1242 2041 70.5 ## 8 Olin Browne 2004 1584 2580 71.3 ## 9 John Cook 2004 954 1550 71.7 ## 10 Omar Uresti 2004 1530 2470 71.7 ## # ... with 2,259 more rows pga1 &lt;- pga %&gt;% select(name, year, total.holes, total.putts, score.avg) Here we can sort a column using arrange(). There are many ways you can arrange your data. sorts data in ascending order pga1 %&gt;% arrange(score.avg) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Tiger Woods 2007 1080 1736 67.8 ## 2 Tiger Woods 2009 1116 1763 68.1 ## 3 Tiger Woods 2006 936 1528 68.1 ## 4 Tiger Woods 2005 1332 2124 68.7 ## 5 Rory McIlroy 2014 1152 1830 68.8 ## 6 Vijay Singh 2004 1980 3216 68.8 ## 7 Luke Donald 2011 1206 1878 68.9 ## 8 Jim Furyk 2006 1584 2539 68.9 ## 9 Rory McIlroy 2012 972 1551 68.9 ## 10 Steve Stricker 2013 846 1348 68.9 ## # ... with 2,259 more rows sorts data in descending order pga1 %&gt;% arrange(-score.avg) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 David Gossett 2004 1026 1693 75.0 ## 2 Kevin Muncrief 2004 900 1490 73.5 ## 3 Chris Couch 2004 990 1635 73.5 ## 4 Hidemichi Tanaka 2006 1314 2136 73.5 ## 5 David Duval 2008 918 1497 73.2 ## 6 Brad Faxon 2010 972 1567 73.1 ## 7 Eric Axley 2009 1206 1916 73.1 ## 8 Hirofumi Miyase 2004 1134 1857 73.1 ## 9 Greg Kraft 2010 828 1370 73.1 ## 10 Stephen Gangluff 2012 918 1507 73.0 ## # ... with 2,259 more rows sorts data in ascending alphabetical order pga1 %&gt;% arrange(name) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Aaron Baddeley 2004 1530 2411 71.6 ## 2 Aaron Baddeley 2005 1476 2303 71.3 ## 3 Aaron Baddeley 2006 1368 2125 71.2 ## 4 Aaron Baddeley 2007 1440 2261 70.1 ## 5 Aaron Baddeley 2008 1314 2072 70.2 ## 6 Aaron Baddeley 2009 1170 1826 71.2 ## 7 Aaron Baddeley 2010 1692 2686 71.0 ## 8 Aaron Baddeley 2011 1350 2129 70.2 ## 9 Aaron Baddeley 2012 1296 2021 71.1 ## 10 Aaron Baddeley 2013 1188 1835 71.5 ## # ... with 2,259 more rows sorts data first by year, then by score pga1 %&gt;% arrange(year, score.avg) ## # A tibble: 2,269 x 5 ## name year total.holes total.putts score.avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Vijay Singh 2004 1980 3216 68.8 ## 2 Ernie Els 2004 1044 1653 69.0 ## 3 Tiger Woods 2004 1296 2048 69.0 ## 4 Phil Mickelson 2004 1422 2287 69.2 ## 5 Retief Goosen 2004 990 1580 69.3 ## 6 Sergio Garcia 2004 1152 1919 69.8 ## 7 Stewart Cink 2004 1746 2731 69.8 ## 8 Stephen Ames 2004 1710 2756 69.9 ## 9 Scott Verplank 2004 1638 2619 69.9 ## 10 David Toms 2004 1332 2174 70.0 ## # ... with 2,259 more rows You can use the pipe, %&gt;%, in tidyverse to run multiple commands at once. pga %&gt;% select(name, year, total.holes, total.putts, score.avg) %&gt;% mutate(putt.avg = total.putts / total.holes) %&gt;% arrange(putt.avg) ## # A tibble: 2,269 x 6 ## name year total.holes total.putts score.avg putt.avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brian Gay 2013 1422 2173 71.2 1.53 ## 2 Kevin Na 2011 1512 2331 70.4 1.54 ## 3 Greg Chalmers 2011 1548 2388 70.6 1.54 ## 4 Justin Leonard 2014 1350 2083 70.9 1.54 ## 5 Steve Stricker 2005 1116 1723 71.1 1.54 ## 6 Aaron Baddeley 2013 1188 1835 71.5 1.54 ## 7 Jordan Spieth 2015 1584 2448 68.9 1.55 ## 8 Jordan Spieth 2014 1764 2730 69.9 1.55 ## 9 Tim Clark 2007 1152 1784 69.9 1.55 ## 10 Jonas Blixt 2012 1260 1952 70.2 1.55 ## # ... with 2,259 more rows 4.7 Wide vs Long Data Sometimes you will need to rearrange your data for some data analysis and more commonly for data visualization. tidyverse has built in functions that can turn your dataframe from long format to wide format and vice versa. pivot_longer makes the dataframes longer by increasing the number of rows by combing the number of columns. This is called long form data, which is needed to tidy data for graphing and some analysis. insert 3 column version. wheel &lt;- read_csv(&quot;data/wheels1.csv&quot;) ## Parsed with column specification: ## cols( ## strain = col_character(), ## id = col_character(), ## day1 = col_double(), ## day2 = col_double(), ## day3 = col_double(), ## day4 = col_double(), ## dob = col_character(), ## mother = col_character(), ## sex = col_character(), ## wheel = col_integer(), ## startdate = col_character(), ## total = col_double(), ## age = col_integer(), ## group = col_character() ## ) head(wheel) ## # A tibble: 6 x 14 ## strain id day1 day2 day3 day4 dob mother sex wheel startdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 B6 692ao 12853 8156. 9028. 12516. 4/18~ 633ax male 9 10/9/2005 ## 2 B6 656aa 2644 4012. 5237 7404. 12/1~ 593ar male 1 9/12/2005 ## 3 B6 675ag 4004. 3054. 3816. 3761 2/9/~ 593ad male 5 10/13/20~ ## 4 B6 675ai 11754. 8863 11784 11684 2/9/~ 593ad male 4 10/13/20~ ## 5 B6 656af 6906. 5322. 10424. 8468. 12/1~ 593ar male 2 10/13/20~ ## 6 B6 656al 6517 4440 5334. 9291 12/1~ 554aa male 8 10/9/2005 ## # ... with 3 more variables: total &lt;dbl&gt;, age &lt;int&gt;, group &lt;chr&gt; Here we can make the wheel data into long form, by combining day1, day2, day3, and day4 into 2 columns one called day and another called distance. wheel_long &lt;-wheel %&gt;% pivot_longer( cols = starts_with(&quot;day&quot;), names_to = &quot;day&quot;, values_to = &quot;distance&quot; ) head(wheel_long) ## # A tibble: 6 x 12 ## strain id dob mother sex wheel startdate total age group day ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B6 692ao 4/18~ 633ax male 9 10/9/2005 42552. 174 inbr~ day1 ## 2 B6 692ao 4/18~ 633ax male 9 10/9/2005 42552. 174 inbr~ day2 ## 3 B6 692ao 4/18~ 633ax male 9 10/9/2005 42552. 174 inbr~ day3 ## 4 B6 692ao 4/18~ 633ax male 9 10/9/2005 42552. 174 inbr~ day4 ## 5 B6 656aa 12/1~ 593ar male 1 9/12/2005 19297 270 inbr~ day1 ## 6 B6 656aa 12/1~ 593ar male 1 9/12/2005 19297 270 inbr~ day2 ## # ... with 1 more variable: distance &lt;dbl&gt; Other ways to do this. pivot_wider() makes the dataframes longer by decreasing the number of rows by separating the number of columns. This is called wide form data, which id usefull for summarying data in tables and using some functions in R. Now let's make the dataframe wide again. wheel_long %&gt;% pivot_wider( names_from = day, values_from = distance ) ## # A tibble: 80 x 14 ## strain id dob mother sex wheel startdate total age group day1 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 B6 692ao 4/18~ 633ax male 9 10/9/2005 42552. 174 inbr~ 12853 ## 2 B6 656aa 12/1~ 593ar male 1 9/12/2005 19297 270 inbr~ 2644 ## 3 B6 675ag 2/9/~ 593ad male 5 10/13/20~ 14634. 246 inbr~ 4004. ## 4 B6 675ai 2/9/~ 593ad male 4 10/13/20~ 44086. 246 inbr~ 11754. ## 5 B6 656af 12/1~ 593ar male 2 10/13/20~ 31122 301 inbr~ 6906. ## 6 B6 656al 12/1~ 554aa male 8 10/9/2005 25582. 294 inbr~ 6517 ## 7 B6 692an 4/18~ 633ax male 1 10/25/20~ 45352 190 inbr~ 11366. ## 8 B6 705ab 7/25~ 675aj male 7 10/25/20~ 51766 92 inbr~ 13794. ## 9 B6 692at 4/20~ 633ba male 1 10/13/20~ 54777 176 inbr~ 10486. ## 10 B6 692au 4/20~ 633ak male 2 10/25/20~ 31062. 188 inbr~ 4592. ## # ... with 70 more rows, and 3 more variables: day2 &lt;dbl&gt;, day3 &lt;dbl&gt;, ## # day4 &lt;dbl&gt; 4.8 Joins When working with sometimes you need to join two different data sets, based on one or more variables. Lets make up some data x &lt;- data.frame(&quot;id&quot; = 1:10, &quot;age&quot; = c(21,25,17,34,25,33,22,27,29,24)) head(x) ## id age ## 1 1 21 ## 2 2 25 ## 3 3 17 ## 4 4 34 ## 5 5 25 ## 6 6 33 y &lt;- data.frame(&quot;id&quot; = 1:10, &quot;height_cm&quot; = c(156, 155, 154, 149, 153, 152, 151, 150, 147, 155), &quot;activity_hr&quot;= c(3,5,3,6,7,4,2,8,4,5), &quot;work_hr&quot; =c(40,35,38,46,50,42,40,46,41,40)) head(y) ## id height_cm activity_hr work_hr ## 1 1 156 3 40 ## 2 2 155 5 35 ## 3 3 154 3 38 ## 4 4 149 6 46 ## 5 5 153 7 50 ## 6 6 152 4 42 full_join() joins 2 different dataframes based on one or more shared variable(s). The following will join based on id. x %&gt;% full_join(y) ## Joining, by = &quot;id&quot; ## id age height_cm activity_hr work_hr ## 1 1 21 156 3 40 ## 2 2 25 155 5 35 ## 3 3 17 154 3 38 ## 4 4 34 149 6 46 ## 5 5 25 153 7 50 ## 6 6 33 152 4 42 ## 7 7 22 151 2 40 ## 8 8 27 150 8 46 ## 9 9 29 147 4 41 ## 10 10 24 155 5 40 Save as a new data frame to use again: new &lt;- x %&gt;% full_join(y) ## Joining, by = &quot;id&quot; head(new) ## id age height_cm activity_hr work_hr ## 1 1 21 156 3 40 ## 2 2 25 155 5 35 ## 3 3 17 154 3 38 ## 4 4 34 149 6 46 ## 5 5 25 153 7 50 ## 6 6 33 152 4 42 left_join right_join "],
["data-visualization.html", "5 Data Visualization 5.1 Intro to ggplot2 5.2 Histogram 5.3 Scatter 5.4 Line 5.5 Boxplot 5.6 Bar Graphs 5.7 All the extras", " 5 Data Visualization The package ggplot2 is used to visualize data. Before starting you should load the tidyverse package, which includes ggplot2 and other tools, at the very top of your script. library(tidyverse) 5.1 Intro to ggplot2 Read in data df &lt;- read_csv(&quot;data/BlueJays.csv&quot;) ## Parsed with column specification: ## cols( ## BirdID = col_character(), ## KnownSex = col_character(), ## BillDepth = col_double(), ## BillWidth = col_double(), ## BillLength = col_double(), ## Head = col_double(), ## Mass = col_double(), ## Skull = col_double(), ## Sex = col_integer() ## ) Start with a black canvas ggplot() Just adding the datset to the blank canvas, but nothing plotted yet. ggplot(df) You need to use aes() for every ggplot you make, and inside aes() we'll put what our x and y axis will be. We say what the x and y are, and it creates scales on each axis, but we didn't tell it what to plot yet. ggplot(df, aes(x=Mass, y=Head) ) Need to add points. ggplot(df, aes(x=Mass, y=Head) ) + geom_point() What if we want to change the color of the points? ggplot(df, aes(x=Mass, y=Head) ) + geom_point(color=&#39;red&#39;) What if we want to color the points based on another variable ggplot(df, aes(x=Mass, y=Head, color = KnownSex) ) + geom_point() Basically, you can just keep adding to the initial plot to make your graph the best visualization to fit your data. There are many different graphs you can make with ggplot. What graph you make will depend on what type of data you have and what you want to visual from your data. The next couple sections will take the base of ggplot and show you examples with different types of points. They will start general then show you how to customize plots. General information about customizing plots in ggplot: 1. ALLL THE COLORS IN THE WORLD https://www.datanovia.com/en/blog/awesome-list-of-657-r-color-names/ http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf change the type of points you plot with geom_point(pch = blah): Different Types of Points Also, you can change the color and size of points by `geom_point(size = 3, color = &quot;blue&quot;). Built in themes: theme_bw() a variation on theme_grey() that uses a white background and thin grey grid lines. theme_linedraw() A theme with only black lines of various widths on white backgrounds, reminiscent of a line drawing. theme_light() similar to theme_linedraw() but with light grey lines and axes, to direct more attention towards the data. theme_dark() the dark cousin of theme_light(), with similar line sizes but a dark background. Useful to make thin colored lines pop out. theme_minimal() A minimalistic theme with no background annotations. theme_classic() A classic-looking theme, with x and y axis lines and no gridlines. theme_void() A completely empty theme For more information about themes look here. 5.2 Histogram Why make a histogram? Histograms are a good place to start with your data, it will give you a visual representation of your data distribution. blah probably needs more, idk what to say. theory of histograms..... .... how to make a histogram by hand.... 5.2.0.1 Stem-and-Leaf Plots related to histograms. Load libraries and data library(tidyverse) df &lt;- read_csv(&quot;data/films.csv&quot;) ## Parsed with column specification: ## cols( ## film = col_character(), ## year = col_integer(), ## rottentomatoes = col_integer(), ## imdb = col_double(), ## metacritic = col_integer() ## ) head(df) ## # A tibble: 6 x 5 ## film year rottentomatoes imdb metacritic ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Avengers: Age of Ultron 2015 74 7.8 66 ## 2 Cinderella 2015 85 7.1 67 ## 3 Ant-Man 2015 80 7.8 64 ## 4 Do You Believe? 2015 18 5.4 22 ## 5 Hot Tub Time Machine 2 2015 14 5.1 29 ## 6 The Water Diviner 2015 63 7.2 50 Making a ggplot2 histogram, which will plot imdb scores. ggplot(df, aes(x=imdb)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Now lets make the bars doderblue and border them white. ggplot(df, aes(x=imdb)) + geom_histogram(color=&#39;white&#39;, fill=&quot;dodgerblue&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. You can also control the bin width of histograms, this code has a bin width of 0.2 rating points ggplot(df, aes(x = imdb)) + geom_histogram(binwidth = 0.2, color=&quot;white&quot;, fill=&quot;dodgerblue&quot;) Further you can control where the boundaries of each bin lie on the x-axis ggplot(df, aes(x = imdb)) + geom_histogram(binwidth = 0.2, color=&quot;white&quot;, fill=&quot;dodgerblue&quot;,boundary=4) Just be careful with using the boundaries that it does not crop your histogram incorrectly. Changing histograms too much can lead to miss representing the data. (idk if that is said correctly) Here is the full code with some additional customizations ggplot(df, aes(x = imdb)) + geom_histogram(binwidth = 0.2, color=&quot;white&quot;, fill=&quot;dodgerblue&quot;,boundary=4) + theme_classic() + ggtitle(&quot;Histogram of IMDB Ratings&quot;) + xlab(&quot;Rating&quot;) + ylab(&quot;Frequency&quot;) Instead of Viewing as a histogram, you can also show the data as a density curve ggplot(df, aes(x = imdb)) + geom_density(color = &quot;navy&quot;, fill = &quot;dodgerblue&quot;) Adding, alpha gives it a bit of transparency ggplot(df, aes(x = imdb)) + geom_density(color = &quot;navy&quot;, fill = &quot;dodgerblue&quot;, alpha=.4) Comparing Distributions with side by side Histograms Switching to new data life &lt;- read_csv(&quot;data/lifeexp.csv&quot;) ## Parsed with column specification: ## cols( ## country = col_character(), ## continent = col_character(), ## year = col_character(), ## lifeExp = col_double(), ## pop = col_double(), ## gdpPercap = col_double() ## ) head(life) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia year_1952 28.8 8425333 779. ## 2 Afghanistan Asia year_2007 43.8 31889923 975. ## 3 Albania Europe year_1952 55.2 1282697 1601. ## 4 Albania Europe year_2007 76.4 3600523 5937. ## 5 Algeria Africa year_1952 43.1 9279525 2449. ## 6 Algeria Africa year_2007 72.3 33333216 6223. tail(life) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Yemen, Rep. Asia year_1952 32.5 4963829 782. ## 2 Yemen, Rep. Asia year_2007 62.7 22211743 2281. ## 3 Zambia Africa year_1952 42.0 2672000 1147. ## 4 Zambia Africa year_2007 42.4 11746035 1271. ## 5 Zimbabwe Africa year_1952 48.5 3080907 407. ## 6 Zimbabwe Africa year_2007 43.5 12311143 470. First lets plot a histogram of life Expectancy across all countries ggplot(life, aes(x=lifeExp)) + geom_histogram(color=&#39;white&#39;, fill=&#39;lightseagreen&#39;) #warning is ok ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Since we've combined data from the two years - we should separate histograms for each year. We can do this by Overlaiding histograms. ggplot(life, aes(x=lifeExp, fill=year)) + geom_histogram(position=&quot;identity&quot;) #horrible ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. That graph was horrible, but if you costomize the plot by defining binwith and adding an alpha level it will get better. ggplot(life, aes(x=lifeExp, fill=year)) + geom_histogram(position=&quot;identity&quot;, alpha=.5, binwidth=2) Let add color now to make it even better, this makes the borders and fill of the bars black ggplot(life, aes(x=lifeExp, fill=year)) + geom_histogram(position=&quot;identity&quot;, alpha=.5, binwidth=2, color=&#39;black&#39;) + scale_fill_manual(values = c(&quot;#999999&quot;, &quot;#E69F00&quot;)) Comparing Distributions can also be done with geom_density(), this is usually simpler to compare. Initial plot ggplot(life, aes(x=lifeExp, fill=year)) + geom_density(aes(fill = year), alpha = 0.4) Adding customized colors ggplot(life, aes(x=lifeExp, fill=year)) + geom_density(aes(fill = year), alpha = 0.4) + scale_fill_manual(values = c(&quot;#999999&quot;, &quot;#E69F00&quot;)) And adding a theme ggplot(life, aes(x=lifeExp, fill=year)) + geom_density(aes(fill = year), alpha = 0.4) + scale_fill_manual(values = c(&quot;#999999&quot;, &quot;#E69F00&quot;)) + theme_classic() 5.3 Scatter Scatter plots are used when you are trying to illustrate a trend in your data. These plots can be important in statistics because it can help show the correlation between two variables. geom_point() is used for scatter plots. Lets look at the dataframe cheese: cheese &lt;- read_csv(&quot;data/cheese.csv&quot;) ## Parsed with column specification: ## cols( ## type = col_character(), ## sat_fat = col_double(), ## polysat_fat = col_double(), ## monosat_fat = col_double(), ## protein = col_double(), ## carb = col_double(), ## chol = col_integer(), ## fiber = col_double(), ## kcal = col_integer() ## ) head(cheese) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 Start with a simple plot looking at the outcome of cholesterol on saturated fat intake. ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point() Adding color ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) Adding trend line with geom_smooth ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + geom_smooth(method = &quot;lm&quot;) Here you can see it automatically puts a shaded area around your trend line, this is supposed to represent the confident interval. There is a way to get rid of it - ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) You can also change the color of the trend line, by adding to geom_smooth() ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + geom_smooth(method = &quot;lm&quot;, se= F, color = &quot;black&quot;) Label your x and y axis ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + geom_smooth(method = &quot;lm&quot;, se= F, color = &quot;black&quot;) + xlab(&quot; Saturated Fat Intake&quot;) + ylab(&quot;Cholesterol&quot;) Add a title and a theme to make your graph complete ggplot(cheese, aes(x=sat_fat, y=chol) ) + geom_point(color = &quot;purple&quot;) + geom_smooth(method = &quot;lm&quot;, se= F, color = &quot;black&quot;) + xlab(&quot; Saturated Fat Intake&quot;) + ylab(&quot;Cholesterol&quot;) + ggtitle(&quot;Saturated Fat Intake and Cholesterol&quot;) + theme_minimal() 5.4 Line Line graph help connect values (y-axis) over time (x-axis) jennifer &lt;- read_csv(&quot;data/jennifer.csv&quot;) ## Parsed with column specification: ## cols( ## year = col_integer(), ## sex = col_character(), ## name = col_character(), ## n = col_integer(), ## prop = col_double() ## ) head(jennifer) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1916 Female Jennifer 5 0.00000461 ## 2 1919 Female Jennifer 6 0.00000511 ## 3 1920 Female Jennifer 7 0.00000563 ## 4 1921 Female Jennifer 5 0.00000391 ## 5 1922 Female Jennifer 7 0.00000561 ## 6 1923 Female Jennifer 9 0.00000719 tail(jennifer) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2012 Female Jennifer 1923 0.000993 ## 2 2013 Female Jennifer 1689 0.000878 ## 3 2014 Female Jennifer 1521 0.000779 ## 4 2015 Female Jennifer 1283 0.000660 ## 5 2016 Female Jennifer 1159 0.000601 ## 6 2017 Female Jennifer 1042 0.000556 How to build up a line graph, start just like the other graphs ggplot(jennifer, aes(x=year, y=n) ) Look at data as points first ggplot(jennifer, aes(x=year, y=n) ) + geom_point() # look at data as points Then as a line ggplot(jennifer, aes(x=year, y=n) ) + geom_line() # instead use a line You can add points and line together ggplot(jennifer, aes(x=year, y=n) ) + geom_point() + geom_line() Changing Color of Line, but not the points ggplot(jennifer, aes(x=year, y=n) ) + geom_point() + geom_line(color = &quot;purple&quot;) This will change the color of both the points and the line ggplot(jennifer, aes(x=year, y=n) ) + geom_point(color = &quot;violet&quot;) + geom_line(color = &quot;purple&quot;) Customize axis labels and title ggplot(jennifer, aes(x=year, y=n) ) + geom_line(color = &quot;purple&quot;) + xlab(&quot;Year&quot;) + ylab(&quot;Number of Children Born&quot;) + ggtitle(&quot;Popularity of Name Jennifer in USA&quot;) Can change width of lines ggplot(jennifer, aes(x=year, y=n) ) + geom_line(color = &#39;purple&#39;, lwd=2) There are many different style of lines here are a few examples ggplot(jennifer, aes(x=year, y=n) ) + geom_line(lty=1) ggplot(jennifer, aes(x=year, y=n) ) + geom_line(lty=2) ggplot(jennifer, aes(x=year, y=n) ) + geom_line(lty=3) ggplot(jennifer, aes(x=year, y=n) ) + geom_line(lty=4) Plotting multiple lines on same graph read in data jenlinda &lt;- read_csv(&quot;data/jenlinda.csv&quot;) ## Parsed with column specification: ## cols( ## year = col_integer(), ## sex = col_character(), ## name = col_character(), ## n = col_integer(), ## prop = col_double() ## ) head(jenlinda) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 Female Linda 27 0.000277 ## 2 1881 Female Linda 38 0.000384 ## 3 1882 Female Linda 36 0.000311 ## 4 1883 Female Linda 49 0.000408 ## 5 1884 Female Linda 33 0.000240 ## 6 1885 Female Linda 60 0.000423 tail(jenlinda) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2015 Female Jennifer 1283 0.000660 ## 2 2015 Female Linda 425 0.000218 ## 3 2016 Female Jennifer 1159 0.000601 ## 4 2016 Female Linda 436 0.000226 ## 5 2017 Female Jennifer 1042 0.000556 ## 6 2017 Female Linda 404 0.000215 Just one line ggplot(jenlinda, aes(x=year, y=n, color=name)) + geom_line() Two lines - one for each name. ggplot(jenlinda, aes(x=year, y=n, color=name)) + geom_line()+ xlab(&quot;Year&quot;) + ylab(&quot;Number of Children Born&quot;) + ggtitle(&quot;Popularity of Names Jennifer &amp; Linda in USA&quot;) 5.5 Boxplot Boxplot can also be used when looking at the distribution of data. These plots can show the median value and quartiles ranges of the data. Import and look at data wheels &lt;- read_csv(&quot;data/wheels1.csv&quot;) ## Parsed with column specification: ## cols( ## strain = col_character(), ## id = col_character(), ## day1 = col_double(), ## day2 = col_double(), ## day3 = col_double(), ## day4 = col_double(), ## dob = col_character(), ## mother = col_character(), ## sex = col_character(), ## wheel = col_integer(), ## startdate = col_character(), ## total = col_double(), ## age = col_integer(), ## group = col_character() ## ) head(wheels) ## # A tibble: 6 x 14 ## strain id day1 day2 day3 day4 dob mother sex wheel startdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 B6 692ao 12853 8156. 9028. 12516. 4/18~ 633ax male 9 10/9/2005 ## 2 B6 656aa 2644 4012. 5237 7404. 12/1~ 593ar male 1 9/12/2005 ## 3 B6 675ag 4004. 3054. 3816. 3761 2/9/~ 593ad male 5 10/13/20~ ## 4 B6 675ai 11754. 8863 11784 11684 2/9/~ 593ad male 4 10/13/20~ ## 5 B6 656af 6906. 5322. 10424. 8468. 12/1~ 593ar male 2 10/13/20~ ## 6 B6 656al 6517 4440 5334. 9291 12/1~ 554aa male 8 10/9/2005 ## # ... with 3 more variables: total &lt;dbl&gt;, age &lt;int&gt;, group &lt;chr&gt; tail(wheels) ## # A tibble: 6 x 14 ## strain id day1 day2 day3 day4 dob mother sex wheel startdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Swiss 700af 11332 11000. 11858 14052 6/6/~ 674ah male 4 9/20/2005 ## 2 Swiss 700ah 9208 7454. 9616. 11182 6/6/~ 674be male 3 9/20/2005 ## 3 Swiss 700ai 5780 5260. 6736 7686. 6/7/~ 670ba male 2 9/20/2005 ## 4 Swiss 700al 7226. 11567 8804. 12274. 6/3/~ 674bp male 1 9/20/2005 ## 5 Swiss 700ap 4496. 5192. 6253 9181 6/3/~ 674af male 6 10/9/2005 ## 6 Swiss 700ar 4021 6878. 5194. 5925 6/3/~ 674bt male 2 10/9/2005 ## # ... with 3 more variables: total &lt;dbl&gt;, age &lt;int&gt;, group &lt;chr&gt; You can get more information about the data and general summaries by using the following code table(wheels$strain) ## ## B6 F1-129B6 F1-B6129 S129 Swiss ## 14 22 15 16 13 summary(wheels) ## strain id day1 day2 ## Length:80 Length:80 Min. : 6.5 Min. : 0.5 ## Class :character Class :character 1st Qu.: 5748.9 1st Qu.: 4727.0 ## Mode :character Mode :character Median : 8707.2 Median : 7347.8 ## Mean : 8953.8 Mean : 7503.7 ## 3rd Qu.:11897.0 3rd Qu.:10789.6 ## Max. :21815.5 Max. :15501.0 ## ## day3 day4 dob mother ## Min. : 1 Min. : 3 Length:80 Length:80 ## 1st Qu.: 4924 1st Qu.: 5854 Class :character Class :character ## Median : 8500 Median :10488 Mode :character Mode :character ## Mean : 8756 Mean :10951 ## 3rd Qu.:12506 3rd Qu.:14815 ## Max. :28515 Max. :30464 ## NA&#39;s :4 ## sex wheel startdate total ## Length:80 Min. :1.000 Length:80 Min. : 28 ## Class :character 1st Qu.:3.000 Class :character 1st Qu.:25118 ## Mode :character Median :5.000 Mode :character Median :32951 ## Mean :5.013 Mean :36766 ## 3rd Qu.:7.000 3rd Qu.:49044 ## Max. :9.000 Max. :80789 ## NA&#39;s :4 ## age group ## Min. : 89.0 Length:80 ## 1st Qu.:107.0 Class :character ## Median :130.0 Mode :character ## Mean :141.8 ## 3rd Qu.:148.5 ## Max. :311.0 ## Plotting the data with geom_boxplot() ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). The warning message is ok - it's saying some rows don't have data. However, notice the outlier. You can overlay the points on the boxplot, which can help you visualize the spread of the dta. ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot() + geom_point() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). You can customizing colors by chaning the fill and color of boxplots ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;dodgerblue&quot;) + geom_point(color=&quot;navy&quot;) ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). You can also make the points wobble a bit... but careful it goes crazy... ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;dodgerblue&quot;) + geom_jitter(color=&quot;navy&quot;) ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). You can constrain it by setting a width value of how much wobble. ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;dodgerblue&quot;) + geom_jitter(color=&quot;navy&quot;, width = .1) ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). Finish the graph by adding titles and labels etc. ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot(color=&quot;navy&quot;, fill=&quot;dodgerblue&quot;) + geom_jitter(color=&quot;navy&quot;, width = .1) + xlab(&quot;Mouse Strain&quot;) + ylab(&quot;Total Wheel Revolutions&quot;) + ggtitle(&quot;Wheel Running By Different Mouse Strains&quot;) + theme_classic() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). A couple of other things you might want to try: Make the outlier more noticeable ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot(outlier.color = &quot;green&quot;, outlier.size = 8) ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). If you prefer violin shapes to boxplots ggplot(wheels, aes(x = strain, y = total)) + geom_violin() ## Warning: Removed 4 rows containing non-finite values (stat_ydensity). You can also color according to the x-axis category ggplot(wheels, aes(x = strain, y = total)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ggplot(wheels, aes(x = strain, y = total, fill = strain)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). Adding points ggplot(wheels, aes(x = strain, y = total, fill = strain)) + geom_boxplot() + geom_jitter(width=.1) ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). 5.6 Bar Graphs Bar graphs are used to visualize groups of differing amounts. Bar graphs are best when the changes between groups are larger. Example 1. If you have a dataset where you HAVE already counted the number of each group Import the number1s.csv data. This shows how many number 1 songs various artists have had in the UK singles charts. df &lt;- read_csv(&quot;data/number1s.csv&quot;) ## Parsed with column specification: ## cols( ## name = col_character(), ## total = col_integer() ## ) head(df) ## # A tibble: 6 x 2 ## name total ## &lt;chr&gt; &lt;int&gt; ## 1 Elvis 21 ## 2 The Beatles 17 ## 3 Cliff Richard 14 ## 4 Westlife 14 ## 5 Madonna 13 ## 6 Take That 12 geom_col() is used when the data is already counted. ggplot(df, aes(x = name, y = total) ) + geom_col() Notice default order is alphabetical. However, you can reorder by putting reorder around the x-axis column. ggplot(df, aes(x = reorder(name, total), y = total) ) + geom_col() ggplot(df, aes(x = reorder(name, -total), y = total) ) + geom_col() When changing color use 'fill' here because it's a shape. ggplot(df, aes(x = reorder(name, -total), y = total) ) + geom_col(fill = &quot;yellow&quot;) Then for the border of the bars use 'color'. ggplot(df, aes(x = reorder(name, -total), y = total) ) + geom_col(fill = &quot;yellow&quot;, color=&quot;black&quot;) Finish with final customizations ggplot(df, aes(x = reorder(name, -total), y = total) ) + geom_col(fill = &quot;orange&quot;, color=&quot;brown&quot;) + xlab(&quot;Artist&quot;) + ylab(&quot;Total Number 1&#39;s&quot;) + ggtitle(&quot;Number 1 hits in UK&quot;) + theme_classic() Example 2. If you have a dataset where you haven't already counted the number of each group We want to plot how many people have each type of pet. This means pet will be on x-axis, and count on y-axis. load in pets.csv dataset and look at the data. pets &lt;- read_csv(&quot;data/pets.csv&quot;) ## Parsed with column specification: ## cols( ## name = col_character(), ## pet = col_character() ## ) head(pets) ## # A tibble: 6 x 2 ## name pet ## &lt;chr&gt; &lt;chr&gt; ## 1 Leon Cat ## 2 Lesley Dog ## 3 Devon Dog ## 4 Timothy Dog ## 5 Paul None ## 6 Jody Cat tail(pets) ## # A tibble: 6 x 2 ## name pet ## &lt;chr&gt; &lt;chr&gt; ## 1 Dong Bird ## 2 Willie Dog ## 3 Kris Cat ## 4 Yong Dog ## 5 Andrea Cat ## 6 Lacy Dog Notice 'pet' is categorical. We don't yet have a 'count' of how many of each pet there is. We can quickly look at the 'count' like this: table(pets$pet) ## ## Bird Cat Dog None ## 2 6 11 6 We can make ggplot plot the frequency/count of each pet as a bar graph Notice here, we use geom_bar() - and it counts for us. We do not need to supply a y column. ggplot(pets, aes(x = pet)) + geom_bar() Then just customize. ggplot(pets, aes(pet)) + geom_bar(color=&quot;black&quot;, fill=&quot;plum3&quot;) + xlab(&quot;Pet&quot;)+ ylab(&quot;Total&quot;)+ ggtitle(&quot;Popularity of Pets in a Class&quot;) 5.7 All the extras "],
["descriptives.html", "6 Descriptives 6.1 Sample vs Population 6.2 Basic Descriptives 6.3 Mean, Median, and Mode 6.4 Standard Deviation 6.5 Standard Error 6.6 Median and Inter-quartile Ranges 6.7 Descriptives for Groups 6.8 Comparing population and sample means", " 6 Descriptives Descriptive statistics describe basic features of the data in simple summaries such as mean, median, and mode. These statistics used to present quantitative descriptions of data in graphing. You can use functions in base R and tidyverse to get descriptive statistics. library(tidyverse) atx &lt;- read_csv(&quot;data/austin_weather.csv&quot;) head(atx) # first 6 rows ## # A tibble: 6 x 4 ## month day year temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 2019 43.3 ## 2 1 2 2019 39.4 ## 3 1 3 2019 41.2 ## 4 1 4 2019 44.1 ## 5 1 5 2019 48.6 ## 6 1 6 2019 48.8 Focus in the temperature column for now ggplot(atx, aes(x= temp)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;, binwidth = 10)+ theme_classic() 6.1 Sample vs Population statistics vs parameters.... insert more here 6.2 Basic Descriptives length(atx$temp) # length this tells you the &#39;n&#39; ## [1] 365 sum(atx$temp) # sum ## [1] 25107.5 range(atx$temp) # range ## [1] 34.5 89.2 min(atx$temp) # minimum ## [1] 34.5 max(atx$temp) # maximum ## [1] 89.2 var(atx$temp) # variance ## [1] 222.2072 6.3 Mean, Median, and Mode mean(atx$temp) # mean ## [1] 68.78767 sum(atx$temp)/length(atx$temp) # mean ## [1] 68.78767 median(atx$temp) # median ## [1] 70.8 #estimate mode function estimate_mode &lt;- function(x) { d &lt;- density(x) d$x[which.max(d$y)] } estimate_mode(atx$temp) # using the function to get estimate mode ## [1] 83.69078 For some descriptives, like mode, there is not a function already built into R, so a function needs to be made. The above code has a function for the estimated mode, this function works best with large data sets. 6.4 Standard Deviation 6.4.1 Average Deviation explain this concept 6.4.2 Standard Deviation explain this concept sd(atx$temp) # sample standard deviation ## [1] 14.90662 #population standard deviation function pop.sd &lt;- function(s) { sqrt(sum((s - mean(s))^2)/length(s)) } pop.sd(atx$temp) # population standard deviation with the function ## [1] 14.88618 6.5 Standard Error What about standard error? length(atx$temp) # we need to know the N to calculate the SEM ## [1] 365 sd(atx$temp) / sqrt(length(atx$temp)) #SEM by hand ## [1] 0.780248 sem &lt;- function(x){sd(x) / sqrt(length(x))} # a function to get SEM sem(atx$temp) #using the function ## [1] 0.780248 please note - if you have missing data use this function for SEM sem &lt;- function(x){sd(x,na.rm=T) / sqrt(length(na.omit(x)))} 6.6 Median and Inter-quartile Ranges 6.6.1 Median is the marker of 50% of the data...... explain with image. 6.6.2 IQRs markers for 25% and 75% - essentially medians of the lower half and upper half of the data. quantile(atx$temp, .25) # this is the lower quartile ## 25% ## 56.5 quantile(atx$temp, .75) # this is the upper quartile ## 75% ## 83.3 IQR(atx$temp) # this is the inter-quartile range. ## [1] 26.8 Don't be worried if these quartiles are slightly different to what you'd get by hand. ggplot(atx, aes(y=temp)) + geom_boxplot(color=&#39;black&#39;, fill=&#39;lightseagreen&#39;) nb we get those numbers on the x-axis because there is no 'group' we can get rid of them like this: ggplot(atx, aes(y=temp)) + geom_boxplot(color=&#39;black&#39;, fill=&#39;lightseagreen&#39;) + scale_x_discrete(breaks = NULL) 6.7 Descriptives for Groups Often in experiments you are looking for group differences, the following code will show you how to get descriptive statistics for each group. This is a time where tidyverse comes in handy! First read in the data and notice it has a group column, genre. vg &lt;- read_csv(&quot;data/videogames.csv&quot;) head(vg) ## # A tibble: 6 x 12 ## name platform year genre publisher NA_sales EU_sales JP_sales global_sales ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Wii ~ Wii 2006 Spor~ Nintendo 41.4 29.0 3.77 82.5 ## 2 Mari~ Wii 2008 Raci~ Nintendo 15.7 12.8 3.79 35.6 ## 3 Wii ~ Wii 2009 Spor~ Nintendo 15.6 11.0 3.28 32.8 ## 4 Wii ~ Wii 2007 Spor~ Nintendo 8.92 8.03 3.6 22.7 ## 5 Wii ~ Wii 2009 Spor~ Nintendo 9.01 8.49 2.53 21.8 ## 6 Gran~ PS3 2013 Acti~ Take-Two~ 7.02 9.14 0.98 21.1 ## # ... with 3 more variables: critic &lt;int&gt;, user &lt;dbl&gt;, rating &lt;chr&gt; The function table() can show you the n for all groups. table(vg$genre) ## ## Action Racing Shooter Sports ## 997 349 583 573 Using describeBy() from the psych package can be a very quick and easy, but a bit annoying to look at. It also ignores missing data which is helpful. remember to install the psych package before using it. library(psych) ## this is not working on my computer describeBy(vg, group = &quot;genre&quot;) Writing code using tidyverse can give us descriptive statistics in a more organized way. vg %&gt;% group_by(genre) %&gt;% summarise(meanNA = mean(NA_sales)) ## # A tibble: 4 x 2 ## genre meanNA ## &lt;chr&gt; &lt;dbl&gt; ## 1 Action 0.407 ## 2 Racing 0.397 ## 3 Shooter 0.555 ## 4 Sports 0.603 If you had missing data, you'd do it like this. Also, as.data.frame() just helps us see decimal places. vg %&gt;% group_by(genre) %&gt;% summarise(meanNA = mean(NA_sales, na.rm = T)) %&gt;% as.data.frame() ## genre meanNA ## 1 Action 0.4071013 ## 2 Racing 0.3967908 ## 3 Shooter 0.5554717 ## 4 Sports 0.6034031 You can do several summaries at once like this vg %&gt;% group_by(genre) %&gt;% summarise(meanNA = mean(NA_sales), sd_NA = sd(NA_sales), meanEU = mean(EU_sales), sd_EU = sd(EU_sales)) %&gt;% as.data.frame() ## genre meanNA sd_NA meanEU sd_EU ## 1 Action 0.4071013 0.8189867 0.2668004 0.5772930 ## 2 Racing 0.3967908 1.0315188 0.3085100 0.8522769 ## 3 Shooter 0.5554717 1.2165479 0.3310292 0.6674269 ## 4 Sports 0.6034031 1.9868151 0.3260733 1.4352370 To save time, you can tell it to just get the summary of all numeric columns. vg$year &lt;- as.factor(vg$year) # just need to make year non-numeric first vg %&gt;% group_by(genre) %&gt;% summarise_if(is.numeric, mean, na.rm = T) %&gt;% as.data.frame() ## genre NA_sales EU_sales JP_sales global_sales critic user ## 1 Action 0.4071013 0.2668004 0.04857573 0.8349649 68.01605 7.117954 ## 2 Racing 0.3967908 0.3085100 0.03246418 0.8630946 69.84241 6.991691 ## 3 Shooter 0.5554717 0.3310292 0.02288165 1.0245969 70.48714 6.948885 ## 4 Sports 0.6034031 0.3260733 0.04808028 1.1108028 72.94764 6.970157 vg %&gt;% group_by(genre) %&gt;% summarise_if(is.numeric, sd, na.rm = TRUE) %&gt;% as.data.frame() ## genre NA_sales EU_sales JP_sales global_sales critic user ## 1 Action 0.8189867 0.5772930 0.14425469 1.670552 14.20560 1.339989 ## 2 Racing 1.0315188 0.8522769 0.24017403 2.363348 14.00640 1.497225 ## 3 Shooter 1.2165479 0.6674269 0.07343275 2.087551 15.11614 1.540249 ## 4 Sports 1.9868151 1.4352370 0.30421425 3.996394 13.44039 1.463734 6.8 Comparing population and sample means Why we need the sample standard deviation. The sample SD is an estimate of the population SD. We need this estimate because we never have data from the full population but only random samples. The formula for population sd is the square root of the variance divided by N, where are the sample sd is the square root of the variance divided by n -1. x &lt;- c(14, 11, 5, 3, 8, 10, 9, 15) pop.sd(x) ## [1] 3.838538 sd(x) ## [1] 4.10357 The is a .3 difference if the standard deviations. However, would the difference be as big with a bigger sample? set.seed(1) # just so we all get the same results x &lt;- rnorm(100, mean = 8) #100 random numbers with mean of 8. pop.sd(x) ## [1] 0.8936971 sd(x) ## [1] 0.8981994 Sample SD is larger than population SD but this difference gets smaller as sample sizes increase. For visual proof that using the population SD underestimates for samples. For the example say we have a population of 1000, with a mean of 199.91 and population SD of 8.28. set.seed(1) population &lt;- rnorm(1000, mean = 200, sd = 8) mean(population) #199.91 ## [1] 199.9068 pop.sd(population) #8.28 ## [1] 8.275186 What if you did not know the real population SD and you were going to take samples of size 15 to try and estimate the population mean and SD. s1 &lt;- sample(population, 15, replace = T) mean(s1) ## [1] 200.7815 pop.sd(s1) ## [1] 9.27111 sd(s1) ## [1] 9.59651 Let's try another sample: s2 &lt;- sample(population, 15, replace = T) mean(s2) ## [1] 202.2548 pop.sd(s2) ## [1] 7.38066 sd(s2) ## [1] 7.639709 So, in one sample the sample SD was closer to the real pop SD in the other, the popSD was closer to the real pop SD. Now let's do this for 10,000 samples: results.means&lt;- vector(&#39;list&#39;,10000) results.popSD&lt;- vector(&#39;list&#39;,10000) results.sampSD&lt;- vector(&#39;list&#39;,10000) for(i in 1:10000){ s &lt;- sample(population, 15, replace = T) results.means[[i]] &lt;- mean(s) results.popSD[[i]] &lt;- pop.sd(s) results.sampSD[[i]] &lt;- sd(s) } Sample mean, remember the population mean = 199.91 means &lt;- unlist(results.means) # sample means mean(means) ## [1] 199.872 Standard deviations using the population SD popSDs &lt;- unlist(results.popSD) # SDs using popSD mean(popSDs) ## [1] 7.839468 Sample SDs sampSDs &lt;- unlist(results.sampSD) # sample SDs mean(sampSDs) ## [1] 8.11462 Now graph everything! This shows the sample mean is a good etimate of the population mean, on average. ggplot(data.frame(means), aes(x=means)) + geom_histogram(color=&#39;black&#39;, fill=&#39;blue&#39;, alpha=.2)+ theme_classic() + geom_vline(xintercept = mean(means), lwd=1, color=&quot;black&quot;) + geom_vline(xintercept = 199.91, lwd=1, color=&quot;orange&quot;, lty=2) + ggtitle(&quot;Sample Means Distribution&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. However, this shows the population SD formula with samples, leads us to underestimating the real population SD - on average there is a lot of variation though. ggplot(data.frame(sampSDs), aes(x=sampSDs)) + geom_histogram(color=&#39;black&#39;, fill=&#39;blue&#39;, alpha=.2)+ theme_classic() + geom_vline(xintercept = mean(sampSDs), lwd=1, color=&quot;black&quot;) + geom_vline(xintercept = 8.28, lwd=1, color=&quot;orange&quot;, lty=2) + ggtitle(&quot;SD Distribution when using Sample SD&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Overall, using the sample SD formula with samples, provides a better estimate of the population SD on average. This is why we divided by n-1 for sample SD. "],
["distributions.html", "7 Distributions 7.1 What is a distribution ? 7.2 z-scores 7.3 What is a Sampling Distribution ? 7.4 Central Limit Theorem. 7.5 Sampling distribution problems. 7.6 The t-distribution", " 7 Distributions blah blah intro blah 7.1 What is a distribution ? In statistical terms a distribution refers to the range of possible values that can come from a sample space. Another way of stating that is to say that a distribution represents how the probabilities of getting various values are distributed. There are several classic distributions in statistics. There are distributions such as the normal, t, Poisson, bimodal etc. We're going to dig a bit deeper into distributions, in particular the normal distribution. The best way to look at distributions is to plot histograms. Let's look at some distributions. 7.1.1 Uniform Distribution The first distribution we'll look at is the uniform. A uniform distribution is one where there's an equal probability of getting each value from the distribution. In the example below, we've grabbed 1,000,000 numbers from a uniform distribution that starts at 0 and ends at 100. Let's look at it's shape: As you can see, the histogram that we have generated is roughly flat across the top. This means that we have equal frequency counts in each bin. Each bin here is 5 across, so the first bin is 0-5, the next bin is 5-10, and so on. We have 25 bins in total in this histogram, and each has roughly 50,000 values in it. This is the classic shape of the uniform distribution. 7.1.2 Bimodal Distribution Another family of distributions that is worth our attention are bimodal distributions. In these distributions we have two peaks in the distribution. You can see an example below: 7.1.3 Normal Distribution In most domains, the type of histograms i.e. distributions, that we most commonly observe don't have 0 peaks like the uniform distribution or 2 peaks like the bimodal distribution, but have just one peak. These are called unimodal distributions. One such classic distribution that is very important to statistics is the normal distribution. The normal distribution has one peak and is symmetrical, with the same proportion of data on each side of the distribution. In addition, we say that a normal distribution has a skewness of 0 and a kurtosis of 3. We'll talk about what those are a little bit more about what that means very shortly. Let's look at a normal distribution. The following normal distribution has a mean of 100 and a standard deviation of 5. We can generate it by collecting 1,000,000 datapoints in R: set.seed(1) x &lt;- rnorm(n = 1000000, mean = 100, sd = 5.0) mean(x) ## [1] 100.0002 sd(x) ## [1] 5.000926 dfnorm &lt;- data.frame(vals = x) p &lt;- ggplot(dfnorm, aes(x = vals)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;purple&quot;, alpha=.4, binwidth = 0.5) + geom_density(alpha = 0.7, fill = &quot;mistyrose&quot;) + theme_classic() + xlab(&quot;values&quot;) p Normal distributions can vary in their means and standard deviations. Below is an image of a selection of four different normal distributions that all vary in their means and standard deviations. The dotted vertical black line in each graph indicates where their respective means lie. We use special notation to indicate that a distribution is a Normal Distribution. For instance, for the normal distribution that has a mean of 17 and a standard deviation of 7, we would write: \\(N(\\mu=17, \\sigma^{2}=49)\\) which demonstrates that the distribution is approximately normal with a mean of 17 and variance of 49 (which is the standard deviation, 7, squared). Normal Distributions 7.1.4 Standard Normal Distribution Although normal distributions can have various means or standard deviations, there is one case that we reserve and call the standard normal distribution. This is for the situation where the mean of the distribution is 0 and the standard deviation (and the variance) is equal to 1. \\(N(\\mu=0, \\sigma^{2}=1)\\) Normal Distributions How does the standard normal distribution come about? We will discuss more about this distribution in section xxx.xxx, but briefly it is obtained by converting all the values of a normal distribution into z-scores. z-scores are calculated by: \\(z=\\frac{x - {\\mu}_x}{\\sigma_x}\\) This standard normal distribution is very useful in statistics because we can precisely calcualte the proportion of the distribution that is to the left or right under the curve at any point of it. This principle forms the basis of several statistical tests. 7.1.5 Skewness and Kurtosis Above we described that a normal distribution has a skewness of 0 and a kurtosis of 3, but then we just skipped along and didn't really say anything else. It's important to take a quick step back and think about these two things. 7.1.5.1 Skewness We most commonly evaluate skewness for unimodal distributions (those with one peak). The skewness of a distribution can be either negative or positive, or, if it has no skew whatsoever it will be 0. It is probably easiest to describe skewness by looking at examples. In the picture below, all distributions have a mean of 100 and a standard deviation of 20. However, they differ in their skewness. The one on the left has a skew of +0.68, the one on the right has a skew of -0.68. The one in the middle has a skew of 0 and is the only one that is normally distributed. Skewness Distributions that have negative skew are also called left skewed because their longest tail extends to the left of the distribution. Similarly, distributions that have positive skew are called right skewed because their longest tail extends to the right of the distribution. Another thing to consider about the skew of the distribution is what happens to the mean, median and mode of the distributions. First, let's look at the normal distribution we made earlier in this section that had a mean of approximately 100 and a standard deviation of approximately 5. If we get the median, mode and mean of that distribution, we get the following: mean(x) ## [1] 100.0002 median(x) ## [1] 100.0025 estimate_mode(x) # R doesn&#39;t have a built in mode function, so I&#39;m using this as a proxy ## [1] 100.05 We can see here, that all three values are really close to 100. We can look at this in our plot of the distribution. We've overlaid a red line for the mean, a blue line for the median and an orange line for the mode. However, they all lie on top of each other at x=100, so it's hard to distinguish them: p + geom_vline(xintercept = median(x), color = &quot;blue&quot;, lwd=1)+ geom_vline(xintercept = mean(x), color = &quot;red&quot;, lwd=1)+ geom_vline(xintercept = estimate_mode(x), color = &quot;darkorange&quot;, lwd=1) Now let's look at some skewed distributions as to what happens to the mode, median and mean in such distributions. In this dataset, we have 7486 rows of data (observations). Each row is a MLB player. The three numerical columns refer to the career total hits (totalH), career total at bats (totalAB), and career batting average (avg) of each player. bats &lt;- read_csv(&quot;data/batting.csv&quot;) ## Parsed with column specification: ## cols( ## playerID = col_character(), ## totalH = col_integer(), ## totalAB = col_integer(), ## avg = col_double() ## ) nrow(bats) ## [1] 7486 head(bats) ## # A tibble: 6 x 4 ## playerID totalH totalAB avg ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 aaronha01 3771 12364 0.305 ## 2 aaronto01 216 944 0.229 ## 3 abbated01 772 3044 0.254 ## 4 abbeybe01 38 225 0.169 ## 5 abbeych01 493 1756 0.281 ## 6 abbotfr01 107 513 0.209 #histogram p1 &lt;- ggplot(bats, aes(x = avg)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;lightseagreen&quot;, alpha = 0.2, binwidth = .005) + geom_density(colour = &#39;black&#39;, lwd=1) + theme_classic() + xlab(&quot;Career Batting Average&quot;) p1 As you can see, this distribution is very negatively (left) skewed. This means that there are many players who have career batting averages between 0.2 and 0.3. There are relatively few players with career averages over 0.3. There are more averages that are less than 0.2 causing the skew. We can directly measure the skewness using the skewness() function from the moments package. We can see that it is highly negatively skewed with a value of -1.01: library(moments) ## Warning: package &#39;moments&#39; was built under R version 3.5.2 skewness(bats$avg) ## [1] -1.012683 Let's look at where the median, mean and mode are for this negatively skewed distribution. median(bats$avg) ## [1] 0.2466792 mean(bats$avg) ## [1] 0.237972 estimate_mode(bats$avg) ## [1] 0.2538827 This time, these descriptive values are not equal. The median and mean are lower than the mode. In fact, the mean is lowest of all. In negative skewed distributions, the mean and median get pulled towards the skewed tail of the distribution, but the mean gets pulled further. p1 + geom_vline(xintercept = median(bats$avg), color = &quot;blue&quot;, lwd=1)+ geom_vline(xintercept = mean(bats$avg), color = &quot;red&quot;, lwd=1)+ geom_vline(xintercept = estimate_mode(bats$avg), color = &quot;darkorange&quot;, lwd=1) Now let's look at what happens to the mode, median and mean in right skewed distributions. Let's look at the career at-bats of MLB players. #histogram p2 &lt;- ggplot(bats, aes(x = totalAB)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;plum&quot;, alpha = 0.2, binwidth = 200) + geom_density(colour = &#39;black&#39;, lwd=1) + theme_classic() p2 This distribution is extremely right (positive) skewed. If we measure the skewness we find that the skewness is 1.63. skewness(bats$totalAB) ## [1] 1.626115 Now, let's look at the median, mean and mode and plot these on the distribution: median(bats$totalAB) ## [1] 1037 mean(bats$totalAB) ## [1] 1958.68 estimate_mode(bats$totalAB) ## [1] 454.0069 p2 + geom_vline(xintercept = median(bats$totalAB), color = &quot;blue&quot;, lwd=1)+ geom_vline(xintercept = mean(bats$totalAB), color = &quot;red&quot;, lwd=1)+ geom_vline(xintercept = estimate_mode(bats$totalAB), color = &quot;darkorange&quot;, lwd=1) In these severely right skewed distribution, we again see that the median and mean get pulled towards the tail of the distribution, with the mean getting pulled even further than the median. Let's have a look at a quick summary figure of where the mean, median and mode lie with respect to each other in skewed distributions. As you can see, the mean always gets pulled the furthest to the tail of distributions.The reason for this is that the mean is much more affected by extreme outliers than the median. The median is simply the boundary which divides the top 50% of the data from the bottom 50% of the data. The mean has to include all values in its calculation, so can be largely affected by extreme values more so than the median. Skewness 7.1.5.2 Kurtosis blah blah blah need to think what to put into this section probably an image of different kurtoses of different distributions. don't need much more than that. kurtosis(bats$totalAB) # &gt;3 = &#39;peaky&#39; less in shoulders of tails ## [1] 5.325537 kurtosis(bats$avg) # &gt;3 = &#39;peaky&#39; less in shoulders of tails ## [1] 4.18612 # kurtosis close to 3 # Note with a smaller sample size, our distribution will # not be as normal, i.e. more skewed and not mesokurtic x1 &lt;- rnorm(n = 10, mean = 100, sd = 5.0) x1 ## [1] 101.45280 95.88441 94.74933 103.10539 99.47551 99.23445 98.87608 ## [8] 102.12954 100.12076 99.62868 mean(x1) ## [1] 99.46569 sd(x1) ## [1] 2.586687 skewness(x1) ## [1] -0.5130139 kurtosis(x1) ## [1] 2.484758 7.2 z-scores z-scores are a useful way of comparing different scores in different distributions. As an example, let's look at the two distributions below. On the left we have the population of all Airedale Terriers that is normally distributed with a mean of 60 lbs with a standard deviation of 6 lbs. On the right we have the population of all Scottish Terriers that is normally distributed with a mean of 20 lbs and a standard deviation of 0.4 lbs. terriers If you owned an Airedale Terrier that was 65 lbs and a Scottish Terrier that was 20.5 lbs, would you be able to say which one was relatively larger than their breed average? Both of them are above the mean of their breeds, but by how much? The Airedale Terrier is (65-60) 5 lbs heavier than the mean of the breed, whereas the Scottish Terrier is only (20.5-20) 0.5 lbs heavier than its breed average. Looking at things in absolute terms however is misleading. It would be better if we could somehow standardize these differences. This is where z-scores come in. z-scores enable us to calcualte how far any datapoint is from the mean of its distribution by saying how many &quot;standard deviations&quot; away from the mean it is. Let's look at the Airedale Terrier. Your Airedale is 5 lbs heavier than the mean of 60 lbs. This is a bit less than one standard deviation above the mean, as the standard deviation is 6 lbs. However, your Scottish Terrier is 0.5 lbs heavier than the mean of 20 lbs, which is a bit more than the standard deviation of 0.4 lbs for that breed. We can calculate precisely how many standard deviations away from the mean they are using z-scores. This is the formula: \\(z=\\frac{x - {\\mu}_x}{\\sigma_x}\\) Using this formula, let's calculate the z-scores for each of our dogs: #Airedale (65 - 60) / 6 ## [1] 0.8333333 #Scottish (20.5 - 20) / 0.4 ## [1] 1.25 The z-score for our 65 lb Airedale is z=0.83. The z-score for our 20.5 lb Scottish is z=1.25. This shows us that our Scottish Terrier is actual more standard deviations away from its breed mean than is our Airedale Terrier dog. We could also plot each of these z-scores on top of the standard normal distribution. Remember, this is the specific case of the normal distribution where the mean of the distribution is 0 and the standard deviation is 1. Shown below, we've plotted on the top row the breed population histograms with red vertical lines the weights of each of these dogs on their respective population histogram. On the bottom row we have these values converted to their z-scores and still shown with a red line. Each is overlaid on top of a standard normal distribution. z-scores z-scores can be very useful ways of standardizing observed values into ways that we can directly compare across different distributions. If we calculate a negative z-score then it's clear that our observed value is below the population mean, and if we calculate a positive z-score then our value is greater than the population mean. The size of the z-score relates to how many population standard deviations from the mean each value is overall. 7.2.1 z-scores in samples. Often we may not know the population mean or standard deviation. In such cases if all we have is a sample mean and a sample standard deviation, we can still calcualte z-scores for such samples. We effectively use the same formula to calculate the z-scores, just subsituting in the sample mean and standard deviation. \\(z=\\frac{x - {\\mu}_x}{\\sigma_x}\\) For instance, let's look at the following sample of ages for players on a village cricket team: ages &lt;- c(23, 19, 21, 33, 51, 40, 16, 15, 61, 55, 30, 28) mean(ages) ## [1] 32.66667 sd(ages) ## [1] 15.74417 These data clearly don't look normally distributed, but we still are able to calculate a mean and standard deviation. We can also still calculate the z-scores for each age: z &lt;- (ages - mean(ages)) / sd(ages) z ## [1] -0.61398401 -0.86804636 -0.74101519 0.02117186 1.16445243 0.46578097 ## [7] -1.05859312 -1.12210871 1.79960831 1.41851478 -0.16937490 -0.29640607 You'll notice that those individuals that have negative z-scores are younger than the mean age of 32.67. Those individuals with positive z-scores are older than the mean age of 32.67. The largest z-score in terms of magnitude (either in the positive or negative direction) is 1.8. This person was 61 and was 1.8 standard deviations older than the average age. Although it's possible to calculate z-scores for any sample, if the sample data come from a normally distributed population then we can use this z-score principle to perform inferential statistics (see xxx.xxx) 7.2.2 Using z-scores to determine probabilities One of the great things about the normal distribution, is that we can calculate quite straightforwardly what proportion of the distribution lies under the curve for any distance away from the mean measured in standard deviation. With computers, this is a very trivial task to perform, as we'll see shortly. Prior to the computer age, these computations weren't as easy, so we'd often use something called the empirical rule. This is basically a framework that tell us what proportion of the distribution lies under the curve at 1\\(\\sigma\\), 2\\(\\sigma\\), 3\\(\\sigma\\), etc. from the mean. Let's look at the distribution below, which is normally distributed with a mean (\\(\\mu\\)) of 14 and a standard deviation (\\(\\sigma\\)) of 4. The first thing to note is that a normal distribution is perfectly symmetrical, with equal area under the curve on either side of the mean. Therefore, in our example, 50% of the distribution lies below the mean of 14, and 50% of datapoints lie above the mean. The area colored in green in the distribution represents the area of the distribution that lies \\(\\mu \\pm1\\sigma\\). The area colored in pinky-purple lie between \\(\\mu+1\\sigma\\) and \\(\\mu+2\\sigma\\) or between \\(\\mu-1\\sigma\\) and \\(\\mu-2\\sigma\\). The area colored in yellow lie between \\(\\mu+2\\sigma\\) and \\(\\mu+3\\sigma\\) or between \\(\\mu-2\\sigma\\) and \\(\\mu-3\\sigma\\). The blue areas represent the proportion of the distribution that lies beyond \\(\\mu \\pm4\\sigma\\). Rather than look at the proportion that lies between two boundaries, often instead we describe the proportion of the distribution that lies to the left of a certain value. The table below shows what proportion of the distribution lie to the left of each value. For instance, in the above distribution \\(\\mu+2\\sigma=22\\). According to the table below, we have 97.72% of the data/distribution that are to the left of \\(x=22\\). The above table and histogram are obviously useful guides for knowing what proportion of the data exist at certain breakpoints. But, what if you had a value of \\(x=17.5\\) in the above distribution? What proportion of the data are below this value? Well we can actually work this out if we convert our raw scores to z-scores. Once we have a z-score, we can calculate the area to the left of any point on the standard normal curve. Our value of \\(x=17.5\\) has a z-score of 0.875, so it is 0.875 standard deviations above the mean. (17.5 - 14) /4 ## [1] 0.875 Let's look at that on the standard normal curve, with the value \\(z = 0.875\\) represented by the red solid line. We can obtain the area in the distribution to the left of this value that is shaded in light red in R, using the function pnorm(). pnorm(0.875) ## [1] 0.809213 This shows us that 80.92% of the distribution lie to the left of \\(z=0.875\\). To get what proportion of the distribution lie to the right of this value, we just subtract it from 1. 1 - pnorm(0.875) ## [1] 0.190787 Let's look at the proportions under the curve to the left of plus or minus 0, 1, 2, or 3 standard deviations from the mean. zvals &lt;- c(-3,-2,-1,0,1,2,3) pnorm(zvals) ## [1] 0.001349898 0.022750132 0.158655254 0.500000000 0.841344746 0.977249868 ## [7] 0.998650102 Hopefully you can see that these values mirror those in the table provided above. 7.2.2.1 z-score and probability problems. Let's take this a little further with some small examples. Example 1 Let's assume that the weights of pineapples are normally distributed with a mean of 1003.5g and a standard deviation of 35g. You bought a random pineapple and it turned out to only be 940g. What proportion of pineapples are less than 940g? How unlucky did you get to buy such a small pineapple? First, let's take a look at the population distribution of pineapples with \\(\\mu=1003.5\\) and \\(\\sigma=35\\). Our pineapple is 940g and is shown with the solid red line below. As our distribution is normal, if we convert this to a z-score we can compare it to where z is in the standard normal distribution on the right. (940 - 1003.5) / 35 ## [1] -1.814286 So we calculated that \\(z = -1.81\\), and we visualize that on our standard normal distribution. We're interested in what proportion of pineapples from the distribution are 940g or less. That is the light red shaded area. To calculate that we can just use pnorm() in R: pnorm(-1.81) ## [1] 0.03514789 From this we can see that only 3.5% of pineapples are less than 940g. We got super unlucky to get such a tiny pineapple. Example 2 What is the probability of getting a pineapple of greater than 1050g ? To answer this we first get the z-score for a pineapple of 1050g, and find that \\(z = 1.33\\). (1050 - 1003.5) / 35 ## [1] 1.328571 Next we recognize that if we're interested in what proportion of pineapples weigh more than 1050g, we need to know what proportion of the standard normal curve is greater than \\(z = 1.33\\) (the shaded light red area below). We can calculate that by using pnorm() to figure out what proportion is to the left of \\(z = 1.33\\), and then subtract that from 1 to get what proportion is to the right. 1 - pnorm(1.33) #0.003 (so 9.18% chance) ## [1] 0.09175914 Example 3 What is the probability of getting a pineapple between 950g and 1045g ? For this question, we're interested in the shaded light red area between \\(z = x\\) and \\(z = z\\) on the standard normal curve. Why these z-values? Because these are the z scores you get if you convert a 950g and a 1045g pineapple to z scores. z1 &lt;- (950 - 1003.5) / 35 z2 &lt;- (1045 - 1003.5) / 35 z1 #-1.53 ## [1] -1.528571 z2 #1.19 ## [1] 1.185714 In R we can calculate the proportion to the left of each of these z-scores using pnorm(). What we need is the shaded area, which we can get if we subtract the area to the left of \\(z = -1.53\\) from the area to the left of \\(z = 1.19\\). We do it like this: ## get proportion of curve to left of each z-score pnorm(z1) # 0.06 ## [1] 0.06318536 pnorm(z2) # 0.88 ## [1] 0.8821324 # so the area between them is: pnorm(z2) - pnorm(z1) #0.82 ## [1] 0.8189471 So, 82% of the distribution lie between 950g and 1040g. 7.3 What is a Sampling Distribution ? Another type of distribution that we will discuss a lot! is the sampling distribution. There are different types of sampling distributions, so for now we'll focus on the sampling distribution of the sample means. The best way to illustrate a sampling distribution, is to show it by example. Say we have a population of 1 million adult Archerfish. The population mean \\(\\mu\\) is 100.0mm, and the population standard deviation \\(\\sigma\\) is equal to 15.0mm. Let's create this population: set.seed(3) archerfish &lt;- rnorm(n = 1000000, mean = 100, sd = 15) mean(archerfish) ## [1] 100.0061 sd(archerfish) ## [1] 15.02418 Let's also plot what this normally distrubted population looks like by making a histogram: # histogram of the population: ggplot(data.frame(archerfish), aes(x = archerfish)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#f584ed&#39;, alpha=.5, binwidth =2) + theme_classic() + xlab(&quot;Body Length mm&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Body Sizes of 1 million Archerfish&quot;) + geom_vline(xintercept = 100.0, lwd=1, color=&#39;black&#39;) OK, let's say that you didn't have the time to go out and catch 1 million Archerfish and measure the body length of every single fish. What shoudld you do? One thing you might decide is to just go and take a random sample of 10 anglerfish (you could have picked another sample size - let's just stick with 10 for now). Once you have your sample of 10 archerfish, you could then measure them and you will be able to calculate the sample mean of that sample. We'll use sample() to randomly select 10 archerfish. set.seed(1) # so we all get the same sample. samp1 &lt;- sample(archerfish, 10, replace = T) samp1 ## [1] 83.93889 103.18896 77.19206 104.34995 98.54528 111.00236 85.95220 ## [8] 94.26505 99.79486 97.50949 mean(samp1) ## [1] 95.57391 Our sample mean is 95.6 - that's fairly close to the actual population mean of 100.0mm. Let's grab another three samples of 10 archerfish and see what the sample means are of those sample: mean(sample(archerfish, 10, replace = T)) # mean of our 2nd sample ## [1] 98.35256 mean(sample(archerfish, 10, replace = T)) # mean of our 3rd sample ## [1] 100.4263 mean(sample(archerfish, 10, replace = T)) # mean of our 4th sample ## [1] 99.98449 These next three samples are closer to 100.0, with some being a little bit less than the population mean, and the other being a bit more. What would happen if we collected thousands and thousands of samples of size 10? Let's do it - you don't need to follow the exact code here of how we're doing this, but essentially we're grabbing 10,000 samples of size 10. From each of these we're getting the sample mean. That means we'll end up with 10,000 sample means. We're storing those results in the object called res. ### What if we were to collect 10,000 samples and for each one get the mean results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- mean(sample(archerfish, 10, replace = T)) } res &lt;- unlist(results) Now we have our 10,000 sample means we could make a histogram of these sample means. psd &lt;- ggplot(data.frame(res), aes(x = res)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#4adbe0&#39;, alpha=.5, binwidth = 1) + theme_classic() + xlab(&quot;Mean Body Length of each sample - mm&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=10&quot;) + geom_vline(xintercept = mean(res), lwd=1) psd This distribution that we've just plotted is the sampling distribution of sample means for samples of size n=10. We picked 10,000 as the number of samples of size 10 to collect as it's reasonably large enough to get enough sample means that we can see the shape of the distribution. We could have picked 100,000 samples to collect, or 1,000,000... in fact the more the better, but 10,000 is enough to get the point across. Out of all of these sample means that we &quot;collected&quot;, what is the average across all of the 10,000 samples? - and what's more, what is the standard deviation of that distribution? mean(res) ## [1] 100.0255 sd(res) #standard deviation of the sampling distribution, aka standard error ## [1] 4.742526 Let's first focus on the mean. Our mean of sample means was 100.0, which is the same as the mean of 1,000,000 archerfish in the population! It turns out that the mean of the sampling distribution is approximately equal to the population mean. By the way, the notation that we use to depcit the mean of the sampling distribution of sample means is \\(\\mu_{\\overline{x}}\\). So \\(\\mu_{\\overline{x}} = 100.0\\). What is the standard deviation of this distribution (the sampling distribution of the sample means)? We just calculated it in R to be 4.74. The notation we use for this is \\(\\sigma_{\\overline{x}} = 4.74\\). It's called the standard deviation of the sampling distribution of sample means, but for short it gets called the standard error. Of course, we never in reality actually collect thousands and thousands of samples - that defeats the point of sampling. If we had time to collect thousands and thousands of samples, we may as well just measure every archerfish in the population. Usually, we just collect one sample. In later sections, we'll discuss how you can estimate what \\(\\mu_{\\overline{x}}\\) and \\(\\sigma_{\\overline{x}}\\) are when you have only collected one sample. However, given we already know the population standard deviation of the 1 million archerfish is \\(\\sigma=15\\), we can calculate the standard deviation of the sampling distribution of sample means for any sample size. It is calculated using the formula: \\(\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) So in our case it should be equal to: 15/sqrt(10) # 4.74 ## [1] 4.743416 As you can see this is the same value that we got in our simulated sampling distribution above. 7.3.1 Sample Size and the Sampling Distribution In the previous section we looked at what would happen if we took samples of size 10 from our archerfish population and looked at the sample means of each sample. We found that when we made a histogram out of these sample means, that the mean of the sample means \\(\\mu_{\\overline{x}}\\) was approximately equal to the population mean of 100. The standard deviation of the sampling distribution of sample means, or standard error, \\(\\sigma_{\\overline{x}}\\) was equal to 4.74. What happens if we were to take a different sized sample each time - say size 50? What would be the mean and standard deviation of this distribution of sample means? Let's find out. We'll again take 1000 samples of size 50 at random from our population of 1 million archerfish. For each sample we'll record the sample mean length of the 50 fish. We'll end up saving these 10,000 sample means in an R object called res1. Again, don't worry about how the code is doing it here, just as long as you follow what we're doing. # ok, get 10,000 samples of size 50 results1&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ results1[[i]] &lt;- mean(sample(archerfish, 50, replace = T)) } res1 &lt;- unlist(results1) Now we have our 10,000 samples, let's plot the histogram psd2 &lt;- ggplot(data.frame(res1), aes(x = res1)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#4adbe0&#39;, alpha=.5, binwidth = .5) + theme_classic() + xlab(&quot;Mean Body Length of each sample - mm&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=50&quot;)+ geom_vline(xintercept = mean(res1),lwd=1) psd2 If we look at the mean and standard deviation of this sampling distribution, we get the following values: mean(res1) # still same as population mean ## [1] 100.0114 sd(res1) # smaller with larger sample size ## [1] 2.116454 The mean of the sampling distribution of sample means \\(\\mu_{\\overline{x}}\\) is still a very good estimate of the population mean - in fact, it's a tiny, tiny bit better than when we had samples of size 10. The biggest difference is in the standard deviation of the sampling distribution, \\(\\sigma_{\\overline{x}}\\) (the standard error), which is much lower than when we had samples of size 10. The reason for this is that the variability in our sample means is much more reduced when we have samples of size 50. On average, our sample means are much closer individual estimates to the population mean. This is what drastically reduces the standard deviation of this sampling distribution. We could have calculated the standard error directly using the formula provided earlier, because we already know the population standard deviation. When we use that formala we get 2.12, essentially the same standard deviation as we got with our simulated sampling distribution: 15 / sqrt(50) ## [1] 2.12132 Let's directly compare the two sampling distributions for the two different sample sizes. We adjusted the x-axis so it is the same for both figures, so you can see the change in the standard deviation between the two sample sizes: 7.4 Central Limit Theorem. In the previous section we discovered that if you take many, many samples from a normally distributed population and calculated the sample mean of each sample, that you would get a sampling distribution of sample means. We also saw that that sampling distribution was normally distributed with a mean \\(\\mu_{\\overline{x}}\\) that was approximately equal to the population mean, and a standard deviation \\(\\sigma_{\\overline{x}}\\) that was equal to the population standard deviation divided by the square root of n \\(\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Is it just coincidence that both the population distribution and the sampling distribution was approximately normally distributed? What we will learn in this chapter is that it does not matter at all what the population distribution is - if we take thousands of samples from any shaped distribution and calculate the sample mean of each sample, when we create the histogram of those sample means we will find that they are approximately normally distributed. This is what we refer to as the central limit theorem. Further, the larger the sample size that we take, the closer to a normal distribution the sampling distribution becomes. Let's look at this by taking samples from various different population distributions. Uniform Distribution First we'll look at a uniform distribution of 1 million numbers between 0 and 75. You might like to think of this as the distance of trees from the center of a forest in km. Let's graph the distribution and calculate the mean and standard deviation. set.seed(1) #get data from uniform distribution x1 &lt;- runif(1000000, min = 0, max = 75) # histogram ggplot(data.frame(x1), aes(x = x1)) + geom_histogram(color=&#39;black&#39;, fill = &quot;#894ae0&quot;, alpha=.3, binwidth = 5, boundary = 0) + theme_classic() + xlab(&quot;Distance from Center of Forest km&quot;) ### Population Mean &amp; SD mean(x1) #37.5 ## [1] 37.49417 sd(x1) #21.6 ## [1] 21.6472 We can see that the mean of this population is 37.5, and the population standard deviation is 37.5. Let's take samples of size 30 at random from this population of 1 million. For each sample, we'll calculate the sample mean. We'll take 10,000 samples (again - we could have picked any really large number here, but 10,000 seems reasonable enough to prove our point). After we get our 10,000 sample means from samples of size 30, we'll plot the histogram of those sample means. ## Let&#39;s get 10,000 samples of size 30 results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- mean(sample(x1, 30, replace = T)) } res &lt;- unlist(results) # This is the sampling distribution. ggplot(data.frame(res), aes(x = res)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#894ae0&#39;, alpha=.5, binwidth = 1) + theme_classic() + geom_vline(xintercept = mean(res), lwd=1) + xlab(&quot;Mean of each sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=30&quot;) This histogram represents our sampling distribution of sample means when we took samples of size 30 from a uniform distribution. Hopefully you notice that it is approximately normally distributed - even though the original population was uniformally distributed! Let's calculate the mean and standard deviation of this sampling distribution: mean(res) # the mean of the sample means is close to 37.5, the population mean ## [1] 37.49964 sd(res) #3.9 - this is a lot smaller than the population SD ## [1] 3.995888 The mean of our sampling distribution \\(\\mu_{\\overline{x}} = 37.5\\) which is again approximately equal to the population mean. The sampling distribution standard deviation \\(\\sigma_{\\overline{x}} = 3.94\\) which is a lot lower than the original population standard deviation. Because we only took 10,000 samples, these values aren't exact, but we could have calculated the standard error by taking the population standard deviation and dividing by the square root of n \\(\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) as follows: sd(x1)/sqrt(30) # standard error. ## [1] 3.952219 Skewed Distributions We can see that the central limit theorem holds true for skewed distributions also. Here, we have a population of 1 million charity donations. Let's draw a hisogram of the population distribution and calculate the mean and standard deviation of the population. set.seed(1) # e.g. Donations to a charity (in $s) q &lt;- rnbinom(1000000, 5, .4) # histogram ggplot(data.frame(q), aes(x = q)) + geom_histogram(color=&#39;black&#39;, fill = &quot;#1ad665&quot;, alpha=.3, binwidth = 1, boundary = 0) + theme_classic() + xlab(&quot;Charity Donation in $&quot;) ### Population Mean mean(q) #7.5 ## [1] 7.504368 sd(q) #4.33 ## [1] 4.331767 It is clear that this population distribution is highly positively skewed. The mean of the population is 7.5 and the standard deviation is 4.33. In the following code we takes samples of size 30 and calculate the sample mean of each sample. This time, we'll collect 50,000 samples, just to be a bit different. results&lt;-vector(&#39;list&#39;,50000) for(i in 1:10000){ results[[i]] &lt;- mean(sample(q, 30, replace = T)) } res &lt;- unlist(results) ### Let&#39;s Draw this as a histogram. ggplot(data.frame(res), aes(x = res)) + geom_histogram(color=&#39;black&#39;, fill=&#39;#31e8d0&#39;, alpha=.5, binwidth = .1) + theme_classic() + geom_vline(xintercept = mean(res), lwd=1) + xlab(&quot;Mean of each sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=30&quot;) So, it happened again. This is our sampling distribution of sample means, collected from samples of size 30 from a highly skewed distribution. But once again, the sampling distribution is approximately normally distributed. It might not be perfectly normally distributed, but it is close to being normal. We can calculate the mean and the standard deviation of this sampling distribution: mean(res) # the mean of the sample means is close to 7.5 ## [1] 7.504313 sd(res) #0.79 ## [1] 0.7981555 \\(\\mu_{\\overline{x}} = 7.5\\) which is once again approximately equal to the original population mean. The sampling distribution standard deviation \\(\\sigma_{\\overline{x}} = 0.79\\). We could have directly calculated that using the formula for the standard error, n \\(\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\), as we know the original population standard deviation: sd(q)/sqrt(30) # standard error = 0.79 ## [1] 0.7908688 We could keep going with even more types of population distributions. We would find the same thing over and over again. If we take many samples from each population and calculated the sample means of all samples, they would form an approximately normally distribution. This will be especially true for larger samples. This is the basis of the central limit theorem. In the following sections we'll learn more about what we can do with these sampling distributions. 7.5 Sampling distribution problems. We can use our knowledge of sampling distributions and z-scores to determine how likely or unlikely we are to observe any one particular sample mean. Let's use an example to illustrate this. Q. Say the weight of chicken eggs is normally distributed with mean 60g and standard deviation of 3g. What is the probability of getting a batch of a dozen eggs that have a mean of less than 58g ? The way to think about these questions is to recognize that we're dealing with a sampling distribution. We're really being asked what proportion of the sampling distribution is less than a sample mean of 58g, when your sample size is 12 (a dozen). First, let's plot the population of chicken eggs and then the sampling distribution of samples means for a sample size of 12. set.seed(1) # so you get the same values as my script ### First, I&#39;ll make some plots of the &#39;population&#39; and &#39;sampling distribution&#39; ## Population x &lt;- rnorm(1000000, mean = 60, sd = 3) p1 &lt;- ggplot(data.frame(x), aes(x = x)) + geom_histogram(color=&#39;black&#39;, fill=&#39;mistyrose&#39;, alpha=.4)+ geom_vline(xintercept = 60, lwd=1) + ggtitle(&quot;Population of chicken eggs&quot;) + theme_classic() + xlab(&quot;Weight&quot;) ## Sampling Distribution of sample means with Sample size of 12 (a dozen). results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- mean(sample(x, 12, replace = T)) } res &lt;- unlist(results) p2 &lt;- ggplot(data.frame(res), aes(x=res)) + geom_histogram(color=&quot;black&quot;, fill=&#39;lightseagreen&#39;, alpha=.4)+ geom_vline(xintercept = mean(res),lwd=1) + ggtitle(&quot;Sampling Distribution of Sample Means&quot;) + theme_classic() + xlab(&quot;sample mean&quot;) library(gridExtra) grid.arrange(p1,p2,nrow=1) What are the mean and standard deviation of this sampling distribution of sample means? Well, \\(\\mu_{\\overline{x}} = 60\\) because that's the population mean, and we know the mean of the sample means is approximately the same. We know we can calculate \\(\\sigma_{\\overline{x}\\) because we know the population standard deviation \\(\\sigma\\). Again, the formula is \\(\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Therefore, the standard deviation of the sampling distribution is \\(\\sigma_{\\overline{x} = 0.87\\): sem &lt;- 3 / sqrt(12) sem ## [1] 0.8660254 Now, we're interested in a sample of 12 that has a sample mean of 58g. Let's visualize what that looks like on the histogram of the sampling distribution: p2 + geom_vline(xintercept = 58, lty=2, lwd=1, color=&quot;red&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means \\n for sample size = 12&quot;) Because we know the mean and standard deviation of this distribution, we can actually calculate the sample mean of 58g as a z-score. Doing this we find that \\(z = -2.31\\), which means that a sample mean of 58g is 2.31 standard deviations below the mean. # so, 58g as a z-score is... z &lt;- (58 - 60) / sem # -2.31 z ## [1] -2.309401 The original question asked what probability there was of getting a sample mean of less than 58g. This is basically what area is under the curve of the above sampling distribution to the left of the 58g sample mean. Because we converted that sample mean of 58g to a z-score of -2.31, we can look at that value on a standard normal curve. The area we are iterested in is the filled in area to the left of z = -2.31: We can look up this value in R, using the function pnorm. pnorm(z) # prob = 0.010 ## [1] 0.01046067 So we can see that the probability of getting a sample mean of lower than 58g is p=0.0105. 7.6 The t-distribution Another distribution that is important to know about is the t-distribution. Like the normal distribution, this is a symmetrical distribution, but it has slighly fatter tails than the normal distribution. The t-distribution comes up most commonly in sampling distributions. In particular, although the central limit theorem prescribes that sampling distributions are approximately normal, it is known that often they aren't approximately normal enough, and instead they follow a t-distribution shape. An important detail about the t-distribution is that there are actually several t-distributions. There are different t-distributions for different degrees of freedom. These differ slightly in how heavy the tails of the distribution are. The degrees of freedom are usually related to the sample size minus one or two (depending upon the test being employed - see sections xxx.xxx and xxx.xxx). The higher the degrees of freedom, the more closely the t-distribution looks like a normal distribution. You can see this in the image below. The standard normal distribution is in red, and the t-distribution is in black. Each panel shows a different t-distribution. As the degrees of freedom increase, the t-distribution essentially becomes the normal distribution. At lower degrees of freedom, there is a lot more difference between the t-distribution and the normal distribution in the tails. "],
["confidence-intervals.html", "8 Confidence Intervals 8.1 Sample means as estimates. 8.2 Calculating a confidence interval with z-distribution 8.3 Confidence Intervals with t-distribution 8.4 Calculating a t-distribution Confidence Interval 8.5 Comparing CIs using the z- and t-distributions", " 8 Confidence Intervals When we collect a sample, we typically calculate a sample mean \\(\\overline{x}\\). We use this as our estimate of the real population mean \\(\\mu\\), as this is unknown to us typically. If we were to collect another sample, we would most likely get a sample mean \\(\\overline{x}\\) that is slightly different to the first one, but would also be an estimate of the population mean \\(\\mu\\). Given that we are not completely sure of what the population mean \\(\\mu\\) is, or how close our sample mean \\(\\overline{x}\\) is to that population mean, one thing that we like to do is to put confidence limits around our sample mean estimate. The confidence interval gives us a range of values which likely contains our population mean. In essence, a confidence interval can be considered to be a margin of error around our sample mean estimate. In this course, we use two separate approaches to calculate confidence intervals around a sample mean \\(\\overline{x}\\). The first method uses the \\(z\\)-distribution to generate the confidence interval. The second method uses the \\(t\\)-distribution. In practice, we almost always use the \\(t\\)-distribution when doing this. In fact, the only time we really use the \\(z\\)-distribution is when teaching introductory stats. The reason for this, is that learning how to make a confidence interval using the \\(z\\)-distribution is a good stepping stone to using the \\(t\\)-distribution. Technically, we can use the \\(z\\)-distribution to calculate the confidence interval when we know the population standard deviation \\(\\sigma\\) and our sample size is relatively large. However, we almost never know \\(\\sigma\\), and so that's why in practice we use the \\(t\\)-distribution. We'll start this chapter by talking about the relationship between the sampling distribution and confidence intervals. Then we'll describe how to use both the \\(z\\)- and \\(t-\\)distributions to generate confidence intervals. 8.1 Sample means as estimates. Let us imagine we have a population of butterflies and we're interested in their wingspan. The population is normally distributed with a population mean \\(\\mu\\) is 7.8cm, with a population standard deviation \\(\\sigma\\) is 0.3cm. This is what this population distribution looks like: x &lt;- rnorm(100000, mean = 7.8, sd = 0.3) library(tidyverse) p1 &lt;- ggplot(data.frame(vals=x),aes(vals))+ geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;purple&quot;, alpha=.4, binwidth = 0.05) + geom_density(alpha = 0.7, fill = &quot;mistyrose&quot;) + theme_classic() + xlab(&quot;Wingspan cm&quot;)+ geom_vline(xintercept = 7.8, color=&#39;black&#39;,lwd=1) p1 Now, let's collect samples of size \\(n=15\\). Here's one sample, and it's sample mean \\(\\overline{x}\\): set.seed(1) samp1 &lt;- sample(x, size = 15, replace = T) samp1 ## [1] 7.532948 8.233244 8.014278 7.446299 8.294182 7.660900 8.045752 7.251308 ## [9] 7.267707 7.742027 8.243312 8.039407 8.367953 8.022449 7.930565 mean(samp1) ## [1] 7.872822 Our observed sample mean is \\(\\overline{x}=7.72\\) which is pretty close to the population mean of \\(\\mu=7.8\\). But if we were to collect another sample, then that sample mean will be slightly different. Let's do it again: samp2 &lt;- sample(x, size = 15, replace = T) samp2 ## [1] 8.050309 7.831114 7.672871 7.681791 7.198823 8.339807 7.520375 8.258590 ## [9] 7.671240 7.189658 8.489073 7.950898 7.670181 7.716136 7.326798 mean(samp2) ## [1] 7.771178 This time our sample mean (our estimate) is \\(\\overline{x}=7.95\\). If we did this thousands of times, then we'd get our sampling distribution of sample means (see section xxx.xxx). This is what our sampling distribution for sample sizes of \\(n=15\\) looks like: #get sample means for sampling distribution results&lt;-vector(&#39;list&#39;,100000) for(i in 1:100000){ results[[i]] &lt;- mean(sample(x, 15, replace = T)) } res &lt;- unlist(results) p2 &lt;- ggplot(data.frame(res), aes(x = res)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;, fill = &quot;#4adbe0&quot;, alpha=.4, binwidth = 0.01) + geom_density(alpha = 0.7, fill = &quot;ghostwhite&quot;) + theme_classic() + xlab(&quot;Sample Mean&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Sampling Distribution of Sample Means for n=15&quot;) + geom_vline(xintercept = mean(res), lwd=1) p2 According to Central Limit Theorem, this sampling distribution is approximately normally distributed. The mean of this sampling distribution is \\(\\mu_{\\overline{x}}=7.8\\) which is the same as the population mean \\(\\mu\\). The standard deviation of this sampling distribution \\(\\sigma{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). 0.3 / sqrt(15) ## [1] 0.07745967 Therefore the standard deviation of this sampling distribution is \\(\\sigma{\\overline{x}} = 0.077\\). Remember, this sampling distribution represents thousands and thousands of potential means from individual samples of size 15 that we could have collected. Each one of them in isolation would be our point estimate of the true population mean \\(\\mu\\). Sometimes we'll be really close to the true population mean, and other times we might bea quite far away. This is why we like to put confidence intervals around our sample means, to give a range of values that likely contain our population mean. One thing we can do first is to think about this - between which two values on the sampling distribution shown above would 95% of the data lie? That is the same as asking, which two values represent the part where 2.5% of the distribution is in each tail (leaving 95% in the middle). To answer this, we just need to remember that according to Central Limit Theorem that our sampling distribution is normally distributed. Therefore we can use the standard normal curve. According to the standard normal distribution, the values of z that leave 2.5% in each tail are \\(z=-1.96\\) and \\(z=1.96\\). That means values that 95% of the distribution lie between 1.96 standard deviations below and above the mean. If you didn't want to take our word for it that +1.96 and -1.96 are the values of z that leave 2.5% in each tail, you could also directly calculate it in R: qnorm(c(0.025, 0.975)) # get the values of z that are the boundaries of 2.5% to the left, and 97.5% to the left. ## [1] -1.959964 1.959964 So if we go back to thinking about our sampling distribution - because we say it is approximately normally distributed, 95% of the distribution will also lie between 1.96 standard deviations below and above the mean. We know that the mean of the sampling distribution is \\(\\mu_{\\overline{x}=7.8}\\) and the standard deviation of the sampling distribution is \\(\\sigma{\\overline{x}} = 0.077\\), as we calculated it above. Therefore, we can use this to calculate which sample mean values in the distribution are 1.96 standard deviations either side of the mean. They are: 7.8 + (1.96 * 0.077) ## [1] 7.95092 7.8 - (1.96 * 0.077) ## [1] 7.64908 So, 95% of our sample means in our sampling distribution lie between 7.65 and 7.95. That area is represented by the shaded red area on our sampling distribution below: What we have just done is the basic principle behind a confidence interval using a \\(z\\)-distribution. Let's look at this in more detail. 8.2 Calculating a confidence interval with z-distribution Let's go back to our first sample of size 15 that we collected, with \\(\\overline{x}=7.72\\). samp1 ## [1] 7.532948 8.233244 8.014278 7.446299 8.294182 7.660900 8.045752 7.251308 ## [9] 7.267707 7.742027 8.243312 8.039407 8.367953 8.022449 7.930565 mean(samp1) ## [1] 7.872822 What we want to do now is put a confidence interval around 7.72. We want to say that our population mean is equal to \\(7.72 \\pm margin.of.error\\) The actual formula for the \\(z\\)-distribution confidence interval is: \\(CI_{95\\%} = \\overline{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\) In this scenario, we are presuming that we don't know what the population mean \\(\\mu\\) is - that's why we're building a confidence interval. Consequently, we also don't precisely know what the mean of the sampling distribution \\(\\mu_{\\overline{x}}\\) is. What we'll do instead, is to assume that our sample mean \\(\\overline{x}\\) is the mean of the sampling distribution \\(\\mu_{\\overline{x}}\\). We already know what the standard deviation of the sampling distribution \\(\\sigma{\\overline{x}}\\) is because we know the population standard deviation \\(\\sigma\\) is. So, \\(\\sigma{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). What value of \\(z\\) should we use? The short answer is 1.96 for the same reasons as above. If our sampling distribution is normally distributed, then we want to know the values that are \\(\\pm 1.96\\) of the sample mean \\(\\overline{x}\\). So, let's just do it - this is how we calculate the 95% confidence interval if we have \\(\\overline{x}\\), \\(\\sigma\\) and \\(n\\). x_bar &lt;- mean(samp1) # sample mean = 7.72 x_bar ## [1] 7.872822 n &lt;- length(samp1) # sample size = n = 15 n ## [1] 15 sigma &lt;- 0.3 # the pop SD given in the example sem &lt;- sigma/sqrt(n) # standard error of the mean (SD of sampling distribution) sem ## [1] 0.07745967 z &lt;- 1.96 # the value of &#39;z&#39; we need to get the middle 95% of the distribution # margin of error z * sem ## [1] 0.1518209 # upper bound of confidence interval x_bar + (z * sem) ## [1] 8.024643 # lower bound of confidence interval x_bar - (z * sem) ## [1] 7.721001 We have just calculated our 95% confidence interval! It has a lower bound of 7.568cm and an upper bound of 7.871cm. We can write this confidence interval in two ways: \\(CI_{95\\%} = 7.72 \\pm 0.152\\) \\(CI_{95\\%} = 7.72 [7.568, 7.871]\\) Below is a graphical representation of our confidence interval around our sample mean \\(\\overline{x}\\). You can see that the true population mean \\(\\mu\\) is within the confidence interval. Remember we collected a second sample that had a sample mean \\(\\overline{x}=7.95\\) ? mean(samp2) ## [1] 7.771178 We could also create a 95% confidence interval for our estimate of the population mean \\(\\mu\\) using this sample mean \\(\\overline{x}\\). We just use the same formula: # upper bound of confidence interval 7.95 + (z * sem) ## [1] 8.101821 # lower bound of confidence interval 7.95 - (z * sem) ## [1] 7.798179 \\(CI_{95\\%} = 7.95 \\pm 0.152\\) \\(CI_{95\\%} = 7.95 [7.798, 8.102]\\) Let's compare this confidence interval with the first one we created: Note that both include the true population mean of 7.8 in their confidence interval. In the second sample mean - it's only just inside, but it's inside! What if we collected 20 new samples, and calculated 20 sample means, and made 20 confidence intervals? Well, the chart below shows 20 95% confidence intervals collected from samples of size 15 selected at random from our population of butterflies: First, notice that the margin of error is equal for all of our confidence intervals around the sample means. This is because we are using the same value of \\(\\sigma\\) and same value of \\(z\\) for all of these confidence intervals. Secondly, you'll notice that not all the confidence intervals include the population mean \\(\\mu\\). Two of them - highlighted in green - do not include the population mean. In this sense, our sample mean and associated confidence interval is not doing a terrific job of estimating the population mean. Actually, it turns out that if you collect enough samples and generate enough sample means, then you will capture the population mean within your confidence interval 95% of the time. So roughly 5 out of every 100 confidence intervals you make from samples will not include the population mean. Technically, this is the definition of a 95% confidence interval. That is, in 95% of your samples you will include the true population mean. However, when talking about confidence intervals in lay-speak, when we have our one confidence interval around our one sample mean e.g. \\(CI_{95\\%} = 7.72 [7.568, 7.871]\\), we often say &quot;there's a 95% chance that the true population mean is between 7.568 and 7.871. This is technically lazy shorthand although it does kind of help us understand the point of a confidence interval. But, please remember, the real definition is that in 95% of samples we'll include the true population mean in our samples. Assumptions We should also briefly just remark on what the assumptions are when generating these \\(z\\)-distribution based confidence intervals. We are assuming that our data are normally distributed and that our sample is randomly drawn from the population, and that all data points are independent of each other. 8.2.1 Other Confidence Intervals ranges We can actually construct confidence intervals for any % value. Most commonly people make 95% confidence intervals, but other common ones include 80%, 90% and 99% confidence intervals. These have the same interepretation as the 95% CI. For instance, a 99% confidence interval means that if you were to take 100 samples from a population and calculate 99% confidence intervals for each, only 1 out of a 100 on average would not include the true population mean \\(\\mu\\). The formulas for each of these confidence intervals when using the \\(z\\)-distribution are as follows: \\(CI_{80\\%} = \\overline{x} \\pm 1.28 \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(CI_{90\\%} = \\overline{x} \\pm 1.64 \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(CI_{95\\%} = \\overline{x} \\pm 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(CI_{99\\%} = \\overline{x} \\pm 2.58 \\times \\frac{\\sigma}{\\sqrt{n}}\\) Where did each of these different numbers come from for \\(z\\) ? Well, if we wish to make a 99% CI, we need to know what values of \\(z\\) are the boundaries that leave 99% of the distribution inside them on the standard normal curve. We exclude 0.5% in each tail. Likewise, for the 80%CI, we want to know the values of \\(z\\) that leave 5% in each tail and 10% in the middle. We can calculate these values in R like this: qnorm(.9)# for 80% CI ## [1] 1.281552 qnorm(.95)# for 90% CI ## [1] 1.644854 qnorm(.975)# for 95% CI ## [1] 1.959964 qnorm(.995)# for 99% CI ## [1] 2.575829 So, if we were to calculate the 80% confidence interval for our first sample, we would do: #margin of error 1.281552 * sem ## [1] 0.09926859 # upper bound of confidence interval x_bar + (1.281552 * sem) ## [1] 7.972091 # lower bound of confidence interval x_bar - (1.281552 * sem) ## [1] 7.773553 Our 80% confidence interval is: \\(CI_{80\\%} = 7.72 \\pm 0.099\\) \\(CI_{80\\%} = 7.72 [7.620, 7.819]\\) Looking at each of these confidence intervals for our first sample mean, we get the following. Clearly, confidence intervals widen with higher percentages: Let's look at this same figure, but this time for our second sample that has an sample mean \\(\\overline{x}=7.95\\): As you can see here, this time the 90% and 80% CIs do not inclue the true population mean \\(\\mu\\). We increase the chances of including the true population mean \\(\\mu\\) inside our confidence interval by increasing the level of our confidence interval. A 99% confidence interval has an increased probability of including the confidence interval compared to a 95% confidence interval and so on. 8.2.2 Confidence Intervals and Sample Size The other variable inside the confidence interval formula that we should think about is the sample size \\(n\\). Let's look at the formula again: \\(CI = \\overline{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\) What happens when we get different sized sample sizes? For instance, look at the 95% confidence intervals below that all have a sample mean of \\(\\overline{x}=7.72\\) but are for different sample sizes: There are two things to note. Firstly, as your sample size increases, for any confidence level (in this situation a 95% CI) the confidence interval is going to shrink. It gets tighter for larger sample sizes. Increasing sample sizes increases certainty. This is because the denominator of the confidence interval formula \\(\\sqrt{n}\\) gets larger, meaning that the margin of error gets smaller. The second thing might seem counterintuitive. Why does it look like in the graph above that a sample size of \\(n=50\\) is only just able to have \\(\\mu\\) contained within it? It would seem that a larger sample size should do a better job of including \\(\\mu\\). Well, remember, that a 95% CI really means that 95% of your sample means will contain \\(\\mu\\).... so across all these sample sizes you have a 95% chance of having captured \\(\\mu\\) inside your CI. The key thing to remember is that with a bigger sample size you are much more likely to get a sample mean \\(\\overline{x}\\) that is close to the population mean \\(\\mu\\). That's because the sampling distribution of sample means is much tighter. In some ways the figure above is a bit misleading as all the sample means are at 7.72. What is more likely to be the case for many samples is that they will be closer to the true population mean. Look at the figure below, that compares samples sizes of 10 with sample sizes of 50. Larger samples lead to CIs that have a sample mean closer to \\(\\mu\\) and that are tighter - but still with a 95% chance of having captured \\(\\mu\\). 8.3 Confidence Intervals with t-distribution Hopefully the preceding sections on creating a confidence interval with the \\(z\\)-distribution helped you in understanding some of the theory about confidence intervals in general. Perhaps there is still one thought going through your mind - isn't all of this a bit strange? Why are we trying to estimate the population mean \\(\\mu\\) from the sample mean \\(\\overline{x}\\) when we also already know the population standard deviation \\(\\sigma\\)? How could you know \\(\\sigma\\) but not know \\(\\mu\\) - that makes no sense, and indeed it doesn't. It turns out that in the real world, that when we collect a sample of data and get our sample mean \\(\\overline{x}\\) and we want to create our confidence interval around it to have some certainty about where the population mean \\(\\mu\\) might lie, we also do not know \\(\\sigma\\). We need a backup plan for how to construct confidence intervals. This back up plan is making confidence intervals with the \\(t\\)-distribution. First, let's look at the formula for making a confidence interval with a \\(t\\)-distribution: \\(CI = \\overline{x} \\pm t \\times \\frac{s}{\\sqrt{n}}\\) Two things are different about this one compared to the formula for calculating a CI with the \\(z\\)-distribution. First, we are using a \\(t\\) value rather than a \\(z\\) value. Secondly, we are using the sample standard deviation \\(s\\) rather than the population standard deviation \\(\\sigma\\). If we do not know \\(\\sigma\\) then our next best option is to use our estimate of the population standard deviation, which is our sample standard deviation \\(s\\). We briefly introduced the \\(t\\)-distribution in section xxx.xxx. Why do we need to use it here? Essentially, the key thing is that when we collect many sample means to create our sampling distribution of sample means, it is not always the case that this samplign distribution will be perfectly normally distributed. In fact, this is especially true for smaller sample sizes. If we collect smaller samples and calculate the sample mean of each, it turns out our sampling distribution will be slighly heavier in the tails than a normal distribution. How far away from normal our sampling distribution will be depends on our sample size. For bigger sample sizes, our sampling distribution will look more normal. This is illustrated below: It's important to remember that the shape of the \\(t\\)-distribution varies for different sample sizes. In fact, we actually state the distribution not in terms of the sample size, but in terms of the degrees of freedom. For instance, for a sample size of 15, we would say that the sampling distribution follows a \\(t\\)-distribution with a shape of degrees of freedom 14. The degrees of freedom is equal to \\(n-1\\) when describing sampling distributions of sample means. As a result of this issue, if we were to assume that our sampling distribution was normally distributed and used \\(z=1.96\\) to calculate our 95% confidence interval, we would be inaccurately determining where the middle 95% of the distribution was. In fact, for a \\(t\\)-distribution, because the tails are heavier, the value of \\(t\\) that leaves 2.5% in each tail will be a larger value than 1.96. Compare the standard normal curve below to the \\(t\\)-distribution for \\(df=14\\). fig here. As we can see, the value of \\(t\\) that leaves 2.5% in each tail is 2.145 which is higher than 1.96. Consequently, all else being equal, this will increase our margin of error. 8.4 Calculating a t-distribution Confidence Interval Let's look more practically at how we calculate a 95% confidence interval for a \\(t\\)-distribtuion. We'll use sample 1 from above that had a mean of \\(\\overline{x}=7.72\\). The formula we use is: \\(CI95\\% = \\overline{x} \\pm t \\times \\frac{s}{\\sqrt{n}}\\) First, we should calculate the sample standard deviation, which is \\(s=0.285\\): samp1 ## [1] 7.532948 8.233244 8.014278 7.446299 8.294182 7.660900 8.045752 7.251308 ## [9] 7.267707 7.742027 8.243312 8.039407 8.367953 8.022449 7.930565 sd(samp1) ## [1] 0.3688933 Next, we need to calculate \\(t\\). This value will be the value that leaves 2.5% in the tails of a \\(t\\)-distribution for degrees of freedom = 14 (\\(n-1 = 14\\)). We can calculate that in R using the function qt(). We enter 0.975 to ask it to return the value of \\(t\\) that leaves 2.5% in the upper tail, and then we enter df=14 to ensure we are using the correct \\(t\\)-distribution: qt(p = 0.975, df = 14) ## [1] 2.144787 This shows us that our value of \\(t=2.145\\). We can now create our estimate of the standard error (the standard deviation of the sampling distribution of sample means), and our confidence intervals: #standard error sem1 &lt;- sd(samp1)/sqrt(15) sem1 ## [1] 0.09524785 # upper bound of confidence interval 7.72 + (2.144787 * sem1) ## [1] 7.924286 # lower bound of confidence interval 7.72 - (2.144787 * sem1) ## [1] 7.515714 Our confidence interval is therefore: \\(CI95\\% = 7.72[7.56,7.88]\\) Likewise, for sample 2, which had \\(\\overline{x}=7.95\\) our 95% confidence interval would be: #standard error sem2 &lt;- sd(samp2)/sqrt(15) sem2 ## [1] 0.1013502 # upper bound of confidence interval 7.95 + (2.144787 * sem2) ## [1] 8.167375 # lower bound of confidence interval 7.95 - (2.144787 * sem2) ## [1] 7.732625 So, our confidence interval for this sample is therefore: \\(CI95\\% = 7.95[7.796,8.104]\\) We can graphically compare this to the 95% confidence interval we calculated using the \\(z\\)-distribution. We've plotted the \\(t\\) confidence intervals in purple and the \\(z\\) confidence intervals in blue. The bottom two confidence intervals are those for sample 1, and the top two are those for sample 2. Notice that the confidence intervals based on the \\(t\\)-distribution are a little wider than those based on the \\(z\\)-distribution. This is because we are using a higher value of \\(t\\) than of \\(z\\) in the equation. This is because we are assuming our sampling distribution is following the \\(t\\) shape rather than the classic \\(z\\) shape, and as the tails are heavier in a \\(t\\)-distribution, the value of \\(t\\) that leaves 2.5% in the tail is further away from 0. There is also one other detail that is a little hard to see from only two samples above. That is that the size of the confidence intervals constructed using the \\(z\\)-distribution are fixed - i.e. they are always the same size. This is because the value of \\(z\\) is fixed (1.96 in this case) and the value of \\(\\sigma\\) is fixed - it's always the same population standard deviation, which doesn't change. However, for confidence intervals made using the \\(t\\)-distribution, the size of these may change from sample to sample. This is because the sample standard deviation changes from sample to sample, meaning that not all confidence intervals will be the same length. We can illustrate this below. Here are 20 95% confidence intervals made using either the \\(z-\\) or \\(t\\)-distribution for 20 different samples of sample size \\(n=15\\). You can see that the \\(z\\)-distribution CIs are all equal in length, whereas the \\(t\\)-distribution ones vary from sample to sample. This is because of the use of the sample standard deviation \\(s\\) in the formula. Most of the time, because of the higher \\(t\\) value in the formula than the \\(z\\) value, it leads to the CIs being wider for those calculated with the \\(t\\)-distribution. This sometimes has important implications. For intance, notice the 9th sample down from the top. Using the \\(z\\)-distribution, this CI does not capture the true population mean \\(\\mu\\), but using the \\(t\\)-distribution does capture it. However, the CIs are not always bigger when using the \\(t\\)-distribution. Sometimes, the sample may just have very little variation in it meaning that the sample standard deviation \\(s\\) is very small. This could lead to a smaller margin of error - as seen with the 19th and 20th samples from the top in the figure. 8.4.1 t-distribution CIs and sample size. With the \\(z\\)-distribution based confidence intervals, when we increased the sample size \\(n\\), the margin of error always decreased because both \\(z\\) and \\(\\sigma\\) are fixed in the formula. For instance, for a 95% CI with a population standard deviation \\(\\sigma=10\\) and sample size \\(n=10\\) or \\(n=30\\), the margin of error using the \\(z\\)-distribution in the CI would be for each sample size: 1.96 * (10/sqrt(10)) ## [1] 6.198064 1.96 * (10/sqrt(30)) ## [1] 3.578454 Clearly, increaseing the sample size reduces the margin of error. The situation is not as consistent when constructing confidence intervals with the \\(t\\)-distribution, although the general pattern remains true. When we collect samples of different sample sizes, two things change in the \\(t\\)-distribution confidence interval formula. Firstly, the value of \\(t\\) used is dependent upon the degrees of freedom. As sample sizes increase, the \\(t\\)- distribution becomes more normal shaped and less heavy in the tails. If, for example, we are interested in making 95% Confidence Intervals, then the value of \\(t\\) that leaves 2.5% in each tail (and 95% of the distribution in the middle) is going to get closer to 1.96 (and negative -1.96) as the sample size increases. This is illustrated in the figure below: Each of these \\(t\\) values can be calculated, by finding the value of \\(t\\) on the \\(t\\)-distribution for the respective degrees of freedom that leaves 2.5% in the upper tail: qt(.975, df = 9) ## [1] 2.262157 qt(.975, df = 19) ## [1] 2.093024 qt(.975, df = 29) ## [1] 2.04523 So, as sample size increases, the value of \\(t\\) decreases for a given confidence interval. This would seem to suggest that this would decrease the margin of error for the confidence interval. This is for the most part true, but not always. Remember the \\(t\\) value is multiplied by the estimated standard deviation of the sampling distribution (the standard error) which is \\(\\frac{s}{\\sqrt{n}}\\). Now again, it looks like increasing \\(n\\) would lead to a larger denominator and a smaller overall margin of error. This is also true. But, because we are using the sample standard deviation \\(s\\) in the formula to estimate the standard error, then \\(s\\) is going to vary from one sample to another. This means that for any given sample, we may actually end up with a tighter confidence interval even if we increase our sample size. However, the main point remains - generally increasing your sample size, will lead to a tighter confidence interval for a given CI range. The final thing that is worth mentioning is a repeat of what is discussed above in the \\(z\\)-distribution section. Increasing sample sizes also leads to sample means that will be, on average, much closer to the true population mean \\(\\mu\\) than you get when using smaller sample sizes. 8.4.2 Other Confidence Intervals ranges for t-distribution Like with the confidence intervals made with the \\(z\\)-distribution, we can create confidence intervals for any range with the \\(t\\)-distribution. The rationiale is the same. If we were to make an 80% CI around a sample mean, what we are effectively saying is that in 80% of all samples that we could collect, we would capture the true population mean \\(\\mu\\). Practically, we use a different value of \\(t\\) for each CI range. This value of \\(t\\) will be the positive and negative value of the \\(t\\)-distribution for a given degree of freedom that leaves the appropriate percentage in the middle of the distribution. For instance, for an 80% CI for a sample size of 25, which had degrees of freedom 24, the value would be \\(t=1.32\\). We calculated this as follows: For an 80% CI, we wish to have 80% of the distribution in the middle (40% either side of our sample mean), leaving 20% in the tails - i.e. 10% in each tail. Therefore, we wish to know the value of \\(t\\) that demarks this boundary. The easiest way to do that is to use the qt() function in R, and ask for the 90%th percentile (the value that leaves 10% in the upper tail) for a \\(t\\) distribution of degrees of freedom = 24. We do that like this: qt(0.90, df=24) ## [1] 1.317836 Thus, if we had a sample mean of \\(\\overline{x}=15.52\\), a sample standard deviation of \\(s=3.3\\) and a sample size of \\(n=25\\), then our 80% confidence interval of the true population mean \\(\\mu\\) would be \\(CI = 15.52[14.65, 16.39]\\): 15.52 + (1.32 * (3.3 / sqrt(25))) ## [1] 16.3912 15.52 - (1.32 * (3.3 / sqrt(25))) ## [1] 14.6488 The value of \\(t\\) used in the confidence interval formula therefore changes based on both your confidence interval size, and your degrees of freedom. Below are some other values of \\(t\\) that would be used for different sample sizes and CI ranges: # 99% CI, n = 20 qt(.995, df = 19) ## [1] 2.860935 # 90% CI, n = 12 qt(.95, df = 11) ## [1] 1.795885 # 99.9% CI, n = 35 qt(.9995, df = 34) ## [1] 3.600716 8.5 Comparing CIs using the z- and t-distributions You might be thinking that using the \\(t\\)-distribution to make 95% confidence intervals seems like a lot of extra legwork to figure out what value of \\(t\\) to use, compared to just using \\(z=1.96\\) when using the \\(z\\)-distribution. This mini section hopefully is an illustration of why you have to do this. These are the two formulas that we use to generate confidence intervals: \\(CI_{95\\%} = \\overline{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\) \\(CI_{95\\%} = \\overline{x} \\pm t \\times \\frac{s}{\\sqrt{n}}\\) But, what if we did just decide to use \\(z\\) when we don't know the population standard deviation \\(\\sigma\\) and we used this formula: \\(CI_{95\\%} = \\overline{x} \\pm z \\times \\frac{s}{\\sqrt{n}}\\) Well, in the figure below, we did just that for 25 sample means collected from samples of size \\(n=8\\) from a population with a mean of \\(\\mu=4\\) and standard deviation of \\(\\sigma=1.5\\). As you can see, the first and third columns that are using the appropriate \\(z\\)- and \\(t\\)-distribution formulas have 23/25 confidence interals that include the true population mean. In fact, out of 1000 simulations of these data (i.e. 1000 sample means collected) precisely 95% of confidence intervals included the population mean for both, which is what we would expect. Conversely, the middle column include the confidence intervals calculated using \\(z=1.96\\) and using \\(s\\) as an estimate of the population standard deviation. With this formula, 6/25 confidence intervals fail to include the true population mean. Out of the 1000 simulations of the data, actually 9.X% of CIs failed to capture the true population mean. This shows that the margin of error calculated using this formula is consistently too small. The reason for this is that when we estimate the population standard deviation, we generally are under-estimating the true value. This is another reason why we should be using the \\(t\\)-distribution. "],
["one-sample-inferential-statistics.html", "9 One Sample Inferential Statistics 9.1 One-sample Z-tests 9.2 One-sample t-tests", " 9 One Sample Inferential Statistics The general question at hand with one-sample inferential tests, is that we wish to test the probability that our one sample of data comes from a population that has a true population mean \\(\\mu\\) that is equal to some specific value. z or t 9.1 One-sample Z-tests 9.2 One-sample t-tests "],
["two-sample-inferential-statistics.html", "10 Two Sample Inferential Statistics 10.1 Comparing two Samples 10.2 Independent Samples t-test 10.3 Background to Student's 2 Sample t-test 10.4 Sampling Distribution of the Difference in Sample Means 10.5 Pooled Standard Deviation 10.6 Confidence Interval for Difference in Means 10.7 Conducting Student t-test 10.8 Doing Student t-test in R 10.9 Effect Sizes 10.10 Paired t-tests 10.11 Confidence Intervals with Paired Data", " 10 Two Sample Inferential Statistics blah 10.1 Comparing two Samples blah blah compare 2 distributions blah 10.2 Independent Samples t-test types 10.3 Background to Student's 2 Sample t-test blah 10.4 Sampling Distribution of the Difference in Sample Means blah 10.5 Pooled Standard Deviation blah 10.6 Confidence Interval for Difference in Means blah 10.7 Conducting Student t-test blah 10.8 Doing Student t-test in R blah 10.9 Effect Sizes Just because you observe a “significant” difference in means between two groups doesn’t mean that it’s interesting or relevant…. i.e. being ‘significantly different’ doesn’t tell you how BIG the difference is – i.e. how LARGE the effect size is. The formula for Cohen's \\(\\delta\\) is as follows: \\(\\Huge \\delta = \\frac{\\overline{X}_{1} - \\overline{X}_{2}}{\\hat{\\sigma}_{\\rho}}\\) 10.10 Paired t-tests more sections to add... 10.11 Confidence Intervals with Paired Data "],
["correlation.html", "11 Correlation 11.1 Pearson Correlation 11.2 Calculating the Pearson Correlation in R 11.3 Cross-products 11.4 Conducting a Pearson Correlation Test 11.5 Assumptions of Pearson's Correlation 11.6 Confidence Intervals for R 11.7 Partial Correlations 11.8 Non-parametric Correlations 11.9 Point-Biserial Correlation", " 11 Correlation intro lines about what correlation is etc... 11.1 Pearson Correlation Pearson's correlation is measured by r and ranges between -1 and +1. +1 indicates that the variables X and Y are maximally positively correlated, such that as values of X increase so do values of Y. -1 indicates a compleltely negative correlation such that as values of X increase, values of Y decrease. A value of 0 indicates that there is no overall relationship. insert figure of negative 0.6, 0 and positive 0.6 here The below image shows scatterplots, each with a sample size of 30. The trendline is to help demonstrate how correlations of different magnitudes look in terms of their association. Correlations 11.2 Calculating the Pearson Correlation in R To calculate the correlation coefficient in R, it's pretty straightforward. You simply can use the cor() function. For instance, let's correlate... cor(1:10,1:10) # replace with an example ## [1] 1 Before we go further into what we should do with these correlations, and how to signficance test them, let's learn a little bit more about how they come about. 11.3 Cross-products The formula for calculating the Pearson's correlation coefficient for a sample is: \\(r = \\frac{\\sum_{}^{} z_{x}z_{y}}{n - 1}\\) When we have a population, we can use the formula: \\(r = \\frac{\\sum_{}^{} z_{x}z_{y}}{N}\\) Essentially, the steps are to convert all the X and Y scores into their respective z-scores. Then you mutliply these two values together to get the cross-product. After summing up all the cross-products for each data point, we divide this number by n-1 if we're dealing with a sample (we usually are), or N if we're dealing with a population. The sum of the cross-products will therefore be largely positive if positive z-scores are multiple together or if negative z-scores are multiplied together. The sum of the cross-products will be largely negative if negative z-scores are multipled with positive z-scores. The following example should help make this clearer. Look at the following data, its scatterplot and the correlation coefficient. They show that we have a positive correlation of r=0.84. Let's break it down how we got that value. x &lt;- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9) y &lt;- c(2.8, 2.9, 1.6, 5.5, 4.7, 8.1, 3.3, 7.7, 7.1, 5.8) df &lt;- data.frame(x, y) df ## x y ## 1 1.1 2.8 ## 2 1.5 2.9 ## 3 2.1 1.6 ## 4 3.5 5.5 ## 5 3.6 4.7 ## 6 3.5 8.1 ## 7 2.6 3.3 ## 8 5.6 7.7 ## 9 4.4 7.1 ## 10 3.9 5.8 ggplot(df, aes(x = x, y = y)) + geom_point(size=2) cor(x,y) ## [1] 0.8418262 First, let's calculate the means and standard deviation (using sd so a sample standard deviation) of x and y. We need to get these values so we can calculate the z-scores of each. # step 1: Get the mean and sd of x and y mean(x) ## [1] 3.18 sd(x) ## [1] 1.370158 mean(y) ## [1] 4.95 sd(y) ## [1] 2.259916 Now, we can calculate the z-scores, remembering that the formula for that is: \\(z = \\frac{x - \\overline{x}}{s_{x}}\\) # step 2. Calculate z-scores of x, and z-scores of y. df$zx &lt;- (x - mean(x)) / sd(x) # z scores of x df$zy &lt;- (y - mean(y)) / sd(y) # z scores of y df ## x y zx zy ## 1 1.1 2.8 -1.5180729 -0.9513626 ## 2 1.5 2.9 -1.2261358 -0.9071132 ## 3 2.1 1.6 -0.7882302 -1.4823557 ## 4 3.5 5.5 0.2335497 0.2433718 ## 5 3.6 4.7 0.3065340 -0.1106236 ## 6 3.5 8.1 0.2335497 1.3938569 ## 7 2.6 3.3 -0.4233088 -0.7301155 ## 8 5.6 7.7 1.7662195 1.2168592 ## 9 4.4 7.1 0.8904082 0.9513626 ## 10 3.9 5.8 0.5254868 0.3761201 Following this, we simply multiple the z-scores of x and y against each other for every data point: # step 3. Calculate the cross-product: zx * zy df$zxzy &lt;- df$zx * df$zy df ## x y zx zy zxzy ## 1 1.1 2.8 -1.5180729 -0.9513626 1.44423785 ## 2 1.5 2.9 -1.2261358 -0.9071132 1.11224399 ## 3 2.1 1.6 -0.7882302 -1.4823557 1.16843751 ## 4 3.5 5.5 0.2335497 0.2433718 0.05683941 ## 5 3.6 4.7 0.3065340 -0.1106236 -0.03390988 ## 6 3.5 8.1 0.2335497 1.3938569 0.32553483 ## 7 2.6 3.3 -0.4233088 -0.7301155 0.30906432 ## 8 5.6 7.7 1.7662195 1.2168592 2.14924036 ## 9 4.4 7.1 0.8904082 0.9513626 0.84710104 ## 10 3.9 5.8 0.5254868 0.3761201 0.19764615 We now have all of our cross-products. Notice why the majority are positive. This is because we have multiplied positive \\(z_{x}\\) with positive \\(z_{y}\\) or we multiplied negative \\(z_{x}\\) with negative \\(z_{y}\\). This happens because datapoints that tend to be above the mean for x are also above the mean for y, and points that are below the mean of x are also below the mean of y. We can add this up to get the sum of the cross-products. That is the \\(\\sum_{}^{} z_{x}z_{y}\\) in the formula. # step 4. Sum up the cross products. sum(df$zxzy) # 7.58 ## [1] 7.576436 We now divide that by n-1 as we have a sample, to get the correlation coefficient r. That gives us an estimation of the average cross-product. # step 5- calculate &#39;r&#39; by dividing by n-1. (for a sample) sum(df$zxzy) / 9 # our n was 10, so n-1 = 9 ## [1] 0.8418262 sum(df$zxzy) / (nrow(df) - 1) # nrow(df) is more generalizable ## [1] 0.8418262 # r=0.84 Just as a quick second example, here is a work through calculating a negative correlation. Notice the \\(z_{x}\\) and \\(z_{y}\\) scores that are multiplied together. They are largely opposite in terms of signs. This is what leads to a negative sum of cross-products and the negative correlation. Why? Because data points that are above the mean for x are generally below the mean in terms of y and visa-versa. ### Example 2. Negative Correlation. x &lt;- c(1.1, 1.5, 2.1, 3.5, 3.6, 3.5, 2.6, 5.6, 4.4, 3.9) y &lt;- c(10.4, 10.0, 8.4, 8.5, 8.4, 6.3, 7.1, 6.2, 8.1, 10.0) df &lt;- data.frame(x, y) ggplot(df, aes(x = x, y = y)) + geom_point(size=2) cor(df$x,df$y) ## [1] -0.6112965 Here is the code, truncated for space: # Calculate z-scores for each x and each y df$zx &lt;- (x - mean(x)) / sd(x) df$zy &lt;- (y - mean(y)) / sd(y) # Calculate the cross-product: zx * zy df$zxzy &lt;- df$zx * df$zy # let&#39;s look at the dataframe # notice the cross products: df ## x y zx zy zxzy ## 1 1.1 10.4 -1.5180729 1.37762597 -2.09133671 ## 2 1.5 10.0 -1.2261358 1.11012578 -1.36116500 ## 3 2.1 8.4 -0.7882302 0.04012503 -0.03162776 ## 4 3.5 8.5 0.2335497 0.10700008 0.02498983 ## 5 3.6 8.4 0.3065340 0.04012503 0.01229968 ## 6 3.5 6.3 0.2335497 -1.36425096 -0.31862038 ## 7 2.6 7.1 -0.4233088 -0.82925058 0.35102907 ## 8 5.6 6.2 1.7662195 -1.43112601 -2.52768263 ## 9 4.4 8.1 0.8904082 -0.16050011 -0.14291061 ## 10 3.9 10.0 0.5254868 1.11012578 0.58335643 # Sum up the cross products and Calculate &#39;r&#39; by dividing by N-1. sum(df$zxzy) / (nrow(df) - 1) ## [1] -0.6112965 cor(df$x,df$y) ## [1] -0.6112965 11.4 Conducting a Pearson Correlation Test Although cor() gives you the correlation between two continuous variables, to actually run a significance test, you need to use cor.test(). Let's use some BlueJay data to do this. We'll just use data on male birds. library(tidyverse) jays &lt;- read_csv(&quot;data/BlueJays.csv&quot;) ## Parsed with column specification: ## cols( ## BirdID = col_character(), ## KnownSex = col_character(), ## BillDepth = col_double(), ## BillWidth = col_double(), ## BillLength = col_double(), ## Head = col_double(), ## Mass = col_double(), ## Skull = col_double(), ## Sex = col_integer() ## ) jayM &lt;- jays %&gt;% filter(KnownSex == &quot;M&quot;) # we&#39;ll just look at Males nrow(jayM) # 63 observations ## [1] 63 head(jayM) ## # A tibble: 6 x 9 ## BirdID KnownSex BillDepth BillWidth BillLength Head Mass Skull Sex ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0000-00000 M 8.26 9.21 25.9 56.6 73.3 30.7 1 ## 2 1142-05901 M 8.54 8.76 25.0 56.4 75.1 31.4 1 ## 3 1142-05905 M 8.39 8.78 26.1 57.3 70.2 31.2 1 ## 4 1142-05909 M 8.71 9.84 25.5 57.3 74.9 31.8 1 ## 5 1142-05912 M 8.74 9.28 25.4 57.1 75.1 31.8 1 ## 6 1142-05914 M 8.72 9.94 30 60.7 78.1 30.7 1 Let's say you're interested in examining whether there is an association between Body Mass and Head Size. First we'll make a scatterplot between the Mass and Head columns. We'll also investigate the correlation using cor(). ggplot(jayM, aes(x=Mass, y=Head)) + geom_point(shape = 21, colour = &quot;navy&quot;, fill = &quot;dodgerblue&quot;) + stat_smooth(method=&quot;lm&quot;, se=F) cor(jayM$Mass, jayM$Head) # r = 0.58, a strong positive correlation. ## [1] 0.5773562 To run the significance test, we do the following: cor.test(jayM$Head, jayM$Mass) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 7.282e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3846090 0.7218601 ## sample estimates: ## cor ## 0.5773562 This gives us a lot of information. Firstly, at the bottom it repeats the correlation coefficient cor. At the top, it gives us the value of t which is essentially how surprising it is for us to get the correlation we did assuming we were drawing our sample from a population where there is no correlation. Associated with this t value is the degrees of freedom which is equal to n-2, so in this case that is 63-2 = 61. The p-value is also given. If we are using alpha=0.05 as our significance level, then we can reject the hypothesis that there is no overall correlation in the population between Body Mass and Head size if p&lt;0.05. The default for cor.test() is to do a two-tailed test. This is testing whether your observed correlation r is different from r=0 in either the positive or negative direction. This default version also gives us the confidence interval for the correlation coefficient. Essentially, this gives us the interval in which we have a 95% confidence that the true population r lies (remember we just have data from one sample that theoretically comes from a population). It's also possible however that you had an a priori prediction about the direction of the effect. For instance, you may have predicted that Body Mass would be positively correlated with Head Size. In this case, you could do a one-tailed correlation test, where your alternative hypothesis is that there is a positive correlation and the null is that the correlation coefficient is equal to 0 or less than 0. To do one-tailed tests you need to add the alternative argument. # testing if there is a positive correlation cor.test(jayM$Head, jayM$Mass, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 3.641e-07 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.4187194 1.0000000 ## sample estimates: ## cor ## 0.5773562 # testing if there is a negative correlation cor.test(jayM$Head, jayM$Mass, alternative = &quot;less&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: jayM$Head and jayM$Mass ## t = 5.5228, df = 61, p-value = 1 ## alternative hypothesis: true correlation is less than 0 ## 95 percent confidence interval: ## -1.0000000 0.7017994 ## sample estimates: ## cor ## 0.5773562 11.5 Assumptions of Pearson's Correlation The Pearson Correlation Coefficient requires your data to be approximately normally distributed. To do this we have various options how to test for normality. Firstly, we could do a Shapiro-Wilk test, which formally determines whether our data are normal. This is done using shapiro.test(), where we assume our data are from a normal population if the resulting p-value is above 0.05. If the p-value is below 0.05 then we have evidence to reject that our data come from a normal population. With our data above, this would look like this when running the test on each variable: shapiro.test(jayM$Mass) # P &gt; 0.05, therefore cannot reject null that data is not normal ## ## Shapiro-Wilk normality test ## ## data: jayM$Mass ## W = 0.97222, p-value = 0.1647 shapiro.test(jayM$Head) # P &gt; 0.05, therefore cannot reject null that data is not normal ## ## Shapiro-Wilk normality test ## ## data: jayM$Head ## W = 0.96521, p-value = 0.07189 We can also make a QQ-plot for each variable. Essentially what we require from this plot is for the majority of our data to fall on the straight line - especially the datapoints in the middle. Some deviation at the tails is ok. This plot orders our data and plots the observed data against values on the x-axis that we would expect to get if our data was truly from a normal population. qqnorm(jayM$Mass) qqline(jayM$Mass, col = &quot;steelblue&quot;, lwd = 2) qqnorm(jayM$Head) qqline(jayM$Head, col = &quot;steelblue&quot;, lwd = 2) Both of these QQ plots are ok, and indicate normality, as does our Shapiro-Wilk tests. Therefore we would be ok to use a Pearson Correlation test with these data. What should you do though if either of your continuous variables are not approximately normally distributed? In that case, there are other correlation coefficients and associated significance tests that you could run instead. We describe these in more detail in Section x.xxx 11.6 Confidence Intervals for R bit more on this and the theory. 11.7 Partial Correlations An important consideration when running a correlation is the third variable problem. In brief, this is when we have the situation that a purported association between X and Y is actually being driven by both of their association with a third variable Z. It is important to consider that a correlation may only exist because both variables are correlated with something else. Alternatively, we may wish to get an &quot;adjusted&quot; correlation between X and Y based on their relationship with Z. This is essentially the theory behing partial correlations. If we have measured the correlation between X and Y (which we'll call \\(r_{xy}\\)) as well as each of their correlations with Z (which we'll call \\(r_{xz}\\) and \\(r_{yz}\\) respectively) then we can calculate this adjusted partial correlation between X and Y which we'll call \\(r_{xy.z}\\). To do this, we use this horrible looking formula: \\(r_{xy.z} = \\frac{r_{xy} - (r_{xz}\\times r_{yz}) } {\\sqrt{1-r_{xz}^2} \\times \\sqrt{1-r_{yz}^2}}\\) In reality though, it's pretty easy to plug the correct values for \\(r\\) or \\(r^2\\) into this formula and get the result required. Let's use an example. In these data we have 50 participants who logged the number of hours that they engaged in a task. We also have a column that shows their score in that task, and a column that records how tired they were. Higher tiredness scores means that they were more tired. (We've changed this from the column name in the original video to make it clear!) gs &lt;- read_csv(&quot;data/gamescore.csv&quot;) ## Parsed with column specification: ## cols( ## name = col_character(), ## hours = col_double(), ## score = col_double(), ## alertness = col_double() ## ) colnames(gs)[4]&lt;-&quot;tiredness&quot; # change this column name to make it more clear nrow(gs) ## [1] 50 head(gs) ## # A tibble: 6 x 4 ## name hours score tiredness ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ebert, Julia 11.6 95.2 4.5 ## 2 Vasquez, Horacio 8.5 109. 6.1 ## 3 Yoakum, Anthony 8 110. 6.6 ## 4 Kroha, Abagail 10.7 95.4 7.3 ## 5 Baray Perez, Daysi 3 81.2 7.7 ## 6 Mcalister, Katherine 8.5 124. 7.2 Say we predicted that the more hours played would lead to a higher score in the task. We might make our scatterplot and then run a one-tailed Pearson's correlation significance test: # scatterplot ggplot(gs, aes(x=hours, y=score)) + geom_point(color=&quot;navy&quot;) + stat_smooth(method=&quot;lm&quot;,se=F) + theme_classic() cor.test(gs$hours, gs$score, alternative = &quot;greater&quot;) # p=0.049 ## ## Pearson&#39;s product-moment correlation ## ## data: gs$hours and gs$score ## t = 1.6887, df = 48, p-value = 0.04888 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.001470868 1.000000000 ## sample estimates: ## cor ## 0.2368152 We can see from this plot that we have a correlation of r=0.24 between hours and score, which is a low to moderate correlation. Our one-tailed significance test also tells us that we have a significant difference of p=0.049 that this is a significantly positive correlation, although the effect is smallish. What if we considered each of these variables' relationship with tiredness ? We can calculate the Pearson correlation for tiredness against score and for hours against tiredness, in addition to our original correlation of hours against score. cor(gs$hours, gs$score) # r = 0.24 ## [1] 0.2368152 cor(gs$tiredness, gs$score) # r = -0.21 ## [1] -0.2099467 cor(gs$hours, gs$tiredness) # r = -0.29 ## [1] -0.2867374 As you can see, tiredness is negatively correlated with score, meaning individuals who reporte themselves as more tired scored lower in the task. Hours was also negatively correlated with tiredness, meaning individuals who were more tired spent fewer hours on the task. It's therefore possible that an individual's tiredness could affect both the number of hours that an individual engages in the task, as well as their overall performance. These relationships may affect the overall obseved relationship between hours and score. If we were to do this in R, we'd use the pcor.test() function from the ppcor package. We just need to tell it what our original X and Y are as well as our third variable Z. library(ppcor) ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select pcor.test(x=gs$hours, y=gs$score, z=gs$tiredness) ## estimate p.value statistic n gp Method ## 1 0.1885594 0.1944534 1.316311 50 1 pearson This output tells us that the adjust correlation, i.e. the partial correlation for \\(r_{xy}\\) is 0.19. There is a p-value associated with that correlation of p=0.194 which indicates that the correlation is no longer significant. This means that the relationship between test score and hours is no longer signficant after you take into account that they are both related to tiredness. We can also look at this 'by hand' using the formula above: r.xy &lt;- cor(gs$hours, gs$score) # r = 0.24 r.xz &lt;- cor(gs$tiredness, gs$score) # r = -0.21 r.yz &lt;- cor(gs$hours, gs$tiredness) # r = -0.29 numerator &lt;- r.xy - (r.xz * r.yz) denominator &lt;- (sqrt(1 - r.xz^2) * (sqrt(1 - r.yz^2)) ) numerator/denominator ## [1] 0.1885594 We get the same result! 11.8 Non-parametric Correlations When at least on of our variables are not normal, then we need to consider alternative approaches to the Pearson correlation for assessing correlations. Let's take this example, where we are interested in seeing if there's an association between saturated fat and cholesterol levels across a bunch of different cheeses: library(tidyverse) cheese &lt;- read_csv(&quot;data/cheese.csv&quot;) ## Parsed with column specification: ## cols( ## type = col_character(), ## sat_fat = col_double(), ## polysat_fat = col_double(), ## monosat_fat = col_double(), ## protein = col_double(), ## carb = col_double(), ## chol = col_integer(), ## fiber = col_double(), ## kcal = col_integer() ## ) head(cheese) ## # A tibble: 6 x 9 ## type sat_fat polysat_fat monosat_fat protein carb chol fiber kcal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 blue 18.7 0.8 7.78 21.4 2.34 75 0 353 ## 2 brick 18.8 0.784 8.60 23.2 2.79 94 0 371 ## 3 brie 17.4 0.826 8.01 20.8 0.45 100 0 334 ## 4 camembert 15.3 0.724 7.02 19.8 0.46 72 0 300 ## 5 caraway 18.6 0.83 8.28 25.2 3.06 93 0 376 ## 6 cheddar 21.1 0.942 9.39 24.9 1.28 105 0 403 # let&#39;s make a scatterplot of saturated fat against cholesterol ggplot(cheese, aes(x = sat_fat, y = chol)) + geom_point() + stat_smooth(method=&quot;lm&quot;,se=F) It looks like there is a pretty obvious relationship, but let's check the normality of each variable before progressing. Firstly the Shapiro-Wilk tests suggest that our data do not come from a normal distribution: shapiro.test(cheese$sat_fat) # P &lt; 0.05, therefore data may not be normal ## ## Shapiro-Wilk normality test ## ## data: cheese$sat_fat ## W = 0.85494, p-value = 6.28e-07 shapiro.test(cheese$chol) # P &lt; 0.05, therefore data may not be normal ## ## Shapiro-Wilk normality test ## ## data: cheese$chol ## W = 0.90099, p-value = 2.985e-05 Secondly, we have quite dramatic deviation from the straight line of our datapoints in our QQ plots. This indicates that our dat are likley skewed. qqnorm(cheese$sat_fat) qqline(cheese$sat_fat, col = &quot;steelblue&quot;, lwd = 2) qqnorm(cheese$chol) qqline(cheese$chol, col = &quot;steelblue&quot;, lwd = 2) We could be thorough and check this by plotting histograms of our data: library(gridExtra) p1 &lt;- ggplot(cheese, aes(x=sat_fat)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;) p2 &lt;- ggplot(cheese, aes(x=chol)) + geom_histogram(color=&quot;black&quot;, fill=&quot;lightseagreen&quot;) grid.arrange(p1,p2,nrow=1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Because our data do not appear to be normal, we cannot do a Pearson correlation coefficient. We should instead use a non-parametric correlation method. There are several of these to choose from. We don't plan to go into the details here of how these methods determine their correlation coefficients or conduct significance test. In brief, these methods generally rank order the datapoints along the x and y axes and then determine how ordered these ranks are with respect to each other. Probably the most commonly used non-parametric correlation test is called the Spearman Rank Correlation test. To run this, we can use cor() to get the correlation or cor.test() to run the signficance test in the same way we did the Pearson test. However, the difference here is that we specify method=&quot;spearman&quot; at the end. cor(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;) ## [1] 0.8677042 cor.test(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;) ## Warning in cor.test.default(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;): ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: cheese$sat_fat and cheese$chol ## S = 8575.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8677042 The correlation coefficient here is 0.87 and is termed rho instead of r. With the significance test, you get a test-statistic S which relates to how well ordered the ranked data are. Also provided is a p-value. As with the Pearson, the default is a 2-tailed test, testing whether the obsvered correlation could have come from a population with a correlation of 0. If the p-value is below 0.05 (using alpha = 0.05 as our criterion), then that is reasonable evidence that there is a significant correlation. You may also notice with Spearman Rank correlations that you are forever getting warnings about computing p-values with ties. Don't worry at all about this - although this is an issue with the test and how it calculates the p-value, it isn't of any real practical concern. If you were interested in conducting a one-tailed correlation test, you could do that in the same way as you did for the Pearson. For instance, if you predicted that cholesterol and saturated fat would have a positive correlation, you could do the following to do a one-tailed test: cor.test(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;, alternative = &quot;greater&quot;) ## Warning in cor.test.default(cheese$sat_fat, cheese$chol, method = &quot;spearman&quot;, : ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: cheese$sat_fat and cheese$chol ## S = 8575.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is greater than 0 ## sample estimates: ## rho ## 0.8677042 You may also notice that the output of the Spearman Rank test does not give confidence intervals for the value of rho. This is unfortunate and is one of the drawbacks of doing a non-parametric correlation. Finally, there are several other types of non-parametric correlations you could choose from if you didn't want to do a Spearman Rank correlation. We personally recommend using a method called Kendalls Tau B correlation, which can be done like this: cor.test(cheese$sat_fat, cheese$chol, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: cheese$sat_fat and cheese$chol ## z = 8.8085, p-value &lt; 2.2e-16 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.7102531 This output gives you a tau value which is the correlation coefficient, and a p-value which you can interpret in the same way as the other tests. Ranked Data If at least one of your variables of your data are rank (ordinal) data, then you should use non-parametric correlations. In the following example, the data show the dominance rank, age, body size and testosterone levels for a group of 18 animals. Lower numbers of the ranks, indicate a higher ranking animal. An animal with rank 1 means that it is the most dominant individual. Perhaps with such data you may be interested in seeing if there was an association between dominance rank and testosterone levels. Because your dominance rank measure is ordinal (a rank), then you should pick a non-parametric correlation. test &lt;- read_csv(&quot;data/testosterone.csv&quot;) ## Parsed with column specification: ## cols( ## drank = col_integer(), ## age = col_double(), ## size = col_integer(), ## testosterone = col_double() ## ) head(test) ## # A tibble: 6 x 4 ## drank age size testosterone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 3 13 183 4.8 ## 2 7 9 155 3.9 ## 3 7 5.5 144 3.8 ## 4 1 11.5 201 6.4 ## 5 12 3.5 125 1.8 ## 6 4 10 166 4.3 ggplot(test, aes(x = drank, y = testosterone)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) + xlab(&quot;Dominance Rank&quot;) + ylab(&quot;Testosterone Level&quot;) cor(test$drank, test$testosterone, method = &quot;spearman&quot;) # rho = -0.91 ## [1] -0.9083378 If you had the a priori prediction, that more dominant animals would have higher testosterone, then you could do a one-tailed test. This would mean that you expect there to be a negative correlation - as the rank number gets higher, the levels of testosterone would fall. In this case, you'd use alternative = &quot;less&quot;. cor.test(test$drank, test$testosterone, method = &quot;spearman&quot;, alternative = &quot;less&quot;) # 1- tailed ## Warning in cor.test.default(test$drank, test$testosterone, method = ## &quot;spearman&quot;, : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: test$drank and test$testosterone ## S = 1849.2, p-value = 9.367e-08 ## alternative hypothesis: true rho is less than 0 ## sample estimates: ## rho ## -0.9083378 Non-parametric Partial Correlation If you look back to section xxx.xxx you'll see that we used a very long formula to calculate the partial correlation for a Pearson correlation. We also used the funtion pcor.test() from the ppcor package to do this for us. We can actually use the same formula or function to do partial correlations for non-parametric correlations. For example, let's examine the Spearman correlations between anxiety, exam score and revision. These are the data: exams &lt;- read_csv(&quot;data/exams1.csv&quot;) ## Parsed with column specification: ## cols( ## code = col_integer(), ## revise = col_integer(), ## exam = col_integer(), ## anxiety = col_double(), ## gender = col_character() ## ) nrow(exams) ## [1] 102 head(exams) ## # A tibble: 6 x 5 ## code revise exam anxiety gender ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 4 40 86.3 Male ## 2 2 11 65 88.7 Female ## 3 3 27 80 70.2 Male ## 4 4 53 80 61.3 Male ## 5 5 4 40 89.5 Male ## 6 6 22 70 60.5 Female Using the Shapiro-Wilk test we note that our data are not approximately normal: # note these data are not approximately normal: # all p&lt;0.05, therefore reject null that they are approximately normal shapiro.test(exams$revise) ## ## Shapiro-Wilk normality test ## ## data: exams$revise ## W = 0.81608, p-value = 6.088e-10 shapiro.test(exams$exam) ## ## Shapiro-Wilk normality test ## ## data: exams$exam ## W = 0.95595, p-value = 0.001841 shapiro.test(exams$anxiety) ## ## Shapiro-Wilk normality test ## ## data: exams$anxiety ## W = 0.85646, p-value = 1.624e-08 We can investigate the individual correlations by plotting the scatterplots and calculating the Spearman correlations: cor(exams$revise, exams$exam, method = &quot;spearman&quot;) ## [1] 0.3330192 cor(exams$revise, exams$anxiety, method = &quot;spearman&quot;) ## [1] -0.6107712 cor(exams$anxiety, exams$exam, method= &quot;spearman&quot;) ## [1] -0.3887703 If we were interested in the correlation between revision time and exam performance controlling for anxiety, we would need to do a partial correlation. We can just use the ppcor.test() function: pcor.test(x=exams$revise, y=exams$exam, z=exams$anxiety, method = &quot;spearman&quot;) ## estimate p.value statistic n gp Method ## 1 0.1310034 0.1916154 1.314798 102 1 spearman Again, we see that the relationship between these two variables is reduced considerably to \\(\\rho=0.13\\) when we controll for each of their relationships with anxiety. 11.9 Point-Biserial Correlation Throughout the entirety of this chapter we've been discussing various correlation measures between two continuous variables. This little subsection is here just to point out that you can in fact also measure the correlation between a categorical variable and a continuous variable. In the following dataset, we have data on the number of hours that various cats spend away from their house over a one week period. They are tracked via cool GPS collars, and these data make pretty pictures of how far they traveled from their homes. cats &lt;- read_csv(&quot;data/cats.csv&quot;) ## Parsed with column specification: ## cols( ## time = col_integer(), ## sex = col_character(), ## sex1 = col_integer() ## ) nrow(cats) ## [1] 60 head(cats) ## # A tibble: 6 x 3 ## time sex sex1 ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 41 male 1 ## 2 40 female 0 ## 3 40 male 1 ## 4 38 male 1 ## 5 34 male 1 ## 6 46 female 0 The data have a time column that is the number of hours away from home. A sex column which is the sex of the cat, and a sex1 column which is the sex of the cat coded as a number. This final column exists to help us make the dotplot below and to help run a correlation: # dot plot ggplot(cats, aes(sex1,time)) + geom_jitter(width = .05) + scale_x_continuous(breaks=c(0,1), labels=c(&quot;female&quot;, &quot;male&quot;))+ stat_smooth(method=&#39;lm&#39;,se=F) + theme_classic() What we have in this plot are individual points representing each cat. The points are jittered to make overlapping datapoints more readable. We have also put a trendline through the data. How was this achieved? Essentially, all the females are given the value 0, and all the males are given the value 1. We then run a Pearson's correlation using the X values to be 0 or 1, and the Y values to be the time measurement. Doing this, we can simply run a Pearson's correlation test using cor.test(): cor.test(cats$sex1, cats$time) ## ## Pearson&#39;s product-moment correlation ## ## data: cats$sex1 and cats$time ## t = 3.1138, df = 58, p-value = 0.002868 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.137769 0.576936 ## sample estimates: ## cor ## 0.3784542 These results suggests that there is a positive relationship between cat sex and time spent away from home, with male cats spending more time away than female cats. Looking at the dotplot again, it looks like the female data are possibly bimodal. As the data are in long format, we can filter out the male and female data: catsF_time &lt;- cats %&gt;% filter(sex==&quot;female&quot;) %&gt;% .$time catsM_time &lt;- cats %&gt;% filter(sex==&quot;male&quot;) %&gt;% .$time shapiro.test(catsF_time) ## ## Shapiro-Wilk normality test ## ## data: catsF_time ## W = 0.81663, p-value = 8.505e-05 shapiro.test(catsM_time) ## ## Shapiro-Wilk normality test ## ## data: catsM_time ## W = 0.97521, p-value = 0.7246 Clearly our suspicions about the female data are true, they do not appear to come from an approximately normal distribution. In that case, we could run a point-biserial Spearman correlation. This is just the same procedure, but instead we apply the Spearman test: cor.test(cats$time, cats$sex1, method=&quot;spearman&quot;) ## Warning in cor.test.default(cats$time, cats$sex1, method = &quot;spearman&quot;): Cannot ## compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: cats$time and cats$sex1 ## S = 23048, p-value = 0.004774 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.3596003 Just as a note - there are probably 'better' ways of running such a point-biserial correlation than using cor.test(), but what we've shown here is probably sufficient to get the idea across that you can convert two groups to 0's and 1's and then run a correlation to test for group differences. "],
["regression.html", "12 Regression 12.1 Introduction to Linear Regression 12.2 a and b 12.3 Residuals 12.4 Standard Error of the Estimate 12.5 Goodness of Fit Test - F-ratio 12.6 Assumptions of Linear Regression 12.7 Examining individual predictor estimates", " 12 Regression Introduction to what regression is... predicting etc... 12.1 Introduction to Linear Regression The regression method that we are going to start with is called Ordinary Least Squares Regression. Hopefully the reason why it's called &quot;least squares&quot; will be come obvious, although we're not too sure why it's called &quot;ordinary&quot;. Let's illustrate the question at hand with some data: library(tidyverse) # Import Data df &lt;- read_csv(&quot;data/parenthood.csv&quot;) ## Parsed with column specification: ## cols( ## dan.sleep = col_double(), ## baby.sleep = col_double(), ## dan.grump = col_integer(), ## day = col_integer() ## ) nrow(df) ## [1] 100 head(df) ## # A tibble: 6 x 4 ## dan.sleep baby.sleep dan.grump day ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 7.59 10.2 56 1 ## 2 7.91 11.7 60 2 ## 3 5.14 7.92 82 3 ## 4 7.71 9.61 55 4 ## 5 6.68 9.75 67 5 ## 6 5.99 5.04 72 6 As you can see, what we have are four columns of data. The first column shows the amount of sleep that Dan got in an evening. The second column relates to the amount of sleep a baby got. The third column is a rating of Dan's grumpiness. The last column is a day identifier. Each row represents a different day. Say we're interested in seeing whether there was an association between Dan's sleep and her grumpiness. We could examine this using a scatterplot: # Scatterplot ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) It looks like these variables are clearly associated, with higher levels of sleep being related to lower levels of grumpiness. To get a measure of how big this relationship is, we could run a correlation test. cor.test(df$dan.sleep, df$dan.grump) ## ## Pearson&#39;s product-moment correlation ## ## data: df$dan.sleep and df$dan.grump ## t = -20.854, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9340614 -0.8594714 ## sample estimates: ## cor ## -0.903384 This shows that the variables are highly correlated with r=-0.90. It's also a highly significant relationship. Using stat_smooth() we also applied a best fitting trendline through the data. This line was actually calculated to be in the position that it is in using regression. The line also has the equation: $y&#39; = a + xb$ You may also se this written in other ways such as: $y&#39; = \\beta_{0} + x\\beta_{1}$ but we'll stick with $y&#39; = a + xb$ 12.2 a and b In the equation for a regression line, $y&#39; = a + xb$ \\(y&#39;\\) is equal to the predicted value of \\(y\\). Essentially if you go from any value of \\(x\\) and go up to the trendline, and then across to the y-axis, that is your predicted value of y. The trendline represents the predicted values of \\(y\\) for all values of \\(x\\). In regression terms, we often refer to \\(x\\) as the predictor variable and \\(y\\) as the outcome variable. The value of \\(b\\) in the equation represents the slope of the regression line. If it's a positive number then it means that the regression line is going upwards (akin to a positive correlation, where \\(y\\) increases as \\(x\\) increases.) If it's a negative number, then it means that the regression line is going downwards (akin to a negative correlation, where \\(y\\) decreases as \\(x\\) increases). The value of \\(b\\) can also be considered to be how much \\(y\\) changes for every 1 unit increase in \\(x\\). So a value of \\(b = -1.4\\) would indicate that as \\(x\\) increases by 1, \\(y\\) will decrease by 1.4. The value of \\(a\\) represents the y-intercept. This is the value of y' that you would get if you extended the regression line to cross at \\(x=0\\). We can illustrate that in the graph below. We extended the trendline (dotted black line) from its ends until it passes through where \\(x=0\\) (the dotted red line). Where it crosses this point is at \\(y=125.96\\) which is depicted by the orange dotted line. This makes the y-intercept for this trendline equal to 125.96. # Scatterplot ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) + xlim(-1,10)+ ylim(40,130)+ geom_abline(intercept =125.956 , slope = -8.937, lty=2)+ geom_vline(xintercept=0, color=&#39;red&#39;, lty=2) + geom_segment(x=-1.6,xend=0,y=125.96,yend=125.96, lty=2, color=&#39;orange&#39;) In reality, we do not ever extend the reression lines like this. In fact, a regression line by definition only fits the range of datapoints along the x-axis that we have - it should not extend beyond those points. This is more a theoretical construct to help us understand where the line is on the graph. To think a bit more about a and b let's compare these regression lines and their respective equations. comparing a and b In the top left image, you can see three parallel regression lines. These all have the same slope value (b = 0.704). How they differ is in their y-intercept. The different values of 'a' indicate that they are at different heights. In the top right, we've extended the regression lines in red back to where the x-axis is 0. Where the lines cross here (x=0) is the value of each y-intercept i.e. 'a'. In the bottom right image, you can see that all the plots have the same y-intercept value (8.32). How they differ is in their slope b. Two are positive values of b, with the larger value having a steeper slope. The negative value of b (-0.622) has a slope going downwards. In the bottom right we extend these trendlines back to where x=0, and you can see that all have the same y-intercept. 12.2.1 How to calculate a and b in R To run a regression in R, we use the lm() function. It looks like the following: mod1 &lt;- lm(dan.grump ~ dan.sleep, data=df) # build regression model The first thing after the bracket is the outcome variable which is dan.grump. Then a tilde (~) and then the predictor variable which is dan.sleep. Finally, we tell lm what dataset we're using. The best way to read that statement is: &quot;dan.grump 'is predicted by' dan.sleep&quot;. We're also saving the regression model as mod1 because there's tons of information that comes along with the regression. To have a look at what a and b are, we can just look at our saved object mod1. mod1 ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Coefficients: ## (Intercept) dan.sleep ## 125.956 -8.937 Here the value underneath &quot;Intercept&quot; (125.956) refers to the y-intercept a. The value underneath dan.sleep, the predictor variable, is our value of b the slope. So this would mean that the regression line for these data would be: \\(y&#39; = 125.956 + -8.937b\\) We can also get these values directly by running the following code: mod1$coefficients ## (Intercept) dan.sleep ## 125.956292 -8.936756 Now, we should also mention one other thing about these values. These regression coefficients are 'estimates'. We have one sample of 100 subjects from which we estimated the true population values of 'a' and 'b'. The true population values of 'a' and 'b' are parameters. 12.2.2 How to calculate a and b 'by hand' To think a bit more about these 'a' and 'b' values, we could look at how these values are actually calculated. First, we calculate 'b'. The formula for this requires knowing the sample standard deviation of the X and Y variables, as well as their Pearson correlation. \\(b = r\\frac{s_{Y}}{s_{X}}\\) So for our example, we'd calculate 'b' like this: r &lt;- cor(df$dan.sleep, df$dan.grump) sy &lt;- sd(df$dan.grump) sx &lt;- sd(df$dan.sleep) b &lt;- r * (sy/sx) b ## [1] -8.936756 Next, we can calculate 'a' using the formula: \\(a = \\overline{Y} - b\\overline{X}\\) . This requires us to know the sample mean of X and Y. So for our example, we calculate 'a' like this: my &lt;- mean(df$dan.grump) mx &lt;- mean(df$dan.sleep) a &lt;- my - (b * mx) a ## [1] 125.9563 Therefore, we have an equation for our trendline which is y' = 125.96 + -8.94b, which means that for every 1 unit increase of sleep, Dan's grumpiness decreases by 8.94. 12.3 Residuals Once we fit a trendline to the data it's clear that not all the datapoints fit to the line. In fact, almost none of the datapoints are on the line! This is because the trendline is our prediction of the value of y based on the value of x. Eeach datapoint actually is either larger or smaller in terms of y than the regression line. Sometimes it's a bit bigger or smaller, other times it might be a lot bigger or smaller. Occasionally, the predicted value of y might be on the regression line. Because our trendline isn't a perfect fit for the data, the formula for the regression line could technically be written as: \\(y&#39; = a + bx + \\epsilon\\) \\(\\epsilon\\) refers to the error, or how far each datapoint is from the predicted value. In fact, the difference of each data point from the predicted value is called a raw residual or ordinary residual. We calculate the size of the residual for each datapoint by the following formula: \\(residual = y - y&#39;\\) This essentially is the difference of each data point from the predicted value. 12.3.1 How to calculate the residuals Using our regression line equation of y' = 125.96 + -8.94b, we can manually calculate the raw residuals. Firstly, we calculate the predicted values of y \\(y&#39;\\) for each value of \\(x\\). We can put these back into our original dataframe. X &lt;- df$dan.sleep # the predictor Y &lt;- df$dan.grump # the outcome Y.pred &lt;- 125.96 + (-8.94 * X) Y.pred ## [1] 58.1054 55.2446 80.0084 57.0326 66.2408 72.4094 52.7414 61.6814 59.8040 ## [10] 67.1348 67.9394 69.9062 72.7670 66.5090 68.6546 69.3698 69.6380 50.2382 ## [19] 61.5026 58.6418 54.4400 60.2510 64.6316 55.6916 82.5116 73.4822 50.8640 ## [28] 64.0058 61.5026 63.4694 52.9202 55.7810 69.9062 48.5396 81.4388 70.6214 ## [37] 68.6546 82.6904 63.1118 57.4796 58.8206 55.1552 53.3672 59.1782 54.5294 ## [46] 77.3264 53.0096 57.8372 73.4822 45.5000 51.6686 65.9726 59.5358 73.2140 ## [55] 49.7912 72.0518 60.7874 60.5192 64.4528 70.3532 63.9164 63.2906 61.5920 ## [64] 69.6380 48.0032 56.0492 53.1884 60.9662 66.0620 58.4630 59.9828 56.8538 ## [73] 78.3992 55.6916 69.1910 62.3966 77.2370 56.2280 62.2178 51.3110 64.0058 ## [82] 62.7542 48.5396 80.4554 82.0646 63.1118 63.2012 57.3902 53.0990 73.3928 ## [91] 74.8232 66.4196 64.7210 76.1642 79.8296 78.4886 56.4962 77.8628 63.2012 ## [100] 68.2970 df$Y.pred &lt;- Y.pred head(df) ## # A tibble: 6 x 5 ## dan.sleep baby.sleep dan.grump day Y.pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 ## 2 7.91 11.7 60 2 55.2 ## 3 5.14 7.92 82 3 80.0 ## 4 7.71 9.61 55 4 57.0 ## 5 6.68 9.75 67 5 66.2 ## 6 5.99 5.04 72 6 72.4 Next, getting the raw residual is simply a matter of taking each observed value of \\(y\\) and subracting the predicted value of \\(y\\). # so to get the residual, y - y&#39; df$dan.grump - Y.pred ## [1] -2.1054 4.7554 1.9916 -2.0326 0.7592 -0.4094 0.2586 -1.6814 ## [9] 0.1960 3.8652 4.0606 -4.9062 1.2330 0.4910 -2.6546 -0.3698 ## [17] 3.3620 1.7618 -0.5026 -5.6418 -0.4400 2.7490 9.3684 0.3084 ## [25] -0.5116 -1.4822 8.1360 1.9942 -1.5026 3.5306 -8.9202 -2.7810 ## [33] 6.0938 -7.5396 4.5612 -10.6214 -5.6546 6.3096 -2.1118 -0.4796 ## [41] 0.1794 4.8448 -5.3672 -6.1782 -4.5294 -5.3264 3.9904 2.1628 ## [49] -3.4822 0.5000 6.3314 2.0274 -1.5358 -2.2140 2.2088 1.9482 ## [57] -1.7874 -1.5192 2.5472 -3.3532 -2.9164 0.7094 -0.5920 -8.6380 ## [65] 5.9968 5.9508 -1.1884 3.0338 -1.0620 6.5370 -2.9828 2.1462 ## [73] 0.6008 -2.6916 -2.1910 -1.3966 4.7630 11.7720 4.7822 2.6890 ## [81] -11.0058 -0.7542 1.4604 -0.4554 8.9354 -1.1118 0.7988 -0.3902 ## [89] 0.9010 -1.3928 3.1768 -3.4196 -5.7210 -2.1642 -3.8296 0.5114 ## [97] -5.4962 4.1372 -8.2012 5.7030 df$residuals &lt;- df$dan.grump - Y.pred head(df) ## # A tibble: 6 x 6 ## dan.sleep baby.sleep dan.grump day Y.pred residuals ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 -2.11 ## 2 7.91 11.7 60 2 55.2 4.76 ## 3 5.14 7.92 82 3 80.0 1.99 ## 4 7.71 9.61 55 4 57.0 -2.03 ## 5 6.68 9.75 67 5 66.2 0.759 ## 6 5.99 5.04 72 6 72.4 -0.409 12.3.2 Visualizing the Residuals Now we have a raw residual for all 100 datapoints in our data. The following plot is our same scatterplot, but this time we've also added a little red line connecting each observed datapoint to the regression line. The size of each of these red lines represents the residuals. p1 &lt;- ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_segment(aes(xend = dan.sleep, yend = dan.grump-residuals), alpha = .5, color=&#39;red&#39;) + theme_classic() + ggtitle(&quot;OLSR best fit trendline&quot;) p1 12.3.3 Comparing our trendline to other trendlines An important question to consider is why did we end up with our trendline and not some other trendline? The answer is that ours is the 'best fit', but what does that really mean? In short, it means that the best-fit regression line is the one that has the smallest squared residuals. The squared residuals are calcualted by squaring every residual and then summing these all up. Let's look at this by looking at one possible trendline that we could have used. The one that we'll choose is a trendline that goes horizonatally through the data with a y value that is the mean of Y. mean(df$dan.grump) ## [1] 63.71 So, the mean of the Y variable dan.grump is 63.71. Let's put a trendline through our data that is horizontal at 63.71. The equation for this line would be: \\(y&#39; = 63.71 + 0x\\) Let's visualize this: ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_hline(yintercept = mean(df$dan.grump), color=&#39;blue&#39;) It doesn't look like using the mean value of Y is that good a predictor of each datapoint. We could actually visualize how good.bad it is by calculating the residual of each datapoint from this new trendline. This time each residual is equal to: \\(residual = y - \\overline{y}\\) Let's calculate these residuals, and then graph them on this scatterplot: # we can work out what the &#39;residuals&#39; would be for this trendline: df$Ymean &lt;- mean(df$dan.grump) df$resid_Ymean &lt;- df$dan.grump - df$Ymean #residual from Ymean head(df) ## # A tibble: 6 x 8 ## dan.sleep baby.sleep dan.grump day Y.pred residuals Ymean resid_Ymean ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 10.2 56 1 58.1 -2.11 63.7 -7.71 ## 2 7.91 11.7 60 2 55.2 4.76 63.7 -3.71 ## 3 5.14 7.92 82 3 80.0 1.99 63.7 18.3 ## 4 7.71 9.61 55 4 57.0 -2.03 63.7 -8.71 ## 5 6.68 9.75 67 5 66.2 0.759 63.7 3.29 ## 6 5.99 5.04 72 6 72.4 -0.409 63.7 8.29 ## visualize this: p2 &lt;- ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_hline(yintercept = mean(df$dan.grump), color=&#39;blue&#39;) + geom_segment(aes(xend = dan.sleep, yend = dan.grump-resid_Ymean), alpha = .5, color=&#39;red&#39;) + # geom_point(aes(y = dan.grump+residuals), shape = 1) + theme_classic() + ggtitle(&quot;Residuals to the mean Y&quot;) p2 If we compare both graphs scatterplots side by side, it's pretty clear that our best-fit trendline is doing a much better job of predicting each datapoint's Y value. The horizontal trendline at the mean of Y looks pretty bad for the majority of datapoints - its residuals are much bigger. library(gridExtra) grid.arrange(p1,p2,nrow=1) In fact, let's actually quantify the difference in residuals between these two trendlines. Because we have both positive and neagative residuals, if we just added them together we'd end up with 0. In statistics, one common way to make numbers positive is to square them. As we saw with standard deviation, this also has the advantage of emphasizing large values. What we do to compare our residuals, is to therefore square them. We call the residuals from our trendline the raw residuals. We call the residuals from the horizontal line the total residuals. df$residuals2 &lt;- df$residuals ^ 2 # raw residuals df$resid_Ymean2 &lt;- df$resid_Ymean ^ 2 # total residuals head(df[c(1,3,6,8,9,10)]) # just showing the relevant columns ## # A tibble: 6 x 6 ## dan.sleep dan.grump residuals resid_Ymean residuals2 resid_Ymean2 ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 56 -2.11 -7.71 4.43 59.4 ## 2 7.91 60 4.76 -3.71 22.6 13.8 ## 3 5.14 82 1.99 18.3 3.97 335. ## 4 7.71 55 -2.03 -8.71 4.13 75.9 ## 5 6.68 67 0.759 3.29 0.576 10.8 ## 6 5.99 72 -0.409 8.29 0.168 68.7 You can see the squared raw residual and squared total residual for the first six datapoints. Ideally, we want the raw residuals to be as small a fraction as possible of the total residuals. If we sum up the squared raw residuals and squared total residuals, we can determine this: SS.resid &lt;- sum(df$residuals2) SS.resid #1838.722 ## [1] 1838.75 SS.tot &lt;- sum(df$resid_Ymean2) SS.tot #9998.59 ## [1] 9998.59 As you can clearly see, the summed squared residuals for our best fit regression line are much smaller than the summed squared residuals when using the mean of y as the regression line. 12.3.4 Coefficient of Determination R2 One way to make these summed squares of residuals numbers more interpretable is to convert them to \\(R^{2}\\). The logic goes as following. If the trendline is absolutely useless at predicting the y values, then the trendline would have residuals as high as the total residuals. If the trendline is perfect at predicting the y values, then the residual SS total would be 0. If we look at the sum of the squares of the raw residuals as a ratio of a sum of the squared total residuals then we can work out how well our trendline fits. We calculate this using the formula, \\(R^{2} = 1 - \\frac{SS_{raw}}{SS_{total}}\\) in the following way: 1 - (SS.resid/SS.tot) # 0.816 (this is R2) ## [1] 0.816099 So for our data, \\(R^{2} = 0.816\\). This means that our regression model (and trendline) is a very good fit to the data. \\(R^{2}\\) ranges from 0 to 1. If its value was 1, then that would indicate that the best-fit trendline perfectly fits the data with no raw residuals. If its value was 0, then that would mean that the best-fit trendline is no better at fitting the data than the horizontal line at the mean of Y. Values of \\(R^{2}\\) that get closer to 1 indicate that the model is doing a better job at estimating each value of Y. We say that the model has a better 'fit' to the data. There is a way to directly get the value of \\(R^{2}\\) in R. You may remember earlier in this chapter that we ran our linear model using the function lm(). We saved the output of this regression model as the object mod1. You can use summary() to get lots of information about the regression model. summary(mod1) # the R2 matches ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 The thing to focus on from this output right now, is the value called Multiple R-squared. This gives us the same value of \\(R^{2}\\) that we calculated by hand: \\(R^{2}=0.816\\). The other value of \\(R^{2}\\) is called Adjusted R-squared. This is relevant when you are conducting multiple regression with more than one predictor in the model. We only have one predictor in the model (dan.sleep) so we won't talk more about Adjusted R squared here. Finally, there is a shortcut way in which we can calculate \\(R^{2}\\). It is simply to square the Pearson's correlation coefficient: \\(R^{2} = r^{2}\\). r &lt;- cor(df$dan.sleep, df$dan.grump) r ## [1] -0.903384 r^2 # same R2 as above ## [1] 0.8161027 12.4 Standard Error of the Estimate \\(R^{2}\\) is one value that gives us a sense of how well our regression model is doing. Another method to assess 'model fit' is to examine the Standard Error of the Estimate. Unfortunately, this value has a number of names. You'll see it referred to as the Standard Error of the Regression, Residual Standard Error, Regression Standard Error, \\(S\\), or \\(\\sigma_{est}\\). We prefer to call it the Standard Error of the Estimate or \\(\\sigma_{est}\\). This is calculated as follows: \\(\\sigma_{est} = \\sqrt{\\frac{\\Sigma (y - y&#39;)^{2}}{n-2}}\\) The \\(\\Sigma (y - y&#39;)^{2}\\) part of this equation is the Sum of the raw residuals squared that we already calculated when calculating \\(R^{2}\\). We can therefore calculate \\(\\sigma_{est}\\) quite straightforwardly in R manually. s_est &lt;- sqrt(SS.resid / 98) # n=100 s_est ## [1] 4.3316 Therefore \\(\\sigma_{est} = 4.332\\) for our model. It's acutally possible to see this value in the output of the summary of the model in R. Here, it's called the 'residual standard error': summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 12.4.1 What to do with the Standard Error of the Estimate ? What we are generally looking for with \\(\\sigma_{est}\\) is a number as small as possible. This is because what essentially it is a measure of is an approximate estimate of the average raw residual. The question is, how small is small? This is difficult to answer. One reason is because \\(\\sigma_{est}\\) is actually in the original units of the Y-axis (the outcome variable). Thefore it's not as simple as saying that it should be close to 0, because the Y-axis may be in units that are very large. In other ways, having this value in the original units of Y can be quite helpful as it is easy to envisage what the size of the average residual is. However, a good rule of thumb is that approximately 95% of the observations (raw datapoints) should fall within plus or minus 2 times the standard error of the estimates from the regression line. We can illustrate this with the following plot: ggplot(df, aes(x = dan.sleep, y = dan.grump)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() + geom_abline(intercept = 125.956+s_est+s_est, slope = -8.937, color = &#39;red&#39;, lty=2)+ geom_abline(intercept = 125.956-s_est-s_est, slope = -8.937, color = &#39;red&#39;, lty=2) + ggtitle(&quot;Regression with 2 x Standard Error of the Estimate&quot;) This is our original scatterplot again. The blue line is still the best fitting trendline. The two dashed red lines are two times \\(\\sigma_{est}\\) above and below the blue line respectively. That is they are 2 * 4.332 = 8.644 units of grumpiness above or below the trendline. If we count up the number of datapoints that our outside of the dotted red lines, we can see that there are six datapoints out of our 100 datapoints that are just outside, although some of htese are very close indeed to the red line. This is probably ok as we're only expecting 95% of datapoints on average to be inside the red lines. 12.5 Goodness of Fit Test - F-ratio \\(R^{2}\\) and \\(\\sigma_{est}\\) are two calculations that help us determine if our regression model (and trendline) is indeed a good fit to the data. A more formal method is to run a statistical test called the Goodness of Fit Test. To do this we calculate an F-ratio that essentially examines how well our trendline (and model) fit the data compared to the null model which is the model where we use the mean of Y as our prediction. As a reminder, these scatterplots show the difference in performance of our fitted model (left) compared to the null model (right). grid.arrange(p1,p2,nrow=1) The F-ratio is essentially a method of determining the proportion of residual variance compared to total variance. We can calculate it using the following formula: $F = $ Here, SSM refers to the sum of squares for the model (the model sum of squares). This is equal to: \\(SSM = SST - SSR\\) That is, it's the difference between the total sum of squares (the sum of the squared residuals for the null model) minus the residual sum of squares (the sum of the squared residuals for the fitted model). The d.f.SSM refers to the degrees of freedom for the model sum of squares, which is equal to the number of predictors in the model. We only have one predictor (dan.sleep), so that means the degrees of freedom are 1. The d.f.SSR refers to the degrees of freedom for the raw residuals. This is equal to the number of observations minus the number of predictors minus 1. Therefore it is equal to 100 - 1 - 1 = 98 as we had 100 datapoints (or obsevations). Let's calculate this in R. Firstly, we'll calculate the model sum of squares: SS.mod &lt;- SS.tot - SS.resid SS.mod #8159.868 ## [1] 8159.84 Next we divide the model sum of squares and the residual sum of squares by their respective degrees of freedom to get the mean sum of squares for each. \\(F\\) is then calculated by dividing the former by the latter: MS.resid &lt;- SS.resid / 98 MS.resid #18.76 ## [1] 18.76276 MS.mod &lt;- SS.mod / 1 MS.mod #8159.868 ## [1] 8159.84 Fval &lt;- MS.mod / MS.resid Fval #434.9 ## [1] 434.8955 So \\(F = 434.9\\) which is a large value. Larger values indicate that we were less likely to get a difference between the sum of squares for our fitted and null models by chance alone. Essentially, the observed value of F is compared to the sampling distribution for F if the null hypothesis is true. The null is that our model (trendline) is no better than random in fitting the datapoints Whether our \\(F\\) value is sufficiently large can be looked up in an F-table (not recommended), or you can simply let R do the work for you. If you use summary() on your saved regression model, it will give you the \\(F\\) value as well as a p-value to go along with it. summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 You can see from the model output that the F-statistic is given on the bottom row along with the degrees of freedom for the model (1) and the residuals (98). The p-value here is 0.0000000000000022 which is basically 0. Typically, if the p-value is less than 0.05 we will say that our fitted model is a better fit to the data than the null model. 12.6 Assumptions of Linear Regression Even if your model is a good fit to the data, there are still several things you need to check before progressing to examining whether your predictor is 'significantly' (a better term is probably 'meaningfully') predicting the outcome variable. The last series of things we need to do is to check whether we have violated the assumptions of linear regression. In short, here are some of the assumptions that we need to adhere to: Normality - Specifically the residuals are normally distributed. Linearity - We need to be examining linear relationships between predictors and outcomes Homogeneity of Variance (homoscedasticity) Uncorrelated Predictors (only relevant if doing more than one predictor) No overly influential datapoints Let's discuss each of these in turn. 12.6.1 Normality of Residuals One assumption of linear regression is that our residuals are approximately normally distributed. We can check this in several ways. But first, we should probably own up and mention that there are several types of residuals. The residuals we have been dealing with from the regression model have been the raw residuals. We got these by simply subtracting the predicted value of y from the observed value of y \\(residual = y - y&#39;\\). However, statisticians like to modify these residuals. One modification they make is to turn these residuals into what are called standardized residuals. These are the raw residuals divided by the standard deviation of the residuals. You can directly access the standardized residuals in R by using the rstandard() function with your model output: df$std_resids &lt;- rstandard(mod1) head(df[c(1,3,6,11)]) # just showing the relevant columns ## # A tibble: 6 x 4 ## dan.sleep dan.grump residuals std_resids ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.59 56 -2.11 -0.494 ## 2 7.91 60 4.76 1.10 ## 3 5.14 82 1.99 0.467 ## 4 7.71 55 -2.03 -0.478 ## 5 6.68 67 0.759 0.172 ## 6 5.99 72 -0.409 -0.0991 We can demonstrate that the standardized residuals really are highly correlated with the raw residuals by plotting them on a scatterplot: ggplot(df, aes(x=residuals, y=std_resids)) + geom_point() + theme_classic()+ xlab(&quot;Raw Residuals&quot;)+ ylab(&quot;Standardized Residuals&quot;) They are effectively the same - just transformed. One advantage of standardized residuals is that they can help us look for unusual datapoints with large residuals. While raw residuals are always in the original units of the y-axis, standardized residuals are, well, standardized. Standardized residuals that are greater than 2 or less than -2 are quite large, those that are larger than 3 or less than 3 are very large indeed and should be looked at in more detail. Let's get back to checking the normality of these residuals. First up, we could plot a histogram and see if we think it's approximately normal: #a) histogram plot ggplot(df, aes(x=std_resids)) + geom_histogram(color=&#39;white&#39;) # possibly ok ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This looks possibly ok. It's not too skewed, but it can be harded from a histogram with just 100 datapoints to get a sense of the shape. A second approach would be to use a Shapiro-Wilk test which more formally test whether the data are approximately normally distributed: #b) Shapiro-Wilk test shapiro.test(df$std_resids) # shapiro test says normal. ## ## Shapiro-Wilk normality test ## ## data: df$std_resids ## W = 0.99201, p-value = 0.8221 Given that the p-value here is above our cut-off of 0.05, this suggests that we have no evidence to reject the hypothesis that our data came from a normal distribution. An easier way of saying this is, our residuals are likey approximately normally distributed. The other method we can employ to check normality is to use a QQ plot: #c) QQ plot qqnorm(df$std_resids) qqline(df$std_resids, col = &quot;steelblue&quot;, lwd = 2) # it&#39;s ok A discussion of precisely what these QQ plots are is beyond the scope here. However, in general terms, what we are plotting is the residual against the theoretical value of each residual that we would expect if our data were normally distributed. In practical terms, what we're looking for is that the bulk of our data fit along the blue line. It's ok to have a bit of wobble at the extremes - that just means that our data distribution probably has slightly fat tails. It's also possible to generate a version of the above plot quickly, directly from the saved linear model object. plot( x = mod1, which = 2 ) # fast way of getting same plot 12.6.2 2. Linearity --- The second major assumption of linear regression is that the relationship between our predictor and outcome is linear! So, data that look like the following would not have a linear relationship. comparing a and b One simple approach is to examine the scatterplot of your predictor (X) and outcome (Y) variables. We already did this, and our data looked pretty linear! Another approach is to examine the relationship between your observed Y values and the predicted Y values \\(y&#39;\\). This should also be a linear relationship. Although we calculated the \\(y&#39;\\) values earlier using the formula for the regression line, we can actually grab them directly from the model object with the fitted.values() command. df$Y.fitted &lt;- fitted.values(mod1) # plot this relationship ggplot(df, aes(x = Y.fitted, y = dan.grump)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) As you can see from this plot, our fitted (predicted) values of Y on the x-axis have a strong linear relationship with our observed values of Y on the y-axis. 12.6.3 3. Homogeneity of Variance / Homoscedasticity The third assumption that we need to check is homoscedasticity (also sometimes referred to as homogeneity of variance). What this really means is that the model should be equally good at predicting Y's across all values of X. Our regression model shouldn't be better at predicting values of Y for e.g. small values of X but not for large values of X. Practically, what this means, is that the size of the residuals should be equal across all values of X. If we plot the values of X (or the predicted/fitted values of Y) on the x-axis against the residuals (in this case standardized residuals) on the Y-axis, then there should be no overall pattern. We should be equally likely to get small or large residuals for any value of X (or predicted/fitted value of Y). This would mean that any trendline on this graph should be a horizontal line. # if true, this should be a straight line ggplot(df, aes(x = Y.fitted, y = std_resids)) + geom_point() + stat_smooth(method=&#39;lm&#39;, se=F) As you can see, our plot is basically a random scatterplot and there is no overall pattern. That is good, it means we have homoscedasticity. If we did not have homoscedasticity, then we'd see a pattern in this scatterplot - such as the residuals getting larger or smaller for larger fitted (predicted) values of Y. You can access a version of this plot directly from the model object like this: plot(mod1, which = 3) There are some more formal tests of homoscedasticity but we don't need to worry about them. 12.6.4 No Colinearity This assumption of linear regression only applies when we have more than one predictor in the model. In our model, we do only have one predictor (dan.sleep) so we don't need to worry about it. If we had added in another variable into the model, e.g. the amount of hours of baby sleep, then we'd have a second predictor. That means, we're trying to predict dan.grump based on both dan.sleep and baby.sleep. In this case, dan.sleep and baby.sleep should not be correlated with each other. 12.6.5 Unusual Datapoints There are a number of ways that datapoints could be unusual. We will discuss data points that are: outliers have high leverage have high influence Generally linear regression models should not be overly affected by individual data points. Usually the category that we most need to be concerned about are points with high influence. 12.6.5.1 i. Outliers Outliers are datapoints that are typically highly unusual in terms of outcome Y but not in terms of the predictor X. These will be datapoints that have high residuals. Let's look at an example dataset. We have a predictor 'x' and an outcome 'y'. With 'y1' we have the same outcome variables, but have removed the value for one datapoint: x &lt;- c(4.1,4.2,5,5.5,6,6.1,6.15,6.4,7,7.2,7.7,8,9.7) y &lt;- c(3.9,4.3,6,5.9,6.1,11.9,6.3,5.8,7.9,6.4,7.8,8,9.1) y1 &lt;- c(3.9,4.3,6,5.9,6.1,NA,6.3,5.8,7.9,6.4,7.8,8,9.1) ddf &lt;- data.frame(x,y,y1) ddf ## x y y1 ## 1 4.10 3.9 3.9 ## 2 4.20 4.3 4.3 ## 3 5.00 6.0 6.0 ## 4 5.50 5.9 5.9 ## 5 6.00 6.1 6.1 ## 6 6.10 11.9 NA ## 7 6.15 6.3 6.3 ## 8 6.40 5.8 5.8 ## 9 7.00 7.9 7.9 ## 10 7.20 6.4 6.4 ## 11 7.70 7.8 7.8 ## 12 8.00 8.0 8.0 ## 13 9.70 9.1 9.1 And here, we're plotting the best fitting regression line in blue. The dotted red line represents the best fitting trendline that we would have if we removed the datapoints that has the y value of 11.9. The dashed black line shows the distance from this datapoint to the trendline we would have if it was removed. ## (Intercept) x ## 0.8247787 0.8785270 As you can see from this small example, outliers are datapoints that have very large residuals from the trendline. They have an unusually large Y. Notice though that the slope of the trendline hasn't change too much at all. It is slighly shifted down after you remove that outlier from the calculations, but overall the coefficient of 'b' is similar to before. This type of outlier is not necessarily a big deal. 12.6.5.2 ii. High Leverage Datapoints that have high leverage are those that have a high influence on the regression line's trajectory, but don't necessarily affect the angle of the slope. They are typically unsual in terms of their X value, but not necessarily in terms of Y, meaning that they don't have to have a high residual. We can measure the leverage of datapoints using the function hatvalues() on the model object. For the scatterplot above, the hat values of each datapoint can be calculated as follows: mod.out &lt;- lm(y~x, data = ddf) ddf$hat_val &lt;- hatvalues(mod.out) ddf ## x y y1 hat_val ## 1 4.10 3.9 3.9 0.25535302 ## 2 4.20 4.3 4.3 0.24009985 ## 3 5.00 6.0 6.0 0.14260536 ## 4 5.50 5.9 5.9 0.10381722 ## 5 6.00 6.1 6.1 0.08206442 ## 6 6.10 11.9 NA 0.07975810 ## 7 6.15 6.3 6.3 0.07886047 ## 8 6.40 5.8 5.8 0.07692761 ## 9 7.00 7.9 7.9 0.08966480 ## 10 7.20 6.4 6.4 0.09936183 ## 11 7.70 7.8 7.8 0.13552914 ## 12 8.00 8.0 8.0 0.16540649 ## 13 9.70 9.1 9.1 0.45055168 As you can see, the 6th datapoint was the outlier but it does not have a large leverage as it's not overly influencing the trajectory of the regression line. Datapoint 13 on the other hand does have a higher leverage. Roughly speaking, a large hatvalue is one which is 2-3 times the average hat value. We can check the mean of the hat values like this: mean(hatvalues(mod.out)) ## [1] 0.1538462 So clearly, the 13th datapoint does have a high leverage. 12.6.5.3 iii. High Influence The third type of unusual datapoint are ones that you need to be most wary of. These are datapoints that have high influence. Essentially a high influence datapoint is a high leverage datapoint that is also an outlier. Let's look at these example data. We are looking at a predictor variable 'x' against an outcome variable 'y2'. 'y3' is the same data as 'y2' with one datapoint's value removed. x &lt;- c(4.1,4.2,5,5.5,6,6.15,6.4,7,7.2,7.7,8,9.7) y2 &lt;- c(3.9,4.3,6,5.9,6.1,6.3,5.8,7.9,6.4,7.8,8,6.1) y3 &lt;- c(3.9,4.3,6,5.9,6.1,6.3,5.8,7.9,6.4,7.8,8,NA) ddf1 &lt;- data.frame(x,y2,y3) ddf1 ## x y2 y3 ## 1 4.10 3.9 3.9 ## 2 4.20 4.3 4.3 ## 3 5.00 6.0 6.0 ## 4 5.50 5.9 5.9 ## 5 6.00 6.1 6.1 ## 6 6.15 6.3 6.3 ## 7 6.40 5.8 5.8 ## 8 7.00 7.9 7.9 ## 9 7.20 6.4 6.4 ## 10 7.70 7.8 7.8 ## 11 8.00 8.0 8.0 ## 12 9.70 6.1 NA Let's plot these data - I'm sparing you the code for this plot: What you can see here is the scatterplot of 'x' against 'y2'. The blue trendline is the best fit line to the regression of 'x' against 'y2'. The red dotted line is the regression line when we remove the datapoint at x=9.7. This datapoint has both high leverage and is an outlier (has an overly large residual). As a consequence of this, when you remove this datapoint it significantly shifts the angle of the slope of the regression line. That means that the value of b changes quite considerably. Let's add the hat values (measuring leverage) to the data. We can also add what is called Cook's Distance which is a measure of the influence of each datapoint. mod.out2 &lt;- lm(y2~x, data = ddf1) ddf1$hat_val &lt;- hatvalues(mod.out2) ddf1$cooks_d &lt;-cooks.distance(mod.out2) ddf1 ## x y2 y3 hat_val cooks_d ## 1 4.10 3.9 3.9 0.26609280 0.2940686663 ## 2 4.20 4.3 4.3 0.25062833 0.1201644267 ## 3 5.00 6.0 6.0 0.15151904 0.0347793264 ## 4 5.50 5.9 5.9 0.11178988 0.0026090653 ## 5 6.00 6.1 6.1 0.08914853 0.0007585906 ## 6 6.15 6.3 6.3 0.08568825 0.0029898530 ## 7 6.40 5.8 5.8 0.08333867 0.0085341275 ## 8 7.00 7.9 7.9 0.09512926 0.1169635836 ## 9 7.20 6.4 6.4 0.10452756 0.0038328585 ## 10 7.70 7.8 7.8 0.13998476 0.0808111497 ## 11 8.00 8.0 8.0 0.16946124 0.1138881520 ## 12 9.70 6.1 NA 0.45269169 2.8757588661 As you can see, datapoint 12 (x=9.7) has high leverage and it's also an outlier. Consequently it has a very large Cook's Distance. Typically a Cook's distance &gt; 1 requires more consideration about how to proceed with your model. You can also quickly get a plot of the Cook's Distances of all datapoints from a regression model like this: plot(mod.out2, which = 4) 12.6.5.4 Checking Influence: Let's look for influential datapoints in our Sleep vs Grumpiness Data. At the beginning of this section, we saved the linear model as mod. We can look at the Cook's Distances with the cooks.distance() function, or we could just plot them: plot(mod1, which = 4) This plot shows each of our 100 datapoints in the order they appear in the original dataframe along the x-axis. Clearly, looking at the y-axis, all our datapoints have Cook's distances of far below 1, so we are fine to assume that we have no overly influential datapoints. 12.7 Examining individual predictor estimates After checking if our model is a good fit and making sure that we did not violate any of the assumptions of the linear regression, we can finally move forward with what we came here to do. That is, we can check whether our predictor is meaningfully useful in predicting our outcome variable. This means, is our observed value of b sufficiently different from 0? To do this, we use two strategies. First, we can generate a 95% confidence interval around our estimate of b. That is the method that we prefer. Secondly, you can run a significance test. 12.7.1 95% confidence interval of 'b'. First, let's do this the easy way. Then we'll work out how we got the 95% confidence interval. You can simply find this by running the confint() function on our regression model object: # easy way - gives us the confidence interval: coefficients(mod1) ## (Intercept) dan.sleep ## 125.956292 -8.936756 confint(object = mod1, level = .95) ## 2.5 % 97.5 % ## (Intercept) 119.971000 131.94158 ## dan.sleep -9.787161 -8.08635 This tells us that the lower bound of the 95% confidence interval of b is -9.79, and the upper bound is -8.09, with our estimate of b being -8.94. This means that if we were to sample 100 days randomly over and over again and measure Dan's sleep and grumpiness on these days, in 95% of the samples we collect we would have the true population parameter value of b. In lay terms, we can suggest that there is approximately a 95% likelihood that this true population value of b is between -9.79 and -8.09. The only way to really get the true population value would have been to measure Dan's sleep and grumpiness for every day that Dan has been alive. In big picture terms, what we can draw from this 95% confidence interval is that the relationship between sleep and grumpiness is highly negative and clearly strong. There really seems to be close to no chance that 0 could be the value of b - it is not inside the confidence interval. In fact, we could generate a ridiculously confidence confidence interval, such as a 99.9999% confidence interval: confint(object = mod1, level = .999999) ## 0.00005 % 99.99995 % ## (Intercept) 110.21037 141.702208 ## dan.sleep -11.17398 -6.699536 ... and 0 is still nowhere near being included. How is this 95% confidence interval calculated? To do this we need to think about those sampling distributions again, and the standard error of b. 12.7.2 Standard Error of b As we've discussed earlier, our one single estimate of b is just one possible estimate. We estimated this value based on our one single sample of 100 days. However, if we had sampled a different sample of 100 days, we would have got a slighlty (or maybe greatly) different estimate of b. If we repeated this procedure thousands of times, then we'd have a sampling distribution of b's. This sampling distribution will be t-distribution shaped and has degrees of freedom = number of observations - the number of predictors (1) - 1. Therefore it is t-distribution shaped with d.f.=98. If we know the standard deviation of this sampling distribution (which is known as the standard error of b - \\(s_{b}\\)), then we can create confidence intervals. However, it turns out that calculating \\(s_{b}\\) is a bit annoying. Fortunately, there is a quick way to find it out by looking at the summary of the model output: summary(mod1) ## ## Call: ## lm(formula = dan.grump ~ dan.sleep, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.025 -2.213 -0.399 2.681 11.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 125.9563 3.0161 41.76 &lt;2e-16 *** ## dan.sleep -8.9368 0.4285 -20.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.332 on 98 degrees of freedom ## Multiple R-squared: 0.8161, Adjusted R-squared: 0.8142 ## F-statistic: 434.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 We can see here that \\(s_{b} = 0.4285\\) If you're interested, then this is the formula to calculate \\(s_{b}\\): \\(s_{b} = \\frac{\\sigma_{est}}{\\sqrt{\\Sigma (x - \\overline{x})}}\\) Here's how we calculate it by hand according to this formula: x_dif &lt;- df$dan.sleep - mean(df$dan.sleep) # difference of each x from mean of x ssx &lt;- sqrt(sum(x_dif^2)) # square root of the sum of these differences squared sb &lt;- s_est/ssx #s_est was calculated earlier sb # this is the standard error of b ## [1] 0.4285351 12.7.3 Calculating 95% confidence interval of 'b' by hand We can use the following formula to calculate the 95% CI for b: \\(CI_{95} = b +/- t * s_{b}\\) As with all confidence intervals, what we do is to presume that our estimate of b is the mean of the sampling distribution. Then knowing that the distribution is t-shaped with d.f.=98, we need to find the value of t that will leave 2.5% of the distribution in each tail. We can look that up in R using qt: tval &lt;- qt(.975, df = 98) tval ## [1] 1.984467 We've now calculated everyting we need to for the Confidence Interval b + (tval * sb) #upper bound ## [1] -8.086342 b - (tval * sb) # lower bound ## [1] -9.78717 As we can see, these match up with the output when using confint(): confint(mod1) ## 2.5 % 97.5 % ## (Intercept) 119.971000 131.94158 ## dan.sleep -9.787161 -8.08635 12.7.4 Signifcance Testing b We can also use this sampling distribution to apply a signifiance test. Our null hypothesis will be that the population value of b is 0. Our alternative hypothesis will be that b is not equal to 0. \\(H_{0}: b=0\\). \\(H_{1}: b\\neq0\\) Given we know the standard deviation of this sampling distribution, \\(s_{b}\\), we can calculate how far away our observed sample value of \\(b\\) is in terms of how many standard deviations from the mean of 0 it is. We call this value a t-statistic and it is calculated like this: \\(t = \\frac{b}{s_{b}}\\) Once we have calculated our t-statistic, then we can determine given the shape of the distribution and the degrees of freedom, what proportion of the distribution is more extreme than our observed value. tobs &lt;- b/sb tobs #-20.854 ## [1] -20.8542 Our observed value is therefore \\(t = -20.854\\). Let's look at this graphically: comparing a and b This shows the t-distribution for d.f. = 98. It looks very strange because we've extended the axes to -21 and +21. We did this so that we could include a dotted blue line for our observed t-value of -20.854. Notice that by the time the t-distribution of d.f.=98 gets to close to a value of t=-3 then there is almost nothing left in the tail of the distribution. Just for completeness, we can calculate what proportion of times we observe a t-value of more extreme (i.e. less than) -20.854 using pt(). pt(-20.854, df=98) ## [1] 4.094577e-38 This essentially gives us a one-tailed p-value of p = 0.00000000000000000000000000000000000004094577, which is very small. To get a 2-tailed p-value we just double this value. Essentially what we can conclude here is that our one value of b is extremely unlikely to have come from a population where b=0, and thus we reject our null hypothesis and accept the alternative. The preceding information was provided to help you think about what's really going on when we calculate the t-statistic. However, tt turns out there is actually a shortcut way of calculating t using n and the Pearson's correlation coefficient r. It's the following formula: \\(t = \\frac{r \\times \\sqrt{n-2}}{\\sqrt{1 - r^{2}}}\\) r &lt;- cor(df$dan.sleep, df$dan.grump) n &lt;- nrow(df) (r * sqrt(n-2)) / (sqrt(1-r^2)) # -20.85 ## [1] -20.8544 As you can see from the above code - it works! "],
["permutation-testing.html", "13 Permutation Testing 13.1 t-test Permutation 13.2 Correlation Coefficient Permutation Tests 13.3 Permutation test for a Paired t-test 13.4 Permutation tests in Packages", " 13 Permutation Testing Permutation tests are a type of randomization test. The theoretial difference between permutation tests and inferential tests is that with permutation tests we build the sampling distribution from the observed data, rather than infering or assuming that a sampling distribution exist. In practice, what a permutation test does is to take your observed data and then shuffle (or permute) part of it. After each shuffle, some aspect of the data is recalculated. That could be for instance the correlation coefficient, or it could be a difference in means between two groups. The data then get randomly reshuffled again, and the test-statistic is recalculated again. This goes on for thousands of times - for as many shuffles are deemed acceptable. This is usually a minimum of 1,000 but typically at least 10,000 shuffles are done. After all the permutations (shuffles) are performed, a distribution of the statistic of interest is generated from the permutations. This is comapred to the original observed statistics (e.g. correlation coefficient, difference in group means) to see if the observed value is unusually large compared to the permuted data. If this seems a little confusing, hopefully seeing it in action will help... 13.1 t-test Permutation Let's look at our two independent samples of exam scores: library(tidyverse) anastasia &lt;- c(65, 74, 73, 83, 76, 65, 86, 70, 80, 55, 78, 78, 90, 77, 68) bernadette &lt;- c(72, 66, 71, 66, 76, 69, 79, 73, 62, 69, 68, 60, 73, 68, 67, 74, 56, 74) # put into a dataframe: dd &lt;- data.frame(values = c(anastasia, bernadette), group = c(rep(&quot;Anastasia&quot;,15), rep(&quot;Bernadette&quot;, 18)) ) dd ## values group ## 1 65 Anastasia ## 2 74 Anastasia ## 3 73 Anastasia ## 4 83 Anastasia ## 5 76 Anastasia ## 6 65 Anastasia ## 7 86 Anastasia ## 8 70 Anastasia ## 9 80 Anastasia ## 10 55 Anastasia ## 11 78 Anastasia ## 12 78 Anastasia ## 13 90 Anastasia ## 14 77 Anastasia ## 15 68 Anastasia ## 16 72 Bernadette ## 17 66 Bernadette ## 18 71 Bernadette ## 19 66 Bernadette ## 20 76 Bernadette ## 21 69 Bernadette ## 22 79 Bernadette ## 23 73 Bernadette ## 24 62 Bernadette ## 25 69 Bernadette ## 26 68 Bernadette ## 27 60 Bernadette ## 28 73 Bernadette ## 29 68 Bernadette ## 30 67 Bernadette ## 31 74 Bernadette ## 32 56 Bernadette ## 33 74 Bernadette We can plot these data as boxplots to get a sense of the within group variation as well as the observed differences between the groups: ggplot(dd, aes(x = group, y = values, fill = group)) + geom_boxplot(alpha=.3, outlier.shape = NA) + geom_jitter(width=.1, size=2) + theme_classic() + scale_fill_manual(values = c(&quot;firebrick&quot;, &quot;dodgerblue&quot;)) Now, from our two independent samples, we can directly observe what the difference in sample means is. This is just calculated by subtracting one sample mean from the other: meandif &lt;- mean(anastasia) - mean(bernadette) # 5.48 meandif ## [1] 5.477778 So, from our samples, we observed a difference in grades of 5.48 between the groups. Typically, we would run an independent t-test to test whether these two samples came from theoretical populations that differ in their means: t.test(anastasia, bernadette, var.equal = T) ## ## Two Sample t-test ## ## data: anastasia and bernadette ## t = 2.1154, df = 31, p-value = 0.04253 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1965873 10.7589683 ## sample estimates: ## mean of x mean of y ## 74.53333 69.05556 This Student's t-test (notice var.equal=T) suggests that this is a significant difference, meaning that the groups do differ in their population means. However, this test relies on several assumptions (see section xx.x.x). Instead, we could apply a permutation test that is free of assumptions. Essentially what we are going to do is ask how surprising it was to get a difference of 5.48 given our real data. Put another way, if we shuffled the data into different groups of 15 and 18 (the respective sample sizes of Anastasia and Bernadette), would we get a difference in sample means of greater or lower than 5.48? If we did this thousands of times, how many times would we get differences in sample means above 5.48? Let's apply this theory to just one permutation. First, we combine all the data: set.seed(1) # just to keep the random number generator the same for all of us allscores &lt;- c(anastasia, bernadette) allscores ## [1] 65 74 73 83 76 65 86 70 80 55 78 78 90 77 68 72 66 71 66 76 69 79 73 62 69 ## [26] 68 60 73 68 67 74 56 74 Next, we shuffle them into new groups of 15 and 18.: x &lt;- split(sample(allscores), rep(1:2, c(15,18))) x ## $`1` ## [1] 80 78 71 73 65 68 67 74 72 74 76 83 68 70 69 ## ## $`2` ## [1] 74 90 69 68 78 66 73 76 62 56 79 65 60 73 55 77 66 86 We have two brand new samples that contain all of the scores from our original data, but they've just been shuffled around. We could look at what the difference in sample means is between these two new samples: x[[1]] # this is our shuffled sample of size 15 ## [1] 80 78 71 73 65 68 67 74 72 74 76 83 68 70 69 x[[2]] # this is our shuffled sample of size 18 ## [1] 74 90 69 68 78 66 73 76 62 56 79 65 60 73 55 77 66 86 mean(x[[1]]) # mean of the new sample of size 15 ## [1] 72.53333 mean(x[[2]]) # mean of the new sample of size 18 ## [1] 70.72222 # what&#39;s the difference in their means? mean(x[[1]]) - mean(x[[2]]) ## [1] 1.811111 The difference in sample means is 1.81, which is a lot smaller than our original difference in sample means. Let's do this same process 10,000 times! Don't worry too much about the details of the code. What we are doing is the above process, just putting it in a loop and asking it to do it 10,000 times. We save all the results in an object called results. results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ x &lt;- split(sample(allscores), rep(1:2, c(15,18))) results[[i]]&lt;-mean(x[[1]]) - mean(x[[2]]) } head(unlist(results)) # these are all our mean differences from 10,000 shuffles of the data. We&#39;re just looking at the first 6. ## [1] -1.8555556 -2.5888889 4.0111111 -3.9333333 0.2222222 3.5222222 We can actually make a histogram showing the distribution of these differences in sample means. df &lt;- data.frame(difs = unlist(results)) ggplot(df, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;green&quot;, alpha=.4) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = 5.48) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This histogram shows that for some of our 10,000 shuffles, we actually got some differences between our two samples of higher than 5.48 (the dotted blue line), but the vast majority of shuffles led to samples that had mean differences lower than 5.48. In fact, several shuffles led to samples where the sample of size 18 (Bernadette in the original data) had a sample mean that was higher than the sample of size 15 (Anastasia in the original data). We can directly calculate how many times out of 10,000 shuffles we got a difference in sample means that was greater than 5.48 sum(unlist(results) &gt; 5.48) # 202 times out of 10000 ## [1] 215 To convert this to a p-value, we simply divide this value by the number of shuffles we ran - which was 10,000. sum(unlist(results) &gt; 5.48) /10000 # which is 0.0202 proportion of the time ## [1] 0.0215 So our p-value is p=0.0215 which is similar to a one-tailed p-value. If we wished to have a 2-tailed p-value we would simply multiply this value by 2: # 2-tailed value 2 * (sum(unlist(results) &gt; 5.48) /10000) ## [1] 0.043 Example 2: Let's take a look at a second example. Here, we have various subjects rating their anxiety levels. They do this after either taking a new anxiolytic drug or a placebo. The subjects in each group are independent of each other. The placebo group has 19 subjects and the drug group has 21 subjects. The data: placebo &lt;- c(15, 16, 19, 19, 17, 20, 18, 14, 18, 20, 20, 20, 13, 11, 16, 19, 19, 16, 10) drug &lt;- c(15, 15, 16, 13, 11, 19, 17, 17, 11, 14, 10, 18, 19, 14, 13, 16, 16, 17, 14, 10, 14) length(placebo) #19 ## [1] 19 length(drug) #21 ## [1] 21 If we were interested in doing a Student's t-test, we might want to check whether the data are approximately normal. We could perform Shapiro-Wilk tests to do this: shapiro.test(drug) # approximately normal as p&gt;.05 ## ## Shapiro-Wilk normality test ## ## data: drug ## W = 0.95184, p-value = 0.3688 shapiro.test(placebo) # not enough evidence to be normal as p&lt;.05 ## ## Shapiro-Wilk normality test ## ## data: placebo ## W = 0.88372, p-value = 0.02494 From this we find that the placebo group is not approximately normally distributed (p value of the Shapiro-Wilk test is &lt;.05). We could do a non-parametric test such as Wilcoxon Ranked Sum test (see xxx.xxx), but an alternative strategy is to perform a permutation test. Let's first plot the data, and then look at our observed difference in anxiety scores between our two independent samples: # put into dataframe - long format ddf &lt;- data.frame(anxiety = c(placebo, drug), group = c(rep(&quot;placebo&quot;, length(placebo)), rep(&quot;drug&quot;, length(drug)) ) ) head(ddf) ## anxiety group ## 1 15 placebo ## 2 16 placebo ## 3 19 placebo ## 4 19 placebo ## 5 17 placebo ## 6 20 placebo #boxplots ggplot(ddf, aes(x=group, y=anxiety, fill=group)) + geom_boxplot(outlier.shape = NA, alpha=.4) + geom_jitter(width=.1) + theme_classic() + scale_fill_manual(values=c(&quot;orange&quot;, &quot;brown&quot;)) mean(placebo) - mean(drug) #2.128 ## [1] 2.12782 So our observed difference in sample means is 2.128. In the permutation test, what we'll do is shuffle all the scores randomly between the two groups, creating new samples of the same size (19 and 21). Then we'll see what difference in sample means we get from those shuffled groups. We'll also do this 10,000 times. allvalues &lt;- c(placebo, drug) results&lt;-vector(&#39;list&#39;,10000) for(i in 1:10000){ x &lt;- split(sample(allvalues), rep(1:2, c(19,21))) results[[i]]&lt;-mean(x[[1]]) - mean(x[[2]]) } head(unlist(results)) # these are the first six of all our mean differences from 10,000 shuffles of the data. ## [1] -0.8796992 -0.7794486 -1.2807018 -0.4786967 2.5288221 1.1253133 Let's plot the distribution of these data to see what proportion of times our shuffled groups got samples that were greater than 2.128. df0 &lt;- data.frame(difs = unlist(results)) ggplot(df0, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, alpha=.4) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = 2.128) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. It looks like very few times did we get two samples that had differences in sample means that were greater than 2.128. We can calculate exactly how many times, and express this as the proportion of times we got a difference in sample means greater than 2.128: sum(unlist(results) &gt; 2.128) # 109 times out of 10000 ## [1] 113 sum(unlist(results) &gt; 2.128) /10000 # which is 0.0109 proportion of the time ## [1] 0.0113 So, in this case we can say that the probability of getting a difference in sample means between the drug and placebo groups that was larger than our observed difference of 2.128 was p = 0.0109. This is very strong evidence that the observed difference is significantly greater than we'd expect by chance. 13.2 Correlation Coefficient Permutation Tests You can apply the logic of permutation tests to almost any statistical test. Let's look at an example for Pearson correlations. In these data, we are looking at 15 subjects who are completing a task. We measured the time they spent on the task and their high scores. library(tidyverse) df &lt;- read_csv(&quot;data/timescore.csv&quot;) ## Parsed with column specification: ## cols( ## subject = col_character(), ## time = col_double(), ## score = col_double() ## ) head(df) ## # A tibble: 6 x 3 ## subject time score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1A 5.5 3 ## 2 2B 2.4 6.9 ## 3 3C 8.8 17.9 ## 4 4D 7 10.5 ## 5 5E 9.3 12.2 ## 6 6F 2.5 3.5 If we make a scatterplot of the data, we can see that those who spent longer on the task tended to get higher scores: # scatterplot ggplot(df, aes(x = time, y = score)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) Using a standard approach, we could find the correlation of these two variables and run a signficance test using cor.test(). We can see that there is a moderate Pearson's r of r=0.55 which is statistically significant (p=0.031). # regular significance test cor.test(df$time,df$score) #r=0.55, p=0.031 ## ## Pearson&#39;s product-moment correlation ## ## data: df$time and df$score ## t = 2.4258, df = 13, p-value = 0.03057 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.0643515 0.8324385 ## sample estimates: ## cor ## 0.5582129 We could take an alternative tack, and decide to do a permutation test. The idea here is again, how surprising is it to get a correlation of 0.55 with these data? Were there other ways of ordering the x and y variables to get higher correlation coefficients? Let's look at our y axis variable, the score: set.seed(1) # just doing this so all our results look same df$score # actual data in order ## [1] 3.0 6.9 17.9 10.5 12.2 3.5 11.0 7.6 8.4 13.4 10.1 9.0 10.1 17.7 6.8 This is the original order of the data. If we use sample() we can shuffle the data: sample(df$score) # actual data but order shuffled ## [1] 10.5 3.5 7.6 10.1 17.9 8.4 13.4 17.7 12.2 3.0 6.9 10.1 9.0 6.8 11.0 Let's shuffle the score again, but this time store it in the original dataframe: df$shuffle1 &lt;- sample(df$score) #create a new column with shuffled data df ## # A tibble: 15 x 4 ## subject time score shuffle1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1A 5.5 3 7.6 ## 2 2B 2.4 6.9 10.1 ## 3 3C 8.8 17.9 10.1 ## 4 4D 7 10.5 12.2 ## 5 5E 9.3 12.2 8.4 ## 6 6F 2.5 3.5 13.4 ## 7 7G 4.8 11 6.9 ## 8 8H 4.1 7.6 3.5 ## 9 9I 5 8.4 3 ## 10 10J 2.9 13.4 17.7 ## 11 11K 6.4 10.1 6.8 ## 12 12L 7.7 9 11 ## 13 13M 9.3 10.1 9 ## 14 14N 8.3 17.7 17.9 ## 15 15O 5.1 6.8 10.5 If we plot this shuffled y (score) against the original x (time), we now get this scatterplot, which basically shows no relationship: # this is what that new column looks like: ggplot(df, aes(x = time, y = shuffle1)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) And the correlation for this new scatterplot is really close to 0! r = 0.0005: cor.test(df$time, df$shuffle1) # now relationship is a bit negative ## ## Pearson&#39;s product-moment correlation ## ## data: df$time and df$shuffle1 ## t = 0.0016429, df = 13, p-value = 0.9987 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.5119267 0.5125988 ## sample estimates: ## cor ## 0.0004556502 We could shuffle the score variable even more times, and directly calculate the r value aginst the time variable for each shuffle using cor(). # we can do this many times cor(df$time, sample(df$score)) # r = 0.35 ## [1] 0.3023584 cor(df$time, sample(df$score)) # r = 0.04 ## [1] -0.05905503 cor(df$time, sample(df$score)) # r = -0.06 ## [1] -0.4665168 cor(df$time, sample(df$score)) # r = 0.15 ## [1] -0.435933 As you can see, the more shuffles we do, we get varied values of r. What we really should do is perform 10,000 (or another really high number) shuffles of the score variable and re-calculate r against the time variable for all 10,000 of these shuffles. Don't worry about the code below, but that's exactly what we're doing. We're saving the r values from the 10,000 shuffles in the object called results. results &lt;- vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- cor(df$time, sample(df$score)) } head(unlist(results)) # this are the correlations for the first 6 of 10,000 shuffles ## [1] 0.274190962 0.005288304 -0.114492469 -0.280528642 0.235874922 ## [6] 0.061278049 We can plot the results in a histogram, and also put a vertical line at 0.56 which was our original observed correlation between time and score from the raw unshuffled data. results.df &lt;- data.frame(x = unlist(results)) ggplot(results.df, aes(x)) + geom_histogram(color=&quot;darkgreen&quot;,fill=&quot;lightseagreen&quot;) + geom_vline(xintercept = 0.56, lwd=1, lty=2) + xlab(&quot;r&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. As you can see, there were a few shuffles (or permutations) that we got an r value of greater than 0.56, but not that many. In fact, we can directly calculate how many: sum(unlist(results) &gt; 0.56) #163 were greater. ## [1] 163 It turns out that 163 times out of 10,000 shuffles we got a r value of greater than 0.56. WE could calculate this as a proportion by dividing by 10,000: sum(unlist(results) &gt; 0.56) / 10000 #0.0163 ## [1] 0.0163 We can use this value as our p-value. Because it is relatively low, we could argue that we were very unlikely by chance alone to have got a r value of 0.56 from our data. This suggests that the correlation between time and score is significant. The advantages of running a permutation test is that it is free of the assumptions of normality for the Pearson's r correlation signifiance test. It's also a cool method, and pretty intuitive. 13.3 Permutation test for a Paired t-test We can apply the same principle of permutation to the paired t-test. Rememeber, essentially the paired t-test is focused on performing a one-sample t-test on the difference in scores between the paired data - testing whether the mean of the differences could potentially come from a population with \\(\\mu=0\\). Let's look at the following data that record scores for the same individual over two time points - 'before' and 'after'. # take a look at these before and after scores ba &lt;- read_csv(&quot;data/beforeafter1.csv&quot;) ## Parsed with column specification: ## cols( ## id = col_character(), ## before = col_double(), ## after = col_double() ## ) head(ba) ## # A tibble: 6 x 3 ## id before after ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mc 5.5 5.3 ## 2 ma 5.7 5.3 ## 3 co 4.4 3.3 ## 4 kj 3.4 3.1 ## 5 ln 5.3 5.3 ## 6 oe 5.2 5.1 We could plot these data using a scatterplot to examine the overall trend of how scores change from before to after: # make a scatterplot with the x being &#39;before&#39; and y being &#39;after&#39; ggplot(ba, aes(x=before, y=after)) + geom_point() + theme_classic()+ geom_abline(intercept =0 , slope = 1) + xlim(2,8)+ ylim(2,8) As most of these points are below the diagonal line, this seems to suggest that the scores for the 'before' data seem to be lower on the whole than the scores for the 'above' data. Typically, we would run a paired t-test with such data to examine if there was a difference: t.test(ba$before, ba$after, paired=T) ## ## Paired t-test ## ## data: ba$before and ba$after ## t = 2.6667, df = 10, p-value = 0.02363 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1315583 1.4684417 ## sample estimates: ## mean of the differences ## 0.8 This suggests that there is a significant difference p&lt;.05 with the 95% confidence interval of the true difference in means being between 0.19 and 1.61. However, the paired t-test assumes that the data are from an approximately normal distribution. In particular, that the differences scores (the difference between the 'before' and 'after' scores for each individual) are normally distributed. We can check that using a Shapiro-Wilk test: # create a difference column for the difference between before and after ba$difference &lt;- ba$before - ba$after # run a Shapiro test on the difference column shapiro.test(ba$difference) ## ## Shapiro-Wilk normality test ## ## data: ba$difference ## W = 0.82621, p-value = 0.02081 With the p-value here being p&lt;.05, this suggests that our data are not normally distributed. One option would be to do a non-parametric Wilcoxon-signed rank test (see section xxxx.xxx). Alternatively, we could do a permutation test. Let's look at our data again, and focus on the difference column. ba ## # A tibble: 11 x 4 ## id before after difference ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mc 5.5 5.3 0.2 ## 2 ma 5.7 5.3 0.4 ## 3 co 4.4 3.3 1.1 ## 4 kj 3.4 3.1 0.300 ## 5 ln 5.3 5.3 0 ## 6 oe 5.2 5.1 0.1 ## 7 mb 3.4 3 0.400 ## 8 dc 7.5 5 2.5 ## 9 dg 3.4 2.1 1.30 ## 10 mj 6.6 3.9 2.70 ## 11 kb 5 5.2 -0.2 Our observed mean for the differences scores is 0.8. mean(ba$difference) ## [1] 0.8 How likely were we to get this mean difference if our 'before' and 'after' conditions were randomized? For example, for individaul 'mj', their before score was 6.6 and after was 3.9 leading to a difference of 2.7. But what if their before and after were switched? Then the difference score would be -2.7. What we would like to do, is to randomly flip the before and after columns for each individual and recalculate the difference scores. Each time we do this, we will calculate the mean of the difference scores. A programmatic shortcut to doing this is to multiple each difference score randomly by either +1 or -1. Here is the first shuffle we could perform: set.seed(1) shuffle1 &lt;- ba$difference * sample(c(-1,1), 11, replace = T) shuffle1 ## [1] -0.2 -0.4 1.1 0.3 0.0 0.1 0.4 2.5 1.3 -2.7 0.2 mean(shuffle1) ## [1] 0.2363636 In this example, the 'before' and 'after' scores were randomly flipped for individuals 'mc', 'ma', 'mj' and 'kb'. Let's do a second shuffle: shuffle2 &lt;- ba$difference * sample(c(-1,1), 11, replace = T) shuffle2 ## [1] -0.2 0.4 -1.1 0.3 0.0 0.1 0.4 -2.5 1.3 2.7 0.2 mean(shuffle2) ## [1] 0.1454545 In this example, the 'before' and 'after' scores were randomly flipped for individuals 'mc', 'co', 'kj', 'oe', 'mb', 'dg' and 'mj'. In both shuffles the mean of the difference scores was less than our observed mean of 0.8. We can put this into a loop to do it 10,000 times: results &lt;- vector(&#39;list&#39;,10000) for(i in 1:10000){ results[[i]] &lt;- mean(ba$difference * sample(c(-1,1), 11, replace = T)) } And we can plot these results as a histogram: df1 &lt;- data.frame(difs = unlist(results)) ggplot(df1, aes(x=difs)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, alpha=.4, binwidth = .05) + geom_vline(color=&quot;navy&quot;,lwd=1,lty=2,xintercept = .8) + theme_classic()+ ggtitle(&quot;Mean Differences from \\n 10000 Permutations of Raw Data&quot;) We can also calculate the number of times out of 10,000 that we observed a mean difference higher than the mean of 0.8 in our original data, which is only in 19 shuffles out fo 10,000: sum(unlist(results)&gt;0.8) ## [1] 17 We divide this number by 10,000 to get our p-value: sum(unlist(results)&gt;0.8) / 10000 ## [1] 0.0017 This suggests that we have a highly significant p=0.002 difference between our 'before' and 'after' data within subjects. 13.4 Permutation tests in Packages Above we wrote script from scratch to perform our permutation tests. In many ways, this is our preferred approach as it is more customizable. However, in some packages there are some permutation tests already available as functions. One example is the independence_test from the package coin that will do a permutation t-test for between subjects. The code for this is below (this requires dataframes to be in the long format): library(coin) ## Loading required package: survival head(ddf) ## anxiety group ## 1 15 placebo ## 2 16 placebo ## 3 19 placebo ## 4 19 placebo ## 5 17 placebo ## 6 20 placebo independence_test(anxiety ~ group, data = ddf, alternative = &quot;less&quot;) ## ## Asymptotic General Independence Test ## ## data: anxiety by group (drug, placebo) ## Z = -2.1998, p-value = 0.01391 ## alternative hypothesis: less As you can see, this gives a roughly similar result to our own permutation script. You can also do a 2-tailed version: #2-tailed permutation test independence_test(anxiety ~ group, data = ddf) ## ## Asymptotic General Independence Test ## ## data: anxiety by group (drug, placebo) ## Z = -2.1998, p-value = 0.02782 ## alternative hypothesis: two.sided "]
]
