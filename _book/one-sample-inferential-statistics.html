<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 One Sample Inferential Statistics | PSY317L Guides</title>
  <meta name="description" content="9 One Sample Inferential Statistics | PSY317L Guides" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="9 One Sample Inferential Statistics | PSY317L Guides" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 One Sample Inferential Statistics | PSY317L Guides" />
  
  
  

<meta name="author" content="James P. Curley &amp; Tyler M. Milewski" />


<meta name="date" content="2020-06-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="confidence-intervals.html"/>
<link rel="next" href="two-sample-inferential-statistics.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome to PSY317!</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-this-book-includes-and-what-it-doesnt"><i class="fa fa-check"></i><b>1.1</b> What this book includes and what it doesn't</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#how-to-use-this-guide"><i class="fa fa-check"></i><b>1.2</b> How to use this guide</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.4</b> References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r"><i class="fa fa-check"></i><b>1.5</b> Other places to find help about R</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r-and-statistics"><i class="fa fa-check"></i><b>1.6</b> Other places to find help about R and Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#downloading-r"><i class="fa fa-check"></i><b>2.1</b> Downloading R</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#downloading-rstudio"><i class="fa fa-check"></i><b>2.2</b> Downloading RStudio</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#using-rcloud-instead-of-rstudio"><i class="fa fa-check"></i><b>2.3</b> Using RCloud instead of RStudio</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#the-rstudio-environment"><i class="fa fa-check"></i><b>2.5</b> The RStudio Environment</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#the-command-prompt"><i class="fa fa-check"></i><b>2.6</b> The Command Prompt</a></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#what-is-an-rscript-file"><i class="fa fa-check"></i><b>2.7</b> What is an RScript File?</a></li>
<li class="chapter" data-level="2.8" data-path="introduction.html"><a href="introduction.html#what-are-packages"><i class="fa fa-check"></i><b>2.8</b> What are Packages</a></li>
<li class="chapter" data-level="2.9" data-path="introduction.html"><a href="introduction.html#project-folders-and-working-directories"><i class="fa fa-check"></i><b>2.9</b> Project Folders and Working Directories</a></li>
<li class="chapter" data-level="2.10" data-path="introduction.html"><a href="introduction.html#where-to-get-help-for-r-stuff"><i class="fa fa-check"></i><b>2.10</b> Where to Get Help for R stuff</a></li>
<li class="chapter" data-level="2.11" data-path="introduction.html"><a href="introduction.html#quitting-r"><i class="fa fa-check"></i><b>2.11</b> Quitting R</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic-syntax.html"><a href="basic-syntax.html"><i class="fa fa-check"></i><b>3</b> Basic Syntax</a><ul>
<li class="chapter" data-level="3.1" data-path="basic-syntax.html"><a href="basic-syntax.html#boring-mathematical-stuff"><i class="fa fa-check"></i><b>3.1</b> Boring mathematical stuff</a></li>
<li class="chapter" data-level="3.2" data-path="basic-syntax.html"><a href="basic-syntax.html#assignment"><i class="fa fa-check"></i><b>3.2</b> assignment</a></li>
<li class="chapter" data-level="3.3" data-path="basic-syntax.html"><a href="basic-syntax.html#vectors"><i class="fa fa-check"></i><b>3.3</b> vectors</a></li>
<li class="chapter" data-level="3.4" data-path="basic-syntax.html"><a href="basic-syntax.html#characters"><i class="fa fa-check"></i><b>3.4</b> Characters</a></li>
<li class="chapter" data-level="3.5" data-path="basic-syntax.html"><a href="basic-syntax.html#naming-of-objects"><i class="fa fa-check"></i><b>3.5</b> Naming of objects</a></li>
<li class="chapter" data-level="3.6" data-path="basic-syntax.html"><a href="basic-syntax.html#logical-operators"><i class="fa fa-check"></i><b>3.6</b> Logical Operators</a></li>
<li class="chapter" data-level="3.7" data-path="basic-syntax.html"><a href="basic-syntax.html#some-things-that-are-useful-to-know."><i class="fa fa-check"></i><b>3.7</b> Some things that are useful to know.</a><ul>
<li class="chapter" data-level="3.7.1" data-path="basic-syntax.html"><a href="basic-syntax.html#tab-is-your-friend"><i class="fa fa-check"></i><b>3.7.1</b> Tab is your friend</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="basic-syntax.html"><a href="basic-syntax.html#error-messages"><i class="fa fa-check"></i><b>3.8</b> Error Messages</a></li>
<li class="chapter" data-level="3.9" data-path="basic-syntax.html"><a href="basic-syntax.html#functions"><i class="fa fa-check"></i><b>3.9</b> Functions</a></li>
<li class="chapter" data-level="3.10" data-path="basic-syntax.html"><a href="basic-syntax.html#chaining-syntax"><i class="fa fa-check"></i><b>3.10</b> Chaining Syntax</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Introduction to Data Carpentry</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#introduction-to-dataframes"><i class="fa fa-check"></i><b>4.1</b> Introduction to Dataframes</a></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#tidyverse"><i class="fa fa-check"></i><b>4.2</b> tidyverse</a></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#filter"><i class="fa fa-check"></i><b>4.3</b> filter()</a></li>
<li class="chapter" data-level="4.4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#select"><i class="fa fa-check"></i><b>4.4</b> select()</a></li>
<li class="chapter" data-level="4.5" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#mutate"><i class="fa fa-check"></i><b>4.5</b> mutate()</a></li>
<li class="chapter" data-level="4.6" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#arrange"><i class="fa fa-check"></i><b>4.6</b> arrange()</a></li>
<li class="chapter" data-level="4.7" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#wide-vs-long-data"><i class="fa fa-check"></i><b>4.7</b> Wide vs Long Data</a></li>
<li class="chapter" data-level="4.8" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#joins"><i class="fa fa-check"></i><b>4.8</b> Joins</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>5</b> Data Visualization</a><ul>
<li class="chapter" data-level="5.1" data-path="data-visualization.html"><a href="data-visualization.html#intro-to-ggplot2"><i class="fa fa-check"></i><b>5.1</b> Intro to ggplot2</a></li>
<li class="chapter" data-level="5.2" data-path="data-visualization.html"><a href="data-visualization.html#histogram"><i class="fa fa-check"></i><b>5.2</b> Histogram</a></li>
<li class="chapter" data-level="5.3" data-path="data-visualization.html"><a href="data-visualization.html#scatter"><i class="fa fa-check"></i><b>5.3</b> Scatter</a></li>
<li class="chapter" data-level="5.4" data-path="data-visualization.html"><a href="data-visualization.html#line"><i class="fa fa-check"></i><b>5.4</b> Line</a></li>
<li class="chapter" data-level="5.5" data-path="data-visualization.html"><a href="data-visualization.html#boxplot"><i class="fa fa-check"></i><b>5.5</b> Boxplot</a></li>
<li class="chapter" data-level="5.6" data-path="data-visualization.html"><a href="data-visualization.html#bar-graphs"><i class="fa fa-check"></i><b>5.6</b> Bar Graphs</a></li>
<li class="chapter" data-level="5.7" data-path="data-visualization.html"><a href="data-visualization.html#all-the-extras"><i class="fa fa-check"></i><b>5.7</b> All the extras</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>6</b> Descriptives</a><ul>
<li class="chapter" data-level="6.1" data-path="descriptives.html"><a href="descriptives.html#sample-vs-population"><i class="fa fa-check"></i><b>6.1</b> Sample vs Population</a></li>
<li class="chapter" data-level="6.2" data-path="descriptives.html"><a href="descriptives.html#basic-descriptives"><i class="fa fa-check"></i><b>6.2</b> Basic Descriptives</a></li>
<li class="chapter" data-level="6.3" data-path="descriptives.html"><a href="descriptives.html#mean-median-and-mode"><i class="fa fa-check"></i><b>6.3</b> Mean, Median, and Mode</a></li>
<li class="chapter" data-level="6.4" data-path="descriptives.html"><a href="descriptives.html#standard-deviation"><i class="fa fa-check"></i><b>6.4</b> Standard Deviation</a><ul>
<li class="chapter" data-level="6.4.1" data-path="descriptives.html"><a href="descriptives.html#average-deviation"><i class="fa fa-check"></i><b>6.4.1</b> Average Deviation</a></li>
<li class="chapter" data-level="6.4.2" data-path="descriptives.html"><a href="descriptives.html#standard-deviation-1"><i class="fa fa-check"></i><b>6.4.2</b> Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="descriptives.html"><a href="descriptives.html#standard-error"><i class="fa fa-check"></i><b>6.5</b> Standard Error</a></li>
<li class="chapter" data-level="6.6" data-path="descriptives.html"><a href="descriptives.html#median-and-inter-quartile-ranges"><i class="fa fa-check"></i><b>6.6</b> Median and Inter-quartile Ranges</a><ul>
<li class="chapter" data-level="6.6.1" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>6.6.1</b> Median</a></li>
<li class="chapter" data-level="6.6.2" data-path="descriptives.html"><a href="descriptives.html#iqrs"><i class="fa fa-check"></i><b>6.6.2</b> IQRs</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="descriptives.html"><a href="descriptives.html#descriptives-for-groups"><i class="fa fa-check"></i><b>6.7</b> Descriptives for Groups</a></li>
<li class="chapter" data-level="6.8" data-path="descriptives.html"><a href="descriptives.html#comparing-population-and-sample-means"><i class="fa fa-check"></i><b>6.8</b> Comparing population and sample means</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>7</b> Distributions</a><ul>
<li class="chapter" data-level="7.1" data-path="distributions.html"><a href="distributions.html#what-is-a-distribution"><i class="fa fa-check"></i><b>7.1</b> What is a distribution ?</a><ul>
<li class="chapter" data-level="7.1.1" data-path="distributions.html"><a href="distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>7.1.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="7.1.2" data-path="distributions.html"><a href="distributions.html#bimodal-distribution"><i class="fa fa-check"></i><b>7.1.2</b> Bimodal Distribution</a></li>
<li class="chapter" data-level="7.1.3" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>7.1.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="7.1.4" data-path="distributions.html"><a href="distributions.html#standard-normal-distribution"><i class="fa fa-check"></i><b>7.1.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="7.1.5" data-path="distributions.html"><a href="distributions.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>7.1.5</b> Skewness and Kurtosis</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="distributions.html"><a href="distributions.html#z-scores"><i class="fa fa-check"></i><b>7.2</b> z-scores</a><ul>
<li class="chapter" data-level="7.2.1" data-path="distributions.html"><a href="distributions.html#z-scores-in-samples."><i class="fa fa-check"></i><b>7.2.1</b> z-scores in samples.</a></li>
<li class="chapter" data-level="7.2.2" data-path="distributions.html"><a href="distributions.html#using-z-scores-to-determine-probabilities"><i class="fa fa-check"></i><b>7.2.2</b> Using z-scores to determine probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="distributions.html"><a href="distributions.html#what-is-a-sampling-distribution"><i class="fa fa-check"></i><b>7.3</b> What is a Sampling Distribution ?</a><ul>
<li class="chapter" data-level="7.3.1" data-path="distributions.html"><a href="distributions.html#sample-size-and-the-sampling-distribution"><i class="fa fa-check"></i><b>7.3.1</b> Sample Size and the Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="distributions.html"><a href="distributions.html#central-limit-theorem."><i class="fa fa-check"></i><b>7.4</b> Central Limit Theorem.</a></li>
<li class="chapter" data-level="7.5" data-path="distributions.html"><a href="distributions.html#sampling-distribution-problems."><i class="fa fa-check"></i><b>7.5</b> Sampling distribution problems.</a></li>
<li class="chapter" data-level="7.6" data-path="distributions.html"><a href="distributions.html#the-t-distribution"><i class="fa fa-check"></i><b>7.6</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>8</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="8.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#sample-means-as-estimates."><i class="fa fa-check"></i><b>8.1</b> Sample means as estimates.</a></li>
<li class="chapter" data-level="8.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-confidence-interval-with-z-distribution"><i class="fa fa-check"></i><b>8.2</b> Calculating a confidence interval with z-distribution</a><ul>
<li class="chapter" data-level="8.2.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges"><i class="fa fa-check"></i><b>8.2.1</b> Other Confidence Intervals ranges</a></li>
<li class="chapter" data-level="8.2.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-and-sample-size"><i class="fa fa-check"></i><b>8.2.2</b> Confidence Intervals and Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-with-t-distribution"><i class="fa fa-check"></i><b>8.3</b> Confidence Intervals with t-distribution</a></li>
<li class="chapter" data-level="8.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-t-distribution-confidence-interval"><i class="fa fa-check"></i><b>8.4</b> Calculating a t-distribution Confidence Interval</a><ul>
<li class="chapter" data-level="8.4.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#t-distribution-cis-and-sample-size."><i class="fa fa-check"></i><b>8.4.1</b> t-distribution CIs and sample size.</a></li>
<li class="chapter" data-level="8.4.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges-for-t-distribution"><i class="fa fa-check"></i><b>8.4.2</b> Other Confidence Intervals ranges for t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="confidence-intervals.html"><a href="confidence-intervals.html#comparing-cis-using-the-z--and-t-distributions"><i class="fa fa-check"></i><b>8.5</b> Comparing CIs using the z- and t-distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>9</b> One Sample Inferential Statistics</a><ul>
<li class="chapter" data-level="9.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-z-tests"><i class="fa fa-check"></i><b>9.1</b> One-sample Z-tests</a><ul>
<li class="chapter" data-level="9.1.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#sampling-distribution-recap"><i class="fa fa-check"></i><b>9.1.1</b> Sampling Distribution Recap</a></li>
<li class="chapter" data-level="9.1.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#calculating-p-values-for-z-test"><i class="fa fa-check"></i><b>9.1.2</b> Calculating p-values for z-test</a></li>
<li class="chapter" data-level="9.1.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#using-critical-values"><i class="fa fa-check"></i><b>9.1.3</b> Using critical values</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-t-tests"><i class="fa fa-check"></i><b>9.2</b> One-sample t-tests</a><ul>
<li class="chapter" data-level="9.2.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#critical-values-for-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.2.1</b> Critical values for the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#conducting-one-sample-t-tests-in-r"><i class="fa fa-check"></i><b>9.3</b> Conducting one-sample t-tests in R</a></li>
<li class="chapter" data-level="9.4" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#assumptions-of-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> Assumptions of the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>10</b> Two Sample Inferential Statistics</a><ul>
<li class="chapter" data-level="10.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#comparing-two-samples"><i class="fa fa-check"></i><b>10.1</b> Comparing two Samples</a></li>
<li class="chapter" data-level="10.2" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#independent-samples-t-test"><i class="fa fa-check"></i><b>10.2</b> Independent Samples t-test</a></li>
<li class="chapter" data-level="10.3" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#background-to-students-2-sample-t-test"><i class="fa fa-check"></i><b>10.3</b> Background to Student's 2 Sample t-test</a></li>
<li class="chapter" data-level="10.4" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#sampling-distribution-of-the-difference-in-sample-means"><i class="fa fa-check"></i><b>10.4</b> Sampling Distribution of the Difference in Sample Means</a></li>
<li class="chapter" data-level="10.5" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#pooled-standard-deviation"><i class="fa fa-check"></i><b>10.5</b> Pooled Standard Deviation</a></li>
<li class="chapter" data-level="10.6" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#confidence-interval-for-difference-in-means"><i class="fa fa-check"></i><b>10.6</b> Confidence Interval for Difference in Means</a></li>
<li class="chapter" data-level="10.7" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#conducting-student-t-test"><i class="fa fa-check"></i><b>10.7</b> Conducting Student t-test</a></li>
<li class="chapter" data-level="10.8" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#doing-student-t-test-in-r"><i class="fa fa-check"></i><b>10.8</b> Doing Student t-test in R</a></li>
<li class="chapter" data-level="10.9" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#effect-sizes"><i class="fa fa-check"></i><b>10.9</b> Effect Sizes</a></li>
<li class="chapter" data-level="10.10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#paired-t-tests"><i class="fa fa-check"></i><b>10.10</b> Paired t-tests</a></li>
<li class="chapter" data-level="10.11" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#confidence-intervals-with-paired-data"><i class="fa fa-check"></i><b>10.11</b> Confidence Intervals with Paired Data</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>11</b> Correlation</a><ul>
<li class="chapter" data-level="11.1" data-path="correlation.html"><a href="correlation.html#pearson-correlation"><i class="fa fa-check"></i><b>11.1</b> Pearson Correlation</a></li>
<li class="chapter" data-level="11.2" data-path="correlation.html"><a href="correlation.html#calculating-the-pearson-correlation-in-r"><i class="fa fa-check"></i><b>11.2</b> Calculating the Pearson Correlation in R</a></li>
<li class="chapter" data-level="11.3" data-path="correlation.html"><a href="correlation.html#cross-products"><i class="fa fa-check"></i><b>11.3</b> Cross-products</a></li>
<li class="chapter" data-level="11.4" data-path="correlation.html"><a href="correlation.html#conducting-a-pearson-correlation-test"><i class="fa fa-check"></i><b>11.4</b> Conducting a Pearson Correlation Test</a></li>
<li class="chapter" data-level="11.5" data-path="correlation.html"><a href="correlation.html#assumptions-of-pearsons-correlation"><i class="fa fa-check"></i><b>11.5</b> Assumptions of Pearson's Correlation</a></li>
<li class="chapter" data-level="11.6" data-path="correlation.html"><a href="correlation.html#confidence-intervals-for-r"><i class="fa fa-check"></i><b>11.6</b> Confidence Intervals for R</a></li>
<li class="chapter" data-level="11.7" data-path="correlation.html"><a href="correlation.html#partial-correlations"><i class="fa fa-check"></i><b>11.7</b> Partial Correlations</a></li>
<li class="chapter" data-level="11.8" data-path="correlation.html"><a href="correlation.html#non-parametric-correlations"><i class="fa fa-check"></i><b>11.8</b> Non-parametric Correlations</a></li>
<li class="chapter" data-level="11.9" data-path="correlation.html"><a href="correlation.html#point-biserial-correlation"><i class="fa fa-check"></i><b>11.9</b> Point-Biserial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>12</b> Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="regression.html"><a href="regression.html#introduction-to-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Introduction to Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="regression.html"><a href="regression.html#a-and-b"><i class="fa fa-check"></i><b>12.2</b> a and b</a><ul>
<li class="chapter" data-level="12.2.1" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-in-r"><i class="fa fa-check"></i><b>12.2.1</b> How to calculate a and b in R</a></li>
<li class="chapter" data-level="12.2.2" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-by-hand"><i class="fa fa-check"></i><b>12.2.2</b> How to calculate a and b 'by hand'</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="regression.html"><a href="regression.html#residuals"><i class="fa fa-check"></i><b>12.3</b> Residuals</a><ul>
<li class="chapter" data-level="12.3.1" data-path="regression.html"><a href="regression.html#how-to-calculate-the-residuals"><i class="fa fa-check"></i><b>12.3.1</b> How to calculate the residuals</a></li>
<li class="chapter" data-level="12.3.2" data-path="regression.html"><a href="regression.html#visualizing-the-residuals"><i class="fa fa-check"></i><b>12.3.2</b> Visualizing the Residuals</a></li>
<li class="chapter" data-level="12.3.3" data-path="regression.html"><a href="regression.html#comparing-our-trendline-to-other-trendlines"><i class="fa fa-check"></i><b>12.3.3</b> Comparing our trendline to other trendlines</a></li>
<li class="chapter" data-level="12.3.4" data-path="regression.html"><a href="regression.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>12.3.4</b> Coefficient of Determination R2</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="regression.html"><a href="regression.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4</b> Standard Error of the Estimate</a><ul>
<li class="chapter" data-level="12.4.1" data-path="regression.html"><a href="regression.html#what-to-do-with-the-standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4.1</b> What to do with the Standard Error of the Estimate ?</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="regression.html"><a href="regression.html#goodness-of-fit-test---f-ratio"><i class="fa fa-check"></i><b>12.5</b> Goodness of Fit Test - F-ratio</a></li>
<li class="chapter" data-level="12.6" data-path="regression.html"><a href="regression.html#assumptions-of-linear-regression"><i class="fa fa-check"></i><b>12.6</b> Assumptions of Linear Regression</a><ul>
<li class="chapter" data-level="12.6.1" data-path="regression.html"><a href="regression.html#normality-of-residuals"><i class="fa fa-check"></i><b>12.6.1</b> Normality of Residuals</a></li>
<li class="chapter" data-level="12.6.2" data-path="regression.html"><a href="regression.html#linearity----"><i class="fa fa-check"></i><b>12.6.2</b> 2. Linearity ---</a></li>
<li class="chapter" data-level="12.6.3" data-path="regression.html"><a href="regression.html#homogeneity-of-variance-homoscedasticity"><i class="fa fa-check"></i><b>12.6.3</b> 3. Homogeneity of Variance / Homoscedasticity</a></li>
<li class="chapter" data-level="12.6.4" data-path="regression.html"><a href="regression.html#no-colinearity"><i class="fa fa-check"></i><b>12.6.4</b> No Colinearity</a></li>
<li class="chapter" data-level="12.6.5" data-path="regression.html"><a href="regression.html#unusual-datapoints"><i class="fa fa-check"></i><b>12.6.5</b> Unusual Datapoints</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="regression.html"><a href="regression.html#examining-individual-predictor-estimates"><i class="fa fa-check"></i><b>12.7</b> Examining individual predictor estimates</a><ul>
<li class="chapter" data-level="12.7.1" data-path="regression.html"><a href="regression.html#confidence-interval-of-b."><i class="fa fa-check"></i><b>12.7.1</b> 95% confidence interval of 'b'.</a></li>
<li class="chapter" data-level="12.7.2" data-path="regression.html"><a href="regression.html#standard-error-of-b"><i class="fa fa-check"></i><b>12.7.2</b> Standard Error of b</a></li>
<li class="chapter" data-level="12.7.3" data-path="regression.html"><a href="regression.html#calculating-95-confidence-interval-of-b-by-hand"><i class="fa fa-check"></i><b>12.7.3</b> Calculating 95% confidence interval of 'b' by hand</a></li>
<li class="chapter" data-level="12.7.4" data-path="regression.html"><a href="regression.html#signifcance-testing-b"><i class="fa fa-check"></i><b>12.7.4</b> Signifcance Testing b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="permutation-testing.html"><a href="permutation-testing.html"><i class="fa fa-check"></i><b>13</b> Permutation Testing</a><ul>
<li class="chapter" data-level="13.1" data-path="permutation-testing.html"><a href="permutation-testing.html#t-test-permutation"><i class="fa fa-check"></i><b>13.1</b> t-test Permutation</a></li>
<li class="chapter" data-level="13.2" data-path="permutation-testing.html"><a href="permutation-testing.html#correlation-coefficient-permutation-tests"><i class="fa fa-check"></i><b>13.2</b> Correlation Coefficient Permutation Tests</a></li>
<li class="chapter" data-level="13.3" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-test-for-a-paired-t-test"><i class="fa fa-check"></i><b>13.3</b> Permutation test for a Paired t-test</a></li>
<li class="chapter" data-level="13.4" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-tests-in-packages"><i class="fa fa-check"></i><b>13.4</b> Permutation tests in Packages</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PSY317L Guides</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="one-sample-inferential-statistics" class="section level1">
<h1><span class="header-section-number">9</span> One Sample Inferential Statistics</h1>
<p>The general question at hand with one-sample inferential tests, is that we wish to test the probability that our one sample of data comes from a population that has a true population mean <span class="math inline">\(\mu\)</span> that is equal to, greater than, or less than, some specific value.</p>
<div id="one-sample-z-tests" class="section level2">
<h2><span class="header-section-number">9.1</span> One-sample Z-tests</h2>
<div id="sampling-distribution-recap" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Sampling Distribution Recap</h3>
<p>In one-sample z-tests we are provided with the population mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. We then collect or are given one sample of data of size <span class="math inline">\(n\)</span>. From this sample, we calculate the sample mean <span class="math inline">\(\overline{x}\)</span>. The question then becomes, how likely were we to get a sample mean as large or as small as the sample that we got?</p>
<p>To answer this, we need to think in terms of the sampling distribution of sample means. We need to recognize that our one observed sample mean <span class="math inline">\(\overline{x}\)</span> is just one sample mean that we could have got from a sampling distribution of sample means.</p>
<p>For instance, look at the population below. This is a normally distributed population of IQ scores, with a population mean <span class="math inline">\(\mu = 100\)</span> and a population standard deviation <span class="math inline">\(\sigma = 15\)</span>.</p>
<div class="figure">
<img src="img/zt1.png" />

</div>
<p>Let's take a sample of size <span class="math inline">\(n=25\)</span> from this population, round the individual scores to 1dp, and get the mean of the sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
samp1 &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">25</span>, <span class="dt">mean=</span><span class="dv">100</span>, <span class="dt">sd=</span><span class="dv">15</span>),<span class="dv">1</span>)
samp1</code></pre></div>
<pre><code>##  [1]  90.6 102.8  87.5 123.9 104.9  87.7 107.3 111.1 108.6  95.4 122.7 105.8
## [13]  90.7  66.8 116.9  99.3  99.8 114.2 112.3 108.9 113.8 111.7 101.1  70.2
## [25] 109.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp1)</code></pre></div>
<pre><code>## [1] 102.532</code></pre>
<p>Our one observed sample has a sample mean of <span class="math inline">\(\overline{x}=102.5\)</span></p>
<p>If we repeated this step and got a second sample of <span class="math inline">\(n=25\)</span>, we could get another sample mean <span class="math inline">\(\overline{x}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp2 &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">25</span>, <span class="dt">mean=</span><span class="dv">100</span>, <span class="dt">sd=</span><span class="dv">15</span>),<span class="dv">1</span>)
samp2</code></pre></div>
<pre><code>##  [1]  99.2  97.7  77.9  92.8 106.3 120.4  98.5 105.8  99.2  79.3  93.8  94.1
## [13]  99.1 116.5 111.4  97.5  96.2 110.5 108.3  89.7  89.4 105.5 111.5  98.3
## [25] 113.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp2)</code></pre></div>
<pre><code>## [1] 100.484</code></pre>
<p>This time the sample mean is <span class="math inline">\(\overline{x}=100.5\)</span>.</p>
<p>If you remember back to section xxx.xxx, if we were to repeat this process thousands and thousands of times, we would get a <strong>sampling distribution of sample means</strong>. We could visualize all of our sample means from many thousands of samples in a histogram, which shows the shape of the sampling distribution:</p>
<div class="figure">
<img src="img/zt2.png" />

</div>
<p>Because of Central Limit Theorem (see section xxx.xxx) then this sampling distribution is normally distributed and it's mean <span class="math inline">\(\mu_{\overline{x}}\)</span> is equal to the population mean <span class="math inline">\(\mu\)</span>. Therefore <span class="math inline">\(\mu_{\overline{x}}=10.0\)</span>. We also know the standard deviation of this sampling distribution, also known as the <strong>standard error</strong> as it can be calculated by: <span class="math inline">\(\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}\)</span>. Therefore, the sampling distribution standard deviation is <span class="math inline">\(\sigma_{\overline{x}}=3.0\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sem &lt;-<span class="st"> </span><span class="dv">15</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>)
sem</code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>Because this sampling distribution is normally distributed, we can determine how far away from the mean any sample mean is in terms of how many standard deviations from the mean they are. For instance, our first sample we got a mean of <span class="math inline">\(\overline{x}=102.5\)</span>. How many sampling distribution standard deviations is this from the mean of the sampling distribution? We can use an amended z-score formula (see section xxx.xxx) to determine this:</p>
<p><span class="math inline">\(z = \frac{\overline{x} - \mu_{\overline{x}}}{\sigma_{\overline{x}}}\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="fl">102.5</span> -<span class="st"> </span><span class="fl">100.0</span>) /<span class="st"> </span><span class="dv">3</span></code></pre></div>
<pre><code>## [1] 0.8333333</code></pre>
<p>So, our sample mean of <span class="math inline">\(\overline{x}=102.5\)</span> is 0.833 standard deviations above the mean. Because our sampling distribution is normally distributed, then we can visualize how far above the mean this value is on the standard normal curve as well as on the sampling distribution:</p>
<div class="figure">
<img src="img/zt3.png" />

</div>
<p>If we were asked what proportion of sample means were at least as big as 102.5, then we'd be interested in knowning what proportion of sample means are to the right of the red lines above. We can calculate the area under a standard normal curve to the left of any <code>z</code> value in R using <code>pnorm()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="fl">0.833</span>)</code></pre></div>
<pre><code>## [1] 0.7975776</code></pre>
<p>To find the proportion of the curve to the right of the red line (i.e. the proportion of sample means that are greater than 102.5), we just subtract this value from 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pnorm</span>(<span class="fl">0.833</span>)</code></pre></div>
<pre><code>## [1] 0.2024224</code></pre>
<p>So 20.2% of samples have a sample mean greater than 102.5. In this situation, our one sample mean was not therefore that unusual or surprising.</p>
</div>
<div id="calculating-p-values-for-z-test" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Calculating p-values for z-test</h3>
<p><strong>One-tailed tests</strong></p>
<p>We can calculate the proportion of sample means that are greater or less than any value. For instance, if we were interested in whether a new reading program in a school boosted the IQ of subjects. We might take a sample of 25 of these students and measure their IQ. If we got a sample mean of <span class="math inline">\(\overline_{x} = 105.7\)</span>, we may wish to test whether this value is surprsingly large given that the population of IQ has a <span class="math inline">\(\mu=100.0\)</span> and <span class="math inline">\(\sigma=15.0\)</span>.</p>
<p>If we were to formally write this in hypothesis terms, it would look like this:</p>
<p><span class="math inline">\(H_{0}: \mu \le 100.0\)</span> <span class="math inline">\(H_{1}: \mu &gt; 100.0\)</span></p>
<p>This is saying that the alternative hypothesis <span class="math inline">\(H_{1}\)</span> is that our sample of 25 come from a population whose mean is greater than 100.0. The null hypothesis that we are testing is that they come from a population whose mean is equal to or less than 100.0.</p>
<p>For the test, we assume with the null hypothesis that our sample did indeed come from a population with <span class="math inline">\(\mu=100.0\)</span> and <span class="math inline">\(\sigma=15.0\)</span>. We already calculated above that the sampling distribution of sample means for <span class="math inline">\(n=25\)</span> has a <span class="math inline">\(\mu_{\overline{x}}=100.0\)</span> and <span class="math inline">\(\sigma_{\overline{x}}=3.0\)</span>. How unusual is our one observed sample mean of <span class="math inline">\(\overline{x}=105.7\)</span>? We need to calculate this in terms of z:</p>
<p><span class="math inline">\(z = \frac{\overline{x} - \mu_{\overline{x}}}{\sigma_{\overline{x}}}\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="fl">105.7</span> -<span class="st"> </span><span class="fl">100.0</span>) /<span class="st"> </span><span class="dv">3</span></code></pre></div>
<pre><code>## [1] 1.9</code></pre>
<p>This suggests that our observed sample mean is 1.9 sampling distribution standard deviations away from the mean of the sampling distribution. We next need to work out what proportion of sample means are greater than this. That is akin to the red shaded area below:</p>
<div class="figure">
<img src="img/zt4.png" />

</div>
<p>We can do this with <code>pnorm()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pnorm</span>(<span class="fl">1.9</span>)</code></pre></div>
<pre><code>## [1] 0.02871656</code></pre>
<p>This shows that only 2.9% of sample means are greater than our observed sample mean, given the population data of <span class="math inline">\(\mu=100.0\)</span> and <span class="math inline">\(\sigma=15.0\)</span>. Therefore, our observed sample mean is quite surprising. We can write the likelihood of getting this sample mean as <code>p = 0.029</code>. Because we a priori had a prediction as to the direction of the mean in our sample (we predicted it to be higher than the population mean of 100.0), then we have in fact just done a one-tailed test. If we decide that any p-values that are below <code>p=0.05</code> are 'significant', then we can say that our reading program in the school has a 'signficant effect' on improving IQ scores.</p>
<p><strong>Two-tailed tests</strong></p>
<p>What instead we had implement a reading program that was quite radical, and we were not sure whether it would be successful or not. We were interested in seeing whether it could increase or decrease IQ? In this situation, we do not have a direction of prediction, and we set up our hypotheses slightly differently:</p>
<p><span class="math inline">\(H_{0}: \mu = 100.0\)</span> <span class="math inline">\(H_{1}: \mu \ne 100.0\)</span></p>
<p>Here, we are interested in whether our observed sample mean of <span class="math inline">\(\mu=105.7\)</span> could have come from a sampling distribution with a population mean of $_{=100.0} or not. The initial steps are the same. We calculate how unusual our observed sample mean was in terms of standard deviations away from the mean of the sampling distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="fl">105.7</span> -<span class="st"> </span><span class="fl">100.0</span>) /<span class="st"> </span><span class="dv">3</span></code></pre></div>
<pre><code>## [1] 1.9</code></pre>
<p>It's still 1.9 standard deviations away.</p>
<p>What we have to now thing about, is that because we did not predict the direction of the difference, we have to double our p-value. This is to account for the fact that in terms of 'surprising' results, we are interested in results that are more extreme than 1.9 standard deviations either side of the mean - i.e. the sum of the shaded area below:</p>
<div class="figure">
<img src="img/zt5.png" />

</div>
<p>Therefore, our p-value for this test is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span> *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span><span class="kw">pnorm</span>(<span class="fl">1.9</span>))</code></pre></div>
<pre><code>## [1] 0.05743312</code></pre>
<p>Which is <code>p=0.057</code>. This would suggest that our reading program did not sufficiently shift the population IQ scores in our school away from 100.0, as our p-value is greater than 0.05.</p>
<p>But is this really true? It is important to consider two things here. One, it is important what initial hypothesis that you set up. If you have a strong a priori belief in the directionality of the hypothesis, then you are justified in doing a one-tailed z-test and using that p-value from that. Secondly, the difference between <code>p=0.029</code> and <code>p=0.057</code> in this case isn't that great. We shouldn't be overly focused on the cut-off value of <code>p=0.05</code> as the criteria as to whether our results are significant or not significant. We should see the bigger picture, that our p-value is just one piece of information as to how different our sample of data is from the population that we believe it came from.</p>
</div>
<div id="using-critical-values" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Using critical values</h3>
<p>In the preceding section, we ran one-tailed and two-tailed z-tests and calculated exact p-values. It is reasonably straightforward to do this in R. There is no reason not to use that approach. We prefer it. However, most often in introductory textbooks, a different approach is used. In this approach, the step of calculating the p-value is missed out. Instead, you are asked to just determine whether your obseved z-value is more extreme than you'd expect by chance. &quot;By chance&quot; in this context means, that your observed z-value is less than 5% likely to occur.</p>
<p><strong>One-tailed z-test</strong></p>
<p>The population mean for SAT scores is <span class="math inline">\(\mu=500\)</span> with a population standard deviation <span class="math inline">\(\sigma=100\)</span>. A tutoring company says that they improve SAT scores. A random sample of 12 students who took the tutoring program had a sample mean of <span class="math inline">\(\overline(x)=551\)</span>. Let's test whether this sample mean came from the population.</p>
<p><span class="math inline">\(H_{0}: \mu \le 500.0\)</span> <span class="math inline">\(H_{1}: \mu &gt; 500.0\)</span></p>
<p>Next, we calculate the mean and standard deviation of the sampling distribution for a sample size <span class="math inline">\(n=12\)</span>. For the test, we assume that the sampling distribution mean <span class="math inline">\(\mu_{\overline{x}}=500.0\)</span> - i.e. is the same as the population mean. The standard deviation of the sampling distribution <span class="math inline">\(\sigma_{\overline{x}} = 28.87\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">100</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">12</span>)</code></pre></div>
<pre><code>## [1] 28.86751</code></pre>
<p>Next, we work out how many sampling distribution standard deviations from the sampling distribution mean is our observed sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">551</span> -<span class="st"> </span><span class="dv">500</span>) /<span class="st"> </span><span class="fl">28.86751</span></code></pre></div>
<pre><code>## [1] 1.766692</code></pre>
<p>This shows that our observed sample mean of <span class="math inline">\(\overline{x}=551\)</span> is 1.77 standard deviations above the mean of the sampling distribution. We could convert this to a p-value and calculate precisely how many sample means are larger than this in the sampling distribution. Instead, we'll take a different approach:</p>
<p>Because this sampling distribution is approximately normal, we can determine how many standard deviations above the mean you'd have to be to be larger than 95% of all samples.</p>
<div class="figure">
<img src="img/zt6.png" />

</div>
<p>It turns out that <span class="math inline">\(z=1.645\)</span> is the value of <span class="math inline">\(z\)</span> that leaves 5% in the right hand tail. Therefore, any value of <span class="math inline">\(z\)</span> greater than <span class="math inline">\(z=1.645\)</span> will be 'unusually' large and have a p-value of less than 0.05. If we were dealing with sample means that were surprisingly small (so a one-tailed test where we are predicting that the sample mean comes from a population with a mean that is smaller than the population mean), then we are looking for <span class="math inline">\(z\)</span> values that are lower than <span class="math inline">\(z=-1.645\)</span>.</p>
<p>We can work out where these 'critical values' are in R using the <code>qnorm()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(<span class="fl">0.95</span>) <span class="co"># leaves 5% in right of tail</span></code></pre></div>
<pre><code>## [1] 1.644854</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(<span class="fl">0.05</span>) <span class="co"># leaves 5% in left of tail</span></code></pre></div>
<pre><code>## [1] -1.644854</code></pre>
<p>If we get back to our observed <span class="math inline">\(z\)</span> value of <span class="math inline">\(z=1.77\)</span>, we can overlay this over the graph above like this:</p>
<div class="figure">
<img src="img/zt7.png" />

</div>
<p>As you can see, our observed <span class="math inline">\(z\)</span> value is more extreme than the 'critical value' of <span class="math inline">\(z=1.645\)</span>. We say that our observed value is therefore in the 'region of rejection' and we can therefore reject the null hypothesis with a p-value of <span class="math inline">\(p&lt;0.05\)</span> and accept the alternate hypothesis that our sample comes from a population with a population mean that is greater than 500. In other words, the tutoring program appears to have a population mean of SAT scroes greater than 500.</p>
<p><strong>Two-tailed z-test</strong></p>
<p>With the one-tailed z-test, we see that our critical values of z are <span class="math inline">\(z=1.645\)</span> for situations in which we are testing whether our sample mean is unexpectedly large, or <span class="math inline">\(z=-1.645\)</span> for situations in which we are testing that our sample mean is unexpectedly small. In a two-tailed situation, we are testing whether our sample mean is unexpectedly large <em>or</em> small. Because we still want only 5% of sample means to be in this 'unexpectedly' large or small category, this time we need a value of <span class="math inline">\(z\)</span> that leaves a total of 5% in the ends of both tails of the normal distribution. This is the same as leaving 2.5% in each tail:</p>
<div class="figure">
<img src="img/zt8.png" />

</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(.<span class="dv">975</span>)  <span class="co"># leaves 2.5% in right tail</span></code></pre></div>
<pre><code>## [1] 1.959964</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qnorm</span>(.<span class="dv">025</span>)  <span class="co"># leaves 2.5% in left tail</span></code></pre></div>
<pre><code>## [1] -1.959964</code></pre>
<p>Let's illustrate this a bit further with the following example. Say we have a bakery that makes cupcakes. The population mean weight of cupcakes is <span class="math inline">\(\mu=6.5\)</span> ounces, with a standard deviation of <span class="math inline">\(\sigma=0.15\)</span> ounces. A customer wants to test if a new cupcake variety is heavier or lighter than 6.5 ounces. They purchase a random sample of 10 cupcakes and find that the sample mean is <span class="math inline">\(\overline(x)=6.42\)</span> ounces.</p>
<p>If we were to conduct a two-tailed test, to test if the population mean that our sample come from is equal to 6.5 ounces or not, then our hypotheses would be:</p>
<p><span class="math inline">\(H_{0}: \mu = 6.5\)</span> <span class="math inline">\(H_{1}: \mu \ne 6.5\)</span></p>
<p>We need to calculate the z-score for our sample mean, to determine how many standard deviations of the sampling distribution it is away from the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sem &lt;-<span class="st"> </span><span class="fl">0.15</span> /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">10</span>)  <span class="co">#standard deviation of the sampling distribution</span>

z &lt;-<span class="st"> </span>(<span class="fl">6.42</span> -<span class="st"> </span><span class="fl">6.5</span>) /<span class="st"> </span>sem  <span class="co"># how many SD away from the mean is our sample</span>

z</code></pre></div>
<pre><code>## [1] -1.686548</code></pre>
<p>We can overlay this observed value of <span class="math inline">\(z\)</span> onto our standard normal curve like this:</p>
<div class="figure">
<img src="img/zt9.png" />

</div>
<p>Our value of <span class="math inline">\(z=-1.69\)</span> is therefore not inside either of the regions of rejection. This means that we do not have sufficient evidence to reject the null hypothesis that our sample comes from a population with mean of equal to 6.5.</p>
</div>
</div>
<div id="one-sample-t-tests" class="section level2">
<h2><span class="header-section-number">9.2</span> One-sample t-tests</h2>
<p>As with confidence intervals based on the <span class="math inline">\(z\)</span>-distribution (the standard normal curve), the major issue with z-tests is that they require you to know the population mean <span class="math inline">\(\sigma\)</span> to perform the calculations. This is almost never the case - with exceptions like standardized tests including IQ and SAT that are designed to have specific means and standard deviations.</p>
<p>If you wish to test whether your observed sample mean is likely or not to come from a population with a given mean, what approach should you take when you do not know <span class="math inline">\(\sigma\)</span>? As with confidence intervals, the approach we take is to use the <span class="math inline">\(t\)</span>-distribution.</p>
<p>In this situation, because we don't know our population standard deviation <span class="math inline">\(\sigma\)</span>, we have to estimate it using the sample standard deviation <span class="math inline">\(s\)</span>. Further, because our sampling distribution may not be precisely normal given this estimation, we say that it comes from a <span class="math inline">\(t\)</span>-distribution. <span class="math inline">\(t\)</span>-distributions have slightly heavier tails than the normal distribution.</p>
<p><strong>One-tailed t-test example</strong></p>
<p>Let's illustrate the steps we take in a one-sample t-test with an example. These steps are identical to the two-tailed t-test up until we calculate the p-value.</p>
<p>The population mean number of words spoken by two year olds by their 2nd birthday is <span class="math inline">\(\mu=50\)</span> words and this is normally distributed. We don't know the population standard deviation <span class="math inline">\(\sigma\)</span>. A researcher wanted to investigate if reading to children increases their word knowledge. They collected data from 12 children (<span class="math inline">\(n=12\)</span>) who were read to for at least two hours every day. These are the number of words spoken by the 12 children:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">45</span>, <span class="dv">53</span>, <span class="dv">71</span>, <span class="dv">35</span>, <span class="dv">51</span>, <span class="dv">59</span>, <span class="dv">49</span>, <span class="dv">55</span>, <span class="dv">78</span>, <span class="dv">27</span>, <span class="dv">66</span>, <span class="dv">59</span>)
x</code></pre></div>
<pre><code>##  [1] 45 53 71 35 51 59 49 55 78 27 66 59</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(x)  </code></pre></div>
<pre><code>## [1] 54</code></pre>
<p>Our one observed sample mean <span class="math inline">\(\overline{x}=54\)</span>. This is higher than 50, but is it meaningfully higher? If we were to formalize our hypothesis for this test, we would write:</p>
<p><span class="math inline">\(H_{0}: \mu \le 50.0\)</span> <span class="math inline">\(H_{1}: \mu &gt; 50.0\)</span></p>
<p>We are testing whether our sample was likely to have come from a population with mean 50 or less (null hypothesis), or if it was more likely to come from a population with a mean of greater than 50.</p>
<p>The first step is to think about the sampling distribution. We need to recognize that our one observed sample mean is just one sample mean that we theoretically could have got from a sample distribution. Under the null hypothesis, we are going to assume that the mean of our sampling distribution <span class="math inline">\(\mu_{\overline{x}}\)</span> is equivalent to the population mean <span class="math inline">\(\mu\)</span>. Therefore, <span class="math inline">\(\mu_{\overline{x}}=50.0\)</span></p>
<p>Next, we need to calculate the standard deviation of the sampling distribution of sample means for n=12 (i.e. the standard error). As we do not know the population standard deviation <span class="math inline">\(\sigma\)</span>, we estimate this by using the following formula:</p>
<p><span class="math inline">\(\sigma_{\overline{x}} = \frac{s}{\sqrt{n}}\)</span></p>
<p>Therefore, our estimate of the standard error is <span class="math inline">\(\sigma_{\overline{x}}=4.14\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sem &lt;-<span class="st"> </span><span class="kw">sd</span>(x) /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">12</span>)
sem</code></pre></div>
<pre><code>## [1] 4.143268</code></pre>
<p>Now that we know both the mean and standard deviation of the t-shaped sampling distribution, next we need to calculate how many standard deviations from this mean is our one observed sample mean. We calculate that using the formula that is similar to the <span class="math inline">\(z\)</span> formula:</p>
<p><span class="math inline">\(t = \frac{\overline{x} - \mu_{\overline{x}}}{\sigma_{\overline{x}}}\)</span></p>
<p>Our sample mean of <span class="math inline">\(\overline{x}=54\)</span> has a <span class="math inline">\(t\)</span> value of <span class="math inline">\(t = 0.965\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">54-50</span>) /<span class="st"> </span>sem</code></pre></div>
<pre><code>## [1] 0.9654216</code></pre>
<p>This means that it is approximately 0.965 standard deviations higher than the mean. We can calculate the proportion of sample means in the sampling distribution that are higher than our one observed sample mean by calculating the area under the curve to the right of our <span class="math inline">\(t\)</span>-value. Remember, that our sampling distribution is <span class="math inline">\(t\)</span>-shaped and has 11 degrees of freedom. The degrees of freedom are <span class="math inline">\(df = n-1\)</span> for a one-sample t-test.</p>
<div class="figure">
<img src="img/tt1.png" />

</div>
<p>To determine what proportion of the curve lies to the right of <span class="math inline">\(t=0.965\)</span> we can use <code>pt()</code> which calculates the proportion to the left of the given value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pt</span>(<span class="fl">0.965</span>, <span class="dt">df=</span><span class="dv">11</span>)</code></pre></div>
<pre><code>## [1] 0.8223595</code></pre>
<p>So, this tells us that for a <span class="math inline">\(t\)</span>-distribution with 11 degrees of freedom, that 82.2% of values are to the left (lower than) of this value. That means that 17.8% of values are to the right, or 17.8% of sample means that could be drawn from the sampling distribution with a mean of 54 and standard deviation of 4.14 will be higher than our observed sample mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pt</span>(<span class="fl">0.965</span>, <span class="dt">df=</span><span class="dv">11</span>)</code></pre></div>
<pre><code>## [1] 0.1776405</code></pre>
<p>We can also get this value by setting <code>lower.tail = FALSE</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pt</span>(<span class="fl">0.965</span>, <span class="dt">df=</span><span class="dv">11</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>) </code></pre></div>
<pre><code>## [1] 0.1776405</code></pre>
<p>Because we made a prediction as to the direction of the hypothesis - i.e. we predicted that reading to children would <em>increase</em> the population mean, we are effectively running a one-tailed test. Our p-value is simply <span class="math inline">\(p=0.177\)</span>. As we use an alpha level of 0.05 (a p-value of 0.05 as our critical value), this suggests that we do not have sufficient evidence to suggest that our sample of 12 children have a mean value that comes from a population with a mean that is greater than 50. We do not reject our null hypothesis.</p>
<p><strong>Two-tailed t-test example</strong></p>
<p>There are 20 psychology students in Dr. Zeppo's class. Here are their scores on a test, as well as their sample mean <span class="math inline">\(\overline{x}\)</span> and sample standard deviation <span class="math inline">\(s\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">zeppo &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">60</span>,<span class="dv">60</span>,<span class="dv">64</span>,<span class="dv">66</span>,<span class="dv">66</span>,<span class="dv">67</span>,<span class="dv">69</span>,<span class="dv">70</span>,<span class="dv">74</span>,<span class="dv">76</span>,<span class="dv">76</span>,<span class="dv">77</span>,<span class="dv">79</span>,<span class="dv">79</span>,<span class="dv">79</span>,<span class="dv">81</span>,<span class="dv">82</span>,<span class="dv">82</span>,<span class="dv">89</span>)

zeppo</code></pre></div>
<pre><code>##  [1] 50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(zeppo)  <span class="co"># 20 - there are 20 students in the sample.</span></code></pre></div>
<pre><code>## [1] 20</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(zeppo)  <span class="co"># the mean of the sample is 72.3</span></code></pre></div>
<pre><code>## [1] 72.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(zeppo)  <span class="co"># the sample SD is 9.52</span></code></pre></div>
<pre><code>## [1] 9.520615</code></pre>
<p>Historically, students in this class get a score of 65 points on this test - that is the population mean <span class="math inline">\(\mu\)</span>. Dr Zeppo wishes to test if this one sample (one class) has a mean (technically 'comes from a population with a mean') that is different to 65. We would write out this hypothesis like this:</p>
<p><span class="math inline">\(H_{0}: \mu = 65.0\)</span> <span class="math inline">\(H_{1}: \mu \ne 65.0\)</span></p>
<p>As with all of these tests, our first job is to recognize that our one sample mean is just one sample mean that we 'theoretically' could have got from lots of samples of size <span class="math inline">\(n=20\)</span>. All of those sample means together are referred to as the sampling distribution of sample means. Under the null hypothesis, we assume that the mean of the sampling distribution of sample means <span class="math inline">\(\mu_{\overline{x}}=65\)</span>, i.e. it is equivalent to the population mean <span class="math inline">\(\mu\)</span>. Next, we have to estimate the standard deviation of the sampling distribution of sample means, i.e. the standard error <span class="math inline">\(\sigma_{\overline{x}}\)</span>. We do this using the same formula as before:</p>
<p><span class="math inline">\(\sigma_{\overline{x}} = \frac{s}{\sqrt{n}}\)</span></p>
<p>So, our standard error is <span class="math inline">\(\sigma_{\overline{x}} = 2.13\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sem &lt;-<span class="st"> </span><span class="kw">sd</span>(zeppo) /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">20</span>)
sem</code></pre></div>
<pre><code>## [1] 2.128874</code></pre>
<p>Following this, we need to calculate how expected or unexpected our one sample mean <span class="math inline">\(\overline{x}\)</span> was. We do this by calculating it in terms of how many sampling distribution standard deviations is it away from the sampling distribution mean. We use the formula:</p>
<p><span class="math inline">\(t = \frac{\overline{x} - \mu_{\overline{x}}}{\sigma_{\overline{x}}}\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">mean</span>(zeppo) -<span class="st"> </span><span class="dv">65</span>) /<span class="st"> </span>sem</code></pre></div>
<pre><code>## [1] 3.429042</code></pre>
<p>Our observed sample mean of <span class="math inline">\(\overline{x} = 72.3\)</span> is 3.43 sampling distribution standard deviations from the sampling distribution mean.</p>
<p>We can picture this as follows:</p>
<div class="figure">
<img src="img/tt2.png" />

</div>
<p>To calculate our p-value for our 2-tailed t-test, we need to calculate not just the area underneath the curve with values of <span class="math inline">\(t\)</span> greater than our observed <span class="math inline">\(t=3.43\)</span>, but also the values under the curve with <span class="math inline">\(t\)</span> values that are more negative than <span class="math inline">\(t=-3.43\)</span>. This is because in a 2-tailed test, we need to test for the probability of getting a <span class="math inline">\(t\)</span>-value as large in both directions. Our p-value can be calculated using <code>pt()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pt</span>(<span class="fl">3.43</span>, <span class="dt">df =</span> <span class="dv">19</span>, <span class="dt">lower.tail =</span> F) +<span class="st"> </span><span class="kw">pt</span>(-<span class="fl">3.43</span>, <span class="dt">df =</span> <span class="dv">19</span>) </code></pre></div>
<pre><code>## [1] 0.002807258</code></pre>
<p>Or alternatively, we could just multiply those values greater than <span class="math inline">\(t=3.43\)</span> by 2:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pt</span>(<span class="fl">3.43</span>, <span class="dt">df =</span> <span class="dv">19</span>, <span class="dt">lower.tail =</span> F) *<span class="st"> </span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.002807258</code></pre>
<p>Either way, we can see that our p-values is <span class="math inline">\(p = 0.003\)</span>, which tells us that our observed mean of <span class="math inline">\(\overline{x}=72.3\)</span> is quite unlikely to have come from a distribution with a population mean <span class="math inline">\(\mu=65\)</span>. This leads us to rejecting our null hypothesis and accepting the alternative, that these psychology students come from a population with a mean that is greater than 65.</p>
<div id="critical-values-for-the-one-sample-t-test" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Critical values for the one-sample t-test</h3>
<p>As with the z-test (see section xxx.xxx), instead of calculating the p-values for our observed values of <span class="math inline">\(t\)</span>, you can simply test whether your value of <span class="math inline">\(t\)</span> exceeds (in either the positive or negative direction) some 'critical value' of <span class="math inline">\(t\)</span>. Again, this approach was more often taken when it wasn't as easy to run computers to do these tests, so it seems a bit obsolete to do it this way. We recommend just doing it the way outlined above. Nevertheless, just for completeness, here is how to calculate these critical values of <span class="math inline">\(t\)</span>.</p>
<p>We'll use data from a different dataset. Here, we have the times taken to complete a crossword puzzle. We have a sample of <span class="math inline">\(n=10\)</span> subjects.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
xt &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/crosstimes.csv&quot;</span>)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   id = col_character(),
##   time1 = col_double(),
##   time2 = col_double(),
##   time3 = col_double(),
##   time4 = col_double(),
##   time5 = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xt$time3</code></pre></div>
<pre><code>##  [1] 15.04036 15.38213 15.70967 14.71215 12.18972 19.90511 20.00015 12.45357
##  [9] 12.86255 12.82682</code></pre>
<p>Let's say we wish to test whether this sample comes from a population with a mean of less than 16.0. We would be doing a one-tailed test and our hypotheses would look like this:</p>
<p><span class="math inline">\(H_{0}: \mu \ge 16.0\)</span> <span class="math inline">\(H_{1}: \mu &lt; 16.0\)</span></p>
<p>When using critical values, all the steps up to completing the observed <span class="math inline">\(t\)</span>-value are the same as before. So, we assume that our one sample mean <span class="math inline">\(\overline{x}\)</span> comes from a sampling distribution of sample means that has a mean equivalent to the population mean of 16. The standard deviation of this sampling distribution is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sem &lt;-<span class="st"> </span><span class="kw">sd</span>(xt$time3) /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">10</span>)
sem</code></pre></div>
<pre><code>## [1] 0.9027857</code></pre>
<p>Next, we calculate our observed <span class="math inline">\(t\)</span> which is a measure of how many sampling deviation standard deviations our observed sample mean is away from the mean of the sampling distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">mean</span>(xt$time3) -<span class="st"> </span><span class="dv">16</span>) /<span class="st"> </span>sem</code></pre></div>
<pre><code>## [1] -0.9878071</code></pre>
<p>Our observed <span class="math inline">\(t\)</span> values is <span class="math inline">\(t = -0.99\)</span>.</p>
<p>As we are conducting a one-tailed t-test, we need to think in terms of what value of <span class="math inline">\(t\)</span> leaves 5% in the tail for a t-distribution with 9 degrees of freedom. This is visualized below:</p>
<div class="figure">
<img src="img/tt3.png" />

</div>
<p>If our observed sample mean with a <span class="math inline">\(t\)</span>-value of -0.99 was unexpectedly small, then it would need to be in the region of rejection (red shaded area). This would mean that it was in the bottom 5% of sample means from such a distribution. The value of <span class="math inline">\(t\)</span> that is the boundary of the the lower 5% can be calculated using the <code>qt()</code> function like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.05</span>, <span class="dt">df=</span><span class="dv">9</span>)</code></pre></div>
<pre><code>## [1] -1.833113</code></pre>
<p>** Two-tailed tests **</p>
<p>For this example, let's use the <code>penguins</code> data. Say we are interested in the flipper length of male Adelie penguins on Biscoe island (a bit specific, but let's go with it), and wanted to know if their mean length was different from <span class="math inline">\(\mu = 188\)</span>. Our hypotheses would be:</p>
<p><span class="math inline">\(H_{0}: \mu = 188.0\)</span> <span class="math inline">\(H_{1}: \mu \ne 188.0\)</span></p>
<p>From our data, we can calculate our <span class="math inline">\(n\)</span> and sample mean <span class="math inline">\(\overline{x}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### Read in the Data Penguins
penguins &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/penguins.csv&quot;</span>)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   species = col_character(),
##   island = col_character(),
##   culmen_length_mm = col_double(),
##   culmen_depth_mm = col_double(),
##   flipper_length_mm = col_integer(),
##   body_mass_g = col_integer(),
##   sex = col_character()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># just look at the females.</span>
adelie &lt;-<span class="st"> </span>penguins %&gt;%<span class="st"> </span><span class="kw">filter</span>(species ==<span class="st"> &quot;Adelie&quot;</span>, sex ==<span class="st"> &quot;MALE&quot;</span>, island ==<span class="st"> &quot;Biscoe&quot;</span>)

<span class="kw">nrow</span>(adelie)</code></pre></div>
<pre><code>## [1] 22</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(adelie$flipper_length_mm)</code></pre></div>
<pre><code>## [1] 190.4091</code></pre>
<p>We can also represent these data as a boxplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(adelie, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">y =</span> flipper_length_mm)) +
<span class="st">  </span><span class="kw">geom_boxplot</span>() +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> .<span class="dv">1</span>, <span class="dt">size=</span><span class="dv">2</span>)+
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title.y=</span><span class="kw">element_blank</span>(),
        <span class="dt">axis.text.y=</span><span class="kw">element_blank</span>(),
        <span class="dt">axis.ticks.y=</span><span class="kw">element_blank</span>()) +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">187</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-346-1.png" width="576" /></p>
<p>Our sample of penguins has a sample size of <span class="math inline">\(n=22\)</span>, with a sample mean of <span class="math inline">\(\overline{x}=190.4\)</span>. Let's calculate our observed value of <span class="math inline">\(t\)</span> for this sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sem &lt;-<span class="st"> </span><span class="kw">sd</span>(adelie$flipper_length_mm) /<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">22</span>)

tobs &lt;-<span class="st"> </span>(<span class="kw">mean</span>(adelie$flipper_length_mm) -<span class="st"> </span><span class="dv">188</span>) /<span class="st"> </span>sem

tobs</code></pre></div>
<pre><code>## [1] 1.748218</code></pre>
<p>This means that our observed sample mean is 1.75 sample standard deviations above the sampling distribution mean.</p>
<p>In terms of critical regions, for a 2-tailed test, we need to know the values of <span class="math inline">\(t\)</span> that leave 2.5% in each tail for a <span class="math inline">\(t\)</span>-distribution with 21 degrees of freedom.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df =</span> <span class="dv">21</span>)</code></pre></div>
<pre><code>## [1] 2.079614</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(.<span class="dv">025</span>, <span class="dt">df =</span> <span class="dv">21</span>)</code></pre></div>
<pre><code>## [1] -2.079614</code></pre>
<div class="figure">
<img src="img/tt4.png" />

</div>
<p>For a <span class="math inline">\(t\)</span>-distribution with 21 degrees of freedom, the top 2.5% of t-values are greater than <span class="math inline">\(t=2.08\)</span>, whilst the lowest 2.5% of t-values are below <span class="math inline">\(t=-2.08\)</span>. Therefore, for us to reject the null hypothesis, our observed t-value needs to be higher than 2.08 or lower than -2.08. That would leave it in the region of rejection (red shaded areas above). Our observed t-value is <span class="math inline">\(t=1.75\)</span> which is not in these areas, so we cannot reject the null hypothesis. We do not have sufficient evidence to suggest that our penguins come from a population with a mean of <span class="math inline">\(\mu = 188\)</span>.</p>
</div>
</div>
<div id="conducting-one-sample-t-tests-in-r" class="section level2">
<h2><span class="header-section-number">9.3</span> Conducting one-sample t-tests in R</h2>
<p>Conducting one-sample t-tests in R is very straightforward.</p>
<p>First, let's consider the sample of 12 two-year olds and their word scores.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">45</span>, <span class="dv">53</span>, <span class="dv">71</span>, <span class="dv">35</span>, <span class="dv">51</span>, <span class="dv">59</span>, <span class="dv">49</span>, <span class="dv">55</span>, <span class="dv">78</span>, <span class="dv">27</span>, <span class="dv">66</span>, <span class="dv">59</span>)
<span class="kw">mean</span>(x)</code></pre></div>
<pre><code>## [1] 54</code></pre>
<p>To test whether this sample mean is likely to have come from a population with a mean greater than 50, we use <code>t.test()</code> in the following way:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(x, <span class="dt">mu =</span> <span class="dv">50</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)  <span class="co"># one-tailed test</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 0.96542, df = 11, p-value = 0.1775
## alternative hypothesis: true mean is greater than 50
## 95 percent confidence interval:
##  46.55917      Inf
## sample estimates:
## mean of x 
##        54</code></pre>
<p><code>mu</code> specifies the mean that we are testing against. <code>alternative = &quot;greater&quot;</code> states that it is a one-tailed test, where we are testing the prediction that the population mean is greater than 50. The output gives us the same observed t-value that we calculated by hand <span class="math inline">\(t=0.97\)</span>, the degrees of freedom and the p-value.</p>
<p>To conduct a two-tailed test, where we just make the prediction that the population mean that the sample came from is not equal to some value, we just drop the <code>alternative</code> argument. For instance, we can test whether the <code>zeppo</code> data come from a population with a mean equal to 65:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(zeppo, <span class="dt">mu =</span> <span class="dv">65</span>)  <span class="co"># two tailed test</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  zeppo
## t = 3.429, df = 19, p-value = 0.002813
## alternative hypothesis: true mean is not equal to 65
## 95 percent confidence interval:
##  67.84422 76.75578
## sample estimates:
## mean of x 
##      72.3</code></pre>
<p>Again, we see the same t-value, degrees of freedom and p-value as we calculated by hand. Also with a two-taield t-test we get the 95% confidence interval of the true population mean (see section xxxx.xxx).</p>
<p>Finally, if we wished to do a one-taile t-test where we were testing whether the sample came from a population with a population mean of less than some value, we would use <code>alternative = &quot;less&quot;</code>. For example, to test if the sample of puzzle competitors came from a population that completed their puzzles in less than 16 minutes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(xt$time3, <span class="dt">mu =</span> <span class="dv">16</span>, <span class="dt">alternative =</span> <span class="st">&quot;less&quot;</span>)</code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  xt$time3
## t = -0.98781, df = 9, p-value = 0.1745
## alternative hypothesis: true mean is less than 16
## 95 percent confidence interval:
##      -Inf 16.76313
## sample estimates:
## mean of x 
##  15.10822</code></pre>
</div>
<div id="assumptions-of-the-one-sample-t-test" class="section level2">
<h2><span class="header-section-number">9.4</span> Assumptions of the one-sample t-test</h2>
<p>The main assumptions of the one-sample t-test are that the observations should be independent of each other, and the values should be approximately normally distributed.</p>
<p>We can more formally test if our data come from a population that is approximately normally distributed using a Shapiro-Wilk test. This test essentially examines the distribution of our data, and determines the probability that it came from a normal distribution. We can perform this test in R using <code>shapiro.test()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(x)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.98341, p-value = 0.9938</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(zeppo)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  zeppo
## W = 0.96205, p-value = 0.5856</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(xt$time3)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  xt$time3
## W = 0.84497, p-value = 0.0506</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(adelie$flipper_length_mm)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  adelie$flipper_length_mm
## W = 0.96541, p-value = 0.6056</code></pre>
<p>As you can see, all four of the datasets that we have performed one-sample t-tests on have p-values for this test that are greater than p=0.05. This suggests that our data are approximately normally distributed. The p-value for the crossword puzzle times is very low <code>p=0.0501</code> which probably suggests that we should look at that data in more detail to be sure that our data are normally distributed.</p>
<p>If your data are not normally distributed, then one option is to perform a non-parametric alternative to the one-sample t-test. This test is called the one-sample Wilcoxon signed rank test. We will not go into the details of the test, but effectively it tests whether your sample is likely to have come from a population with a median of a specified value. It is run like this in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(xt$time3, <span class="dt">mu =</span> <span class="dv">16</span>, <span class="dt">alternative =</span> <span class="st">&quot;less&quot;</span>)</code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test
## 
## data:  xt$time3
## V = 19, p-value = 0.2158
## alternative hypothesis: true location is less than 16</code></pre>
<p>Again, with this test, we are looking for a p-value lower than 0.05 to reject the null hypothesis and accept the alternative that our sample comes from a population with a median of less than 16.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="confidence-intervals.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="two-sample-inferential-statistics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
