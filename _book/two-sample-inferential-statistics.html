<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Two Sample Inferential Statistics | PSY317L Guides</title>
  <meta name="description" content="10 Two Sample Inferential Statistics | PSY317L Guides" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Two Sample Inferential Statistics | PSY317L Guides" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Two Sample Inferential Statistics | PSY317L Guides" />
  
  
  

<meta name="author" content="James P. Curley &amp; Tyler M. Milewski" />


<meta name="date" content="2020-06-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="one-sample-inferential-statistics.html"/>
<link rel="next" href="correlation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome to PSY317!</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-this-book-includes-and-what-it-doesnt"><i class="fa fa-check"></i><b>1.1</b> What this book includes and what it doesn't</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#how-to-use-this-guide"><i class="fa fa-check"></i><b>1.2</b> How to use this guide</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.4</b> References</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r"><i class="fa fa-check"></i><b>1.5</b> Other places to find help about R</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#other-places-to-find-help-about-r-and-statistics"><i class="fa fa-check"></i><b>1.6</b> Other places to find help about R and Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#downloading-r"><i class="fa fa-check"></i><b>2.1</b> Downloading R</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#downloading-rstudio"><i class="fa fa-check"></i><b>2.2</b> Downloading RStudio</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#using-rcloud-instead-of-rstudio"><i class="fa fa-check"></i><b>2.3</b> Using RCloud instead of RStudio</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#the-rstudio-environment"><i class="fa fa-check"></i><b>2.5</b> The RStudio Environment</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#the-command-prompt"><i class="fa fa-check"></i><b>2.6</b> The Command Prompt</a></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#what-is-an-rscript-file"><i class="fa fa-check"></i><b>2.7</b> What is an RScript File?</a></li>
<li class="chapter" data-level="2.8" data-path="introduction.html"><a href="introduction.html#what-are-packages"><i class="fa fa-check"></i><b>2.8</b> What are Packages</a></li>
<li class="chapter" data-level="2.9" data-path="introduction.html"><a href="introduction.html#project-folders-and-working-directories"><i class="fa fa-check"></i><b>2.9</b> Project Folders and Working Directories</a></li>
<li class="chapter" data-level="2.10" data-path="introduction.html"><a href="introduction.html#where-to-get-help-for-r-stuff"><i class="fa fa-check"></i><b>2.10</b> Where to Get Help for R stuff</a></li>
<li class="chapter" data-level="2.11" data-path="introduction.html"><a href="introduction.html#quitting-r"><i class="fa fa-check"></i><b>2.11</b> Quitting R</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic-syntax.html"><a href="basic-syntax.html"><i class="fa fa-check"></i><b>3</b> Basic Syntax</a><ul>
<li class="chapter" data-level="3.1" data-path="basic-syntax.html"><a href="basic-syntax.html#boring-mathematical-stuff"><i class="fa fa-check"></i><b>3.1</b> Boring mathematical stuff</a></li>
<li class="chapter" data-level="3.2" data-path="basic-syntax.html"><a href="basic-syntax.html#assignment"><i class="fa fa-check"></i><b>3.2</b> assignment</a></li>
<li class="chapter" data-level="3.3" data-path="basic-syntax.html"><a href="basic-syntax.html#vectors"><i class="fa fa-check"></i><b>3.3</b> vectors</a></li>
<li class="chapter" data-level="3.4" data-path="basic-syntax.html"><a href="basic-syntax.html#characters"><i class="fa fa-check"></i><b>3.4</b> Characters</a></li>
<li class="chapter" data-level="3.5" data-path="basic-syntax.html"><a href="basic-syntax.html#naming-of-objects"><i class="fa fa-check"></i><b>3.5</b> Naming of objects</a></li>
<li class="chapter" data-level="3.6" data-path="basic-syntax.html"><a href="basic-syntax.html#logical-operators"><i class="fa fa-check"></i><b>3.6</b> Logical Operators</a></li>
<li class="chapter" data-level="3.7" data-path="basic-syntax.html"><a href="basic-syntax.html#some-things-that-are-useful-to-know."><i class="fa fa-check"></i><b>3.7</b> Some things that are useful to know.</a><ul>
<li class="chapter" data-level="3.7.1" data-path="basic-syntax.html"><a href="basic-syntax.html#tab-is-your-friend"><i class="fa fa-check"></i><b>3.7.1</b> Tab is your friend</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="basic-syntax.html"><a href="basic-syntax.html#error-messages"><i class="fa fa-check"></i><b>3.8</b> Error Messages</a></li>
<li class="chapter" data-level="3.9" data-path="basic-syntax.html"><a href="basic-syntax.html#functions"><i class="fa fa-check"></i><b>3.9</b> Functions</a></li>
<li class="chapter" data-level="3.10" data-path="basic-syntax.html"><a href="basic-syntax.html#chaining-syntax"><i class="fa fa-check"></i><b>3.10</b> Chaining Syntax</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Introduction to Data Carpentry</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#introduction-to-dataframes"><i class="fa fa-check"></i><b>4.1</b> Introduction to Dataframes</a></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#tidyverse"><i class="fa fa-check"></i><b>4.2</b> tidyverse</a></li>
<li class="chapter" data-level="4.3" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#filter"><i class="fa fa-check"></i><b>4.3</b> filter()</a></li>
<li class="chapter" data-level="4.4" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#select"><i class="fa fa-check"></i><b>4.4</b> select()</a></li>
<li class="chapter" data-level="4.5" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#mutate"><i class="fa fa-check"></i><b>4.5</b> mutate()</a></li>
<li class="chapter" data-level="4.6" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#arrange"><i class="fa fa-check"></i><b>4.6</b> arrange()</a></li>
<li class="chapter" data-level="4.7" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#wide-vs-long-data"><i class="fa fa-check"></i><b>4.7</b> Wide vs Long Data</a></li>
<li class="chapter" data-level="4.8" data-path="introduction-to-data-carpentry.html"><a href="introduction-to-data-carpentry.html#joins"><i class="fa fa-check"></i><b>4.8</b> Joins</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>5</b> Data Visualization</a><ul>
<li class="chapter" data-level="5.1" data-path="data-visualization.html"><a href="data-visualization.html#intro-to-ggplot2"><i class="fa fa-check"></i><b>5.1</b> Intro to ggplot2</a></li>
<li class="chapter" data-level="5.2" data-path="data-visualization.html"><a href="data-visualization.html#histogram"><i class="fa fa-check"></i><b>5.2</b> Histogram</a></li>
<li class="chapter" data-level="5.3" data-path="data-visualization.html"><a href="data-visualization.html#scatter"><i class="fa fa-check"></i><b>5.3</b> Scatter</a></li>
<li class="chapter" data-level="5.4" data-path="data-visualization.html"><a href="data-visualization.html#line"><i class="fa fa-check"></i><b>5.4</b> Line</a></li>
<li class="chapter" data-level="5.5" data-path="data-visualization.html"><a href="data-visualization.html#boxplot"><i class="fa fa-check"></i><b>5.5</b> Boxplot</a></li>
<li class="chapter" data-level="5.6" data-path="data-visualization.html"><a href="data-visualization.html#bar-graphs"><i class="fa fa-check"></i><b>5.6</b> Bar Graphs</a></li>
<li class="chapter" data-level="5.7" data-path="data-visualization.html"><a href="data-visualization.html#all-the-extras"><i class="fa fa-check"></i><b>5.7</b> All the extras</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>6</b> Descriptives</a><ul>
<li class="chapter" data-level="6.1" data-path="descriptives.html"><a href="descriptives.html#sample-vs-population"><i class="fa fa-check"></i><b>6.1</b> Sample vs Population</a></li>
<li class="chapter" data-level="6.2" data-path="descriptives.html"><a href="descriptives.html#basic-descriptives"><i class="fa fa-check"></i><b>6.2</b> Basic Descriptives</a></li>
<li class="chapter" data-level="6.3" data-path="descriptives.html"><a href="descriptives.html#mean-median-and-mode"><i class="fa fa-check"></i><b>6.3</b> Mean, Median, and Mode</a></li>
<li class="chapter" data-level="6.4" data-path="descriptives.html"><a href="descriptives.html#standard-deviation"><i class="fa fa-check"></i><b>6.4</b> Standard Deviation</a><ul>
<li class="chapter" data-level="6.4.1" data-path="descriptives.html"><a href="descriptives.html#average-deviation"><i class="fa fa-check"></i><b>6.4.1</b> Average Deviation</a></li>
<li class="chapter" data-level="6.4.2" data-path="descriptives.html"><a href="descriptives.html#standard-deviation-1"><i class="fa fa-check"></i><b>6.4.2</b> Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="descriptives.html"><a href="descriptives.html#standard-error"><i class="fa fa-check"></i><b>6.5</b> Standard Error</a></li>
<li class="chapter" data-level="6.6" data-path="descriptives.html"><a href="descriptives.html#median-and-inter-quartile-ranges"><i class="fa fa-check"></i><b>6.6</b> Median and Inter-quartile Ranges</a><ul>
<li class="chapter" data-level="6.6.1" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>6.6.1</b> Median</a></li>
<li class="chapter" data-level="6.6.2" data-path="descriptives.html"><a href="descriptives.html#iqrs"><i class="fa fa-check"></i><b>6.6.2</b> IQRs</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="descriptives.html"><a href="descriptives.html#descriptives-for-groups"><i class="fa fa-check"></i><b>6.7</b> Descriptives for Groups</a></li>
<li class="chapter" data-level="6.8" data-path="descriptives.html"><a href="descriptives.html#comparing-population-and-sample-means"><i class="fa fa-check"></i><b>6.8</b> Comparing population and sample means</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>7</b> Distributions</a><ul>
<li class="chapter" data-level="7.1" data-path="distributions.html"><a href="distributions.html#what-is-a-distribution"><i class="fa fa-check"></i><b>7.1</b> What is a distribution ?</a><ul>
<li class="chapter" data-level="7.1.1" data-path="distributions.html"><a href="distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>7.1.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="7.1.2" data-path="distributions.html"><a href="distributions.html#bimodal-distribution"><i class="fa fa-check"></i><b>7.1.2</b> Bimodal Distribution</a></li>
<li class="chapter" data-level="7.1.3" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>7.1.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="7.1.4" data-path="distributions.html"><a href="distributions.html#standard-normal-distribution"><i class="fa fa-check"></i><b>7.1.4</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="7.1.5" data-path="distributions.html"><a href="distributions.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>7.1.5</b> Skewness and Kurtosis</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="distributions.html"><a href="distributions.html#z-scores"><i class="fa fa-check"></i><b>7.2</b> z-scores</a><ul>
<li class="chapter" data-level="7.2.1" data-path="distributions.html"><a href="distributions.html#z-scores-in-samples."><i class="fa fa-check"></i><b>7.2.1</b> z-scores in samples.</a></li>
<li class="chapter" data-level="7.2.2" data-path="distributions.html"><a href="distributions.html#using-z-scores-to-determine-probabilities"><i class="fa fa-check"></i><b>7.2.2</b> Using z-scores to determine probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="distributions.html"><a href="distributions.html#what-is-a-sampling-distribution"><i class="fa fa-check"></i><b>7.3</b> What is a Sampling Distribution ?</a><ul>
<li class="chapter" data-level="7.3.1" data-path="distributions.html"><a href="distributions.html#sample-size-and-the-sampling-distribution"><i class="fa fa-check"></i><b>7.3.1</b> Sample Size and the Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="distributions.html"><a href="distributions.html#central-limit-theorem."><i class="fa fa-check"></i><b>7.4</b> Central Limit Theorem.</a></li>
<li class="chapter" data-level="7.5" data-path="distributions.html"><a href="distributions.html#sampling-distribution-problems."><i class="fa fa-check"></i><b>7.5</b> Sampling distribution problems.</a></li>
<li class="chapter" data-level="7.6" data-path="distributions.html"><a href="distributions.html#the-t-distribution"><i class="fa fa-check"></i><b>7.6</b> The t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>8</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="8.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#sample-means-as-estimates."><i class="fa fa-check"></i><b>8.1</b> Sample means as estimates.</a></li>
<li class="chapter" data-level="8.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-confidence-interval-with-z-distribution"><i class="fa fa-check"></i><b>8.2</b> Calculating a confidence interval with z-distribution</a><ul>
<li class="chapter" data-level="8.2.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges"><i class="fa fa-check"></i><b>8.2.1</b> Other Confidence Intervals ranges</a></li>
<li class="chapter" data-level="8.2.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-and-sample-size"><i class="fa fa-check"></i><b>8.2.2</b> Confidence Intervals and Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-with-t-distribution"><i class="fa fa-check"></i><b>8.3</b> Confidence Intervals with t-distribution</a></li>
<li class="chapter" data-level="8.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-a-t-distribution-confidence-interval"><i class="fa fa-check"></i><b>8.4</b> Calculating a t-distribution Confidence Interval</a><ul>
<li class="chapter" data-level="8.4.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#t-distribution-cis-and-sample-size."><i class="fa fa-check"></i><b>8.4.1</b> t-distribution CIs and sample size.</a></li>
<li class="chapter" data-level="8.4.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#other-confidence-intervals-ranges-for-t-distribution"><i class="fa fa-check"></i><b>8.4.2</b> Other Confidence Intervals ranges for t-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="confidence-intervals.html"><a href="confidence-intervals.html#comparing-cis-using-the-z--and-t-distributions"><i class="fa fa-check"></i><b>8.5</b> Comparing CIs using the z- and t-distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>9</b> One Sample Inferential Statistics</a><ul>
<li class="chapter" data-level="9.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-z-tests"><i class="fa fa-check"></i><b>9.1</b> One-sample Z-tests</a><ul>
<li class="chapter" data-level="9.1.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#sampling-distribution-recap"><i class="fa fa-check"></i><b>9.1.1</b> Sampling Distribution Recap</a></li>
<li class="chapter" data-level="9.1.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#calculating-p-values-for-z-test"><i class="fa fa-check"></i><b>9.1.2</b> Calculating p-values for z-test</a></li>
<li class="chapter" data-level="9.1.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#using-critical-values"><i class="fa fa-check"></i><b>9.1.3</b> Using critical values</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#one-sample-t-tests"><i class="fa fa-check"></i><b>9.2</b> One-sample t-tests</a><ul>
<li class="chapter" data-level="9.2.1" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#critical-values-for-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.2.1</b> Critical values for the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#conducting-one-sample-t-tests-in-r"><i class="fa fa-check"></i><b>9.3</b> Conducting one-sample t-tests in R</a></li>
<li class="chapter" data-level="9.4" data-path="one-sample-inferential-statistics.html"><a href="one-sample-inferential-statistics.html#assumptions-of-the-one-sample-t-test"><i class="fa fa-check"></i><b>9.4</b> Assumptions of the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html"><i class="fa fa-check"></i><b>10</b> Two Sample Inferential Statistics</a><ul>
<li class="chapter" data-level="10.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#comparing-two-samples"><i class="fa fa-check"></i><b>10.1</b> Comparing two Samples</a></li>
<li class="chapter" data-level="10.2" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#independent-samples-t-test"><i class="fa fa-check"></i><b>10.2</b> Independent Samples t-test</a></li>
<li class="chapter" data-level="10.3" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#sampling-distribution-of-the-difference-in-sample-means"><i class="fa fa-check"></i><b>10.3</b> Sampling Distribution of the Difference in Sample Means</a><ul>
<li class="chapter" data-level="10.3.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#visualizing-the-sampling-distribution"><i class="fa fa-check"></i><b>10.3.1</b> Visualizing the Sampling Distribution</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#pooled-standard-deviation"><i class="fa fa-check"></i><b>10.4</b> Pooled Standard Deviation</a></li>
<li class="chapter" data-level="10.5" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#theory-behind-students-t-test"><i class="fa fa-check"></i><b>10.5</b> Theory behind Student's t-test</a></li>
<li class="chapter" data-level="10.6" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#confidence-interval-for-difference-in-means"><i class="fa fa-check"></i><b>10.6</b> Confidence Interval for Difference in Means</a></li>
<li class="chapter" data-level="10.7" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#conducting-the-student-t-test-in-r"><i class="fa fa-check"></i><b>10.7</b> Conducting the Student t-test in R</a></li>
<li class="chapter" data-level="10.8" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#assumptions-of-the-independent-t-test"><i class="fa fa-check"></i><b>10.8</b> Assumptions of the Independent t-test</a></li>
<li class="chapter" data-level="10.9" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#welchs-t-test"><i class="fa fa-check"></i><b>10.9</b> Welch's t-test</a></li>
<li class="chapter" data-level="10.10" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#effect-size-for-independent-two-sample-t-tests"><i class="fa fa-check"></i><b>10.10</b> Effect Size for Independent two sample t-tests:</a></li>
<li class="chapter" data-level="10.11" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#paired-t-tests"><i class="fa fa-check"></i><b>10.11</b> Paired t-tests</a><ul>
<li class="chapter" data-level="10.11.1" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#the-paired-t-test-is-a-one-sample-t-test"><i class="fa fa-check"></i><b>10.11.1</b> The paired t-test is a one-sample t-test</a></li>
<li class="chapter" data-level="10.11.2" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#one-tailed-paired-t-tests"><i class="fa fa-check"></i><b>10.11.2</b> One-tailed paired t-tests</a></li>
<li class="chapter" data-level="10.11.3" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#calculating-effect-sizes"><i class="fa fa-check"></i><b>10.11.3</b> Calculating effect sizes</a></li>
</ul></li>
<li class="chapter" data-level="10.12" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#non-parametric-alternatives-for-independent-t-tests"><i class="fa fa-check"></i><b>10.12</b> Non-parametric Alternatives for Independent t-tests</a></li>
<li class="chapter" data-level="10.13" data-path="two-sample-inferential-statistics.html"><a href="two-sample-inferential-statistics.html#non-parametric-alternatives-to-the-two-sample-t-tests"><i class="fa fa-check"></i><b>10.13</b> Non-parametric Alternatives to the Two Sample t-tests</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>11</b> Correlation</a><ul>
<li class="chapter" data-level="11.1" data-path="correlation.html"><a href="correlation.html#pearson-correlation"><i class="fa fa-check"></i><b>11.1</b> Pearson Correlation</a></li>
<li class="chapter" data-level="11.2" data-path="correlation.html"><a href="correlation.html#calculating-the-pearson-correlation-in-r"><i class="fa fa-check"></i><b>11.2</b> Calculating the Pearson Correlation in R</a></li>
<li class="chapter" data-level="11.3" data-path="correlation.html"><a href="correlation.html#cross-products"><i class="fa fa-check"></i><b>11.3</b> Cross-products</a></li>
<li class="chapter" data-level="11.4" data-path="correlation.html"><a href="correlation.html#conducting-a-pearson-correlation-test"><i class="fa fa-check"></i><b>11.4</b> Conducting a Pearson Correlation Test</a></li>
<li class="chapter" data-level="11.5" data-path="correlation.html"><a href="correlation.html#assumptions-of-pearsons-correlation"><i class="fa fa-check"></i><b>11.5</b> Assumptions of Pearson's Correlation</a></li>
<li class="chapter" data-level="11.6" data-path="correlation.html"><a href="correlation.html#confidence-intervals-for-r"><i class="fa fa-check"></i><b>11.6</b> Confidence Intervals for R</a></li>
<li class="chapter" data-level="11.7" data-path="correlation.html"><a href="correlation.html#partial-correlations"><i class="fa fa-check"></i><b>11.7</b> Partial Correlations</a></li>
<li class="chapter" data-level="11.8" data-path="correlation.html"><a href="correlation.html#non-parametric-correlations"><i class="fa fa-check"></i><b>11.8</b> Non-parametric Correlations</a></li>
<li class="chapter" data-level="11.9" data-path="correlation.html"><a href="correlation.html#point-biserial-correlation"><i class="fa fa-check"></i><b>11.9</b> Point-Biserial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>12</b> Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="regression.html"><a href="regression.html#introduction-to-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Introduction to Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="regression.html"><a href="regression.html#a-and-b"><i class="fa fa-check"></i><b>12.2</b> a and b</a><ul>
<li class="chapter" data-level="12.2.1" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-in-r"><i class="fa fa-check"></i><b>12.2.1</b> How to calculate a and b in R</a></li>
<li class="chapter" data-level="12.2.2" data-path="regression.html"><a href="regression.html#how-to-calculate-a-and-b-by-hand"><i class="fa fa-check"></i><b>12.2.2</b> How to calculate a and b 'by hand'</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="regression.html"><a href="regression.html#residuals"><i class="fa fa-check"></i><b>12.3</b> Residuals</a><ul>
<li class="chapter" data-level="12.3.1" data-path="regression.html"><a href="regression.html#how-to-calculate-the-residuals"><i class="fa fa-check"></i><b>12.3.1</b> How to calculate the residuals</a></li>
<li class="chapter" data-level="12.3.2" data-path="regression.html"><a href="regression.html#visualizing-the-residuals"><i class="fa fa-check"></i><b>12.3.2</b> Visualizing the Residuals</a></li>
<li class="chapter" data-level="12.3.3" data-path="regression.html"><a href="regression.html#comparing-our-trendline-to-other-trendlines"><i class="fa fa-check"></i><b>12.3.3</b> Comparing our trendline to other trendlines</a></li>
<li class="chapter" data-level="12.3.4" data-path="regression.html"><a href="regression.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>12.3.4</b> Coefficient of Determination R2</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="regression.html"><a href="regression.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4</b> Standard Error of the Estimate</a><ul>
<li class="chapter" data-level="12.4.1" data-path="regression.html"><a href="regression.html#what-to-do-with-the-standard-error-of-the-estimate"><i class="fa fa-check"></i><b>12.4.1</b> What to do with the Standard Error of the Estimate ?</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="regression.html"><a href="regression.html#goodness-of-fit-test---f-ratio"><i class="fa fa-check"></i><b>12.5</b> Goodness of Fit Test - F-ratio</a></li>
<li class="chapter" data-level="12.6" data-path="regression.html"><a href="regression.html#assumptions-of-linear-regression"><i class="fa fa-check"></i><b>12.6</b> Assumptions of Linear Regression</a><ul>
<li class="chapter" data-level="12.6.1" data-path="regression.html"><a href="regression.html#normality-of-residuals"><i class="fa fa-check"></i><b>12.6.1</b> Normality of Residuals</a></li>
<li class="chapter" data-level="12.6.2" data-path="regression.html"><a href="regression.html#linearity----"><i class="fa fa-check"></i><b>12.6.2</b> 2. Linearity ---</a></li>
<li class="chapter" data-level="12.6.3" data-path="regression.html"><a href="regression.html#homogeneity-of-variance-homoscedasticity"><i class="fa fa-check"></i><b>12.6.3</b> 3. Homogeneity of Variance / Homoscedasticity</a></li>
<li class="chapter" data-level="12.6.4" data-path="regression.html"><a href="regression.html#no-colinearity"><i class="fa fa-check"></i><b>12.6.4</b> No Colinearity</a></li>
<li class="chapter" data-level="12.6.5" data-path="regression.html"><a href="regression.html#unusual-datapoints"><i class="fa fa-check"></i><b>12.6.5</b> Unusual Datapoints</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="regression.html"><a href="regression.html#examining-individual-predictor-estimates"><i class="fa fa-check"></i><b>12.7</b> Examining individual predictor estimates</a><ul>
<li class="chapter" data-level="12.7.1" data-path="regression.html"><a href="regression.html#confidence-interval-of-b."><i class="fa fa-check"></i><b>12.7.1</b> 95% confidence interval of 'b'.</a></li>
<li class="chapter" data-level="12.7.2" data-path="regression.html"><a href="regression.html#standard-error-of-b"><i class="fa fa-check"></i><b>12.7.2</b> Standard Error of b</a></li>
<li class="chapter" data-level="12.7.3" data-path="regression.html"><a href="regression.html#calculating-95-confidence-interval-of-b-by-hand"><i class="fa fa-check"></i><b>12.7.3</b> Calculating 95% confidence interval of 'b' by hand</a></li>
<li class="chapter" data-level="12.7.4" data-path="regression.html"><a href="regression.html#signifcance-testing-b"><i class="fa fa-check"></i><b>12.7.4</b> Signifcance Testing b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="permutation-testing.html"><a href="permutation-testing.html"><i class="fa fa-check"></i><b>13</b> Permutation Testing</a><ul>
<li class="chapter" data-level="13.1" data-path="permutation-testing.html"><a href="permutation-testing.html#t-test-permutation"><i class="fa fa-check"></i><b>13.1</b> t-test Permutation</a></li>
<li class="chapter" data-level="13.2" data-path="permutation-testing.html"><a href="permutation-testing.html#correlation-coefficient-permutation-tests"><i class="fa fa-check"></i><b>13.2</b> Correlation Coefficient Permutation Tests</a></li>
<li class="chapter" data-level="13.3" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-test-for-a-paired-t-test"><i class="fa fa-check"></i><b>13.3</b> Permutation test for a Paired t-test</a></li>
<li class="chapter" data-level="13.4" data-path="permutation-testing.html"><a href="permutation-testing.html#permutation-tests-in-packages"><i class="fa fa-check"></i><b>13.4</b> Permutation tests in Packages</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PSY317L Guides</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="two-sample-inferential-statistics" class="section level1">
<h1><span class="header-section-number">10</span> Two Sample Inferential Statistics</h1>
<p>blah</p>
<div id="comparing-two-samples" class="section level2">
<h2><span class="header-section-number">10.1</span> Comparing two Samples</h2>
<p>blah</p>
<div class="figure">
<img src="img/compare_dists.png" />

</div>
<p>blah</p>
<div class="figure">
<img src="img/compare_dists1.png" alt="compare 2 distributions" />
<p class="caption">compare 2 distributions</p>
</div>
<p>blah</p>
</div>
<div id="independent-samples-t-test" class="section level2">
<h2><span class="header-section-number">10.2</span> Independent Samples t-test</h2>
<p>Gather data from two samples..... is there a difference between these samples in their means? Well if they are samples, then there more than often will be a difference. For instance, x x x x,. Only if their sample means were precisely the same is there no difference. The more interesting bigger question is.... could these two indepednent samples theoretically have come from the same population... ie could we have taken our two samples at random from the same population..... or do we have sufficient evidence to suggest that they come from two separate populations that differ in their true population means.</p>
<p>focused on identifying if there is a difference in means between the populations..... to do this.....</p>
<p>could get 95% CIs of each..... or a 95% CI of the difference in means from our one observed sample mean difference.... or run t-tests...</p>
<p>two types... types Welch's .... Student's.....</p>
</div>
<div id="sampling-distribution-of-the-difference-in-sample-means" class="section level2">
<h2><span class="header-section-number">10.3</span> Sampling Distribution of the Difference in Sample Means</h2>
<p>For a two-sample <span class="math inline">\(t\)</span>-test, the sampling distribution we use is different to the sampling distribution for a one-sample <span class="math inline">\(t\)</span>-test. In a one-sample test, we are focused on the sampling distribution of sample means. In a two-sample test we are focused on the <em>sampling distribution of the differences in sample means</em>.</p>
<div id="visualizing-the-sampling-distribution" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Visualizing the Sampling Distribution</h3>
<p>Let's try to illustrate this by looking at the sampling distributions we could get when we draw samples from two populations. In the first example, the populations differ in their means. In the second example, the populations do not differ in their means.</p>
<p>** Difference in Population Means **</p>
<p>Say we have two normally distributed populations, A and B and each has 500,000 subjects. A has a population mean <span class="math inline">\(\mu = 15\)</span>, and a population standard deviation <span class="math inline">\(\sigma = 2.5\)</span>. B has a population mean <span class="math inline">\(\mu = 12\)</span>, and a population SD <span class="math inline">\(\sigma = 2.5\)</span>.</p>
<p>We can generate these population by simulation in R like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)

A &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">500000</span>, <span class="dt">mean =</span> <span class="dv">15</span>, <span class="dt">sd =</span> <span class="fl">2.5</span>)
B &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">500000</span>, <span class="dt">mean =</span> <span class="dv">12</span>, <span class="dt">sd =</span> <span class="fl">2.5</span>)</code></pre></div>
<p>We can check their mean and standard deviation, and we see that the standard deviations are identical at 2.5, but the population means differ by 3:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># and their summary descriptive stats</span>
<span class="kw">mean</span>(A)   </code></pre></div>
<pre><code>## [1] 14.99879</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(A)     </code></pre></div>
<pre><code>## [1] 2.500036</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(B)   </code></pre></div>
<pre><code>## [1] 12.00144</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(B)    </code></pre></div>
<pre><code>## [1] 2.500893</code></pre>
<p>We can visualize these population distributions as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(A, B) %&gt;%
<span class="st">  </span><span class="kw">pivot_longer</span>(<span class="dv">1</span>:<span class="dv">2</span>) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>value, <span class="dt">fill=</span>name)) +
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Comparison of Population A vs B&quot;</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">15</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">1</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">12</span>, <span class="dt">color=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>) +
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">2</span>,<span class="dv">26</span>,<span class="dv">2</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-357-1.png" width="672" /></p>
<p>We can calculate the true difference in population means <span class="math inline">\(\mu_{A} - \mu_{B}\)</span> to be 2.997.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(A) -<span class="st"> </span><span class="kw">mean</span>(B) </code></pre></div>
<pre><code>## [1] 2.997348</code></pre>
<p>If we knew nothing about the populations of A and B, but were interested in whether they differed in their population means, then we might take a sample of A and a sample of B and compare their sample means. In this example, we are going to take a sample of size <span class="math inline">\(n=17\)</span> from population A, and a sample of size <span class="math inline">\(n=20\)</span> from population B. These sample sizes were picked more or less at random - they can be the same sized samples, or they can be different sizes.</p>
<p>Let's take our samples, and then examine the differences in our sample means:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>) <span class="co"># so we get the same values</span>

a1 &lt;-<span class="st"> </span><span class="kw">sample</span>(A, <span class="dv">17</span>)
a1</code></pre></div>
<pre><code>##  [1] 10.94866 12.16234 17.07043 14.24800 18.32036 13.53377 16.75155 12.33184
##  [9] 14.57132 13.99580 18.42332 14.02078 17.01337 16.13922 17.08173 12.27186
## [17] 16.63431</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b1 &lt;-<span class="st"> </span><span class="kw">sample</span>(B, <span class="dv">20</span>)
b1</code></pre></div>
<pre><code>##  [1] 12.537514 14.522155 12.748609 12.434654  9.656833 12.989337 13.932283
##  [8]  9.149013 12.879667 15.671584  9.702438 12.187646 10.221455 11.629551
## [15] 10.486143 12.363354 10.939839  7.645530 17.179484 14.750259</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(a1)</code></pre></div>
<pre><code>## [1] 15.03051</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(b1)</code></pre></div>
<pre><code>## [1] 12.18137</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(a1) -<span class="st"> </span><span class="kw">mean</span>(b1)  </code></pre></div>
<pre><code>## [1] 2.849142</code></pre>
<p>Here we find that the difference in means between our sample from population A and our sample from population B is 2.85. In other words, <span class="math inline">\(\overline{x}_{A} - \overline{x}_{B} = 2.85\)</span> This is pretty close to the true difference in population means (although we're currently pretending that we don't know that difference).</p>
<p>We could do this again. Let's select another sample from population A of size 17, and another sample from population B of size 20:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a2 &lt;-<span class="st"> </span><span class="kw">sample</span>(A, <span class="dv">17</span>)
a2</code></pre></div>
<pre><code>##  [1] 16.60520 17.48478 13.34281 12.84191 14.55227 17.46490 11.16910 15.54571
##  [9] 13.82853 13.37768 15.89125 16.73950 16.60102 16.58975 17.68252 15.97616
## [17] 18.97452</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b2 &lt;-<span class="st"> </span><span class="kw">sample</span>(B, <span class="dv">20</span>)
b2</code></pre></div>
<pre><code>##  [1] 12.187211 14.091217 13.646950 10.417987 12.217799 12.050471 14.878786
##  [8] 11.958330  9.023702 12.231641  9.526726 15.290766 14.616339 10.158127
## [15] 12.574242 11.859272 11.188990 15.499437 11.161044  9.462869</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(a2)</code></pre></div>
<pre><code>## [1] 15.56868</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(b2)</code></pre></div>
<pre><code>## [1] 12.2021</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(a2) -<span class="st"> </span><span class="kw">mean</span>(b2)  </code></pre></div>
<pre><code>## [1] 3.366588</code></pre>
<p>This time, <span class="math inline">\(\overline{x}_{A} - \overline{x}_{B} = 3.37\)</span> This is again pretty close to the true difference in population means, but a bit higher tis time.</p>
<p>We can keep drawing samples of size 17 from A, and size 20 from B and examining the difference in sample means <span class="math inline">\(\overline{x}_{A} - \overline{x}_{B}\)</span>. A quick way of writing that code in R is as follows - where we repeat it 5 more times:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(A,<span class="dv">17</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(B,<span class="dv">20</span>))</code></pre></div>
<pre><code>## [1] 2.50799</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(A,<span class="dv">17</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(B,<span class="dv">20</span>))</code></pre></div>
<pre><code>## [1] 3.109358</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(A,<span class="dv">17</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(B,<span class="dv">20</span>))</code></pre></div>
<pre><code>## [1] 2.188502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(A,<span class="dv">17</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(B,<span class="dv">20</span>))</code></pre></div>
<pre><code>## [1] 2.471076</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(A,<span class="dv">17</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(B,<span class="dv">20</span>))</code></pre></div>
<pre><code>## [1] 3.588477</code></pre>
<p>As you can see, the values we get tend to be a little bit above or a little bit below 3.</p>
<p>If we did this thousands and thousands of times, we'd get a distribution of the differences in sample means. This would be the <em>sampling distribution of the differences in sample means</em>. Below we repeat the above step 10,000 times to get a visualization of what this sampling distribution looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
difs &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>, <span class="dv">10000</span>)

for(i in <span class="dv">1</span>:<span class="dv">10000</span>){
  difs[[i]] &lt;-<span class="st">  </span><span class="kw">mean</span>(<span class="kw">sample</span>(A, <span class="dv">17</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(B, <span class="dv">20</span>))
}

df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">dif =</span> <span class="kw">unlist</span>(difs))


<span class="co">#get mean difference</span>
<span class="kw">mean</span>(<span class="kw">unlist</span>(difs))  <span class="co"># 3.00</span></code></pre></div>
<pre><code>## [1] 2.986595</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#make histogram</span>
<span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">x=</span>dif)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;dodgerblue&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">binwidth =</span> .<span class="dv">1</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Difference in Sample Means&quot;</span>)+
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Difference in Sample Means&quot;</span>) +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(<span class="kw">unlist</span>(difs)), <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-362-1.png" width="672" /></p>
<p>As you can see, the sampling distribution is approximately symmetrical. If we ran it for more simulations, it would become even more symmetrical. The average (mean) difference in sample means across all samples is <span class="math inline">\(\mu_{\overline{x}_{A} - \overline{x}_{B}}= 2.987\)</span> which is approximately equal to the real difference in population means <span class="math inline">\(\mu_{A} - \mu_{B}\)</span>.</p>
<p>An obvious next question is what is the standard deviation of this sampling distribution? i.e. the standard deviation of the sampling distribution of the differences in sample means. If we knew this, then we'd be able to describe how likely or unlikely we were to get any particular difference in sample means in terms of how many sampling distribution standard deviations that score is away from the mean.</p>
<p>There are actually two different formulas that we can use to work out the standard deviation - which we will discuss shortly (see section xx.xxx). In brief, it is a bit trickier to calculate the standard deviation of this sampling distribution compared to the standard deviation of the sampling distribution of sample means, because our difference in sample means uses two different samples each with their own sample standard deviation.</p>
<p>Given we just simulated our sampling distribution of the differences in sample means, we could just look at its standard deviation in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(<span class="kw">unlist</span>(difs))</code></pre></div>
<pre><code>## [1] 0.8208998</code></pre>
<p>This tells us that the standard deviation of this sampling distribution is approximately equal to <span class="math inline">\(\sigma_{\overline{x}_{A} - \overline{x}_{B}}= 0.82\)</span>. As well as using the formulas that we will introduce shortly to estimate this value, there is actually another shortcut we could have used too as we know the original population standard deviations. This is not essential to know or remember, as it's not something we would ever do in practice, it's just something to point out in passing.</p>
<p>There is something called <a href="http://onlinestatbook.com/2/summarizing_distributions/variance_sum_law.html">variance sum law</a>. Essentially, if you want to know the difference of two variables, then it is equal to the sum of the variance of the two variables.</p>
<p>Our variables in this scenario are the sampling distributions of sample means for A and B. We can calculate the variance of each by <span class="math inline">\(\sigma^2/n\)</span>. If we add them together according to the variance sum law, we get the variance for the difference in these two variables. Then we square-root to get the standard deviation - which is equivalent to our sampling distribution of the difference in sample means standard deviation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>((<span class="kw">var</span>(A)/<span class="dv">17</span>) +<span class="st"> </span>(<span class="kw">var</span>(B)/<span class="dv">20</span>)) </code></pre></div>
<pre><code>## [1] 0.8248519</code></pre>
<p>Again, this tells us that <span class="math inline">\(\sigma_{\overline{x}_{A} - \overline{x}_{B}}= 0.82\)</span>.</p>
<p>However, in practice, we only have our two samples (our one sample of population A and our one sample of population B). We don't know anything else about the sampling distribution or the populations. Therefore we cannot use either of the above methods to calculate the standard deviation of the sampling distribution. We'll get to what method you need to use soon.</p>
<p>** No Difference in Population Means **</p>
<p>Let's look at an example of what the sampling distribution of difference in sample means looks like when there is no difference between population means.</p>
<p>In this scenario, we have two normally distributed populations, C and D and each has 500,000 subjects. C has a population mean <span class="math inline">\(\mu = 10\)</span>, and a population standard deviation <span class="math inline">\(\sigma = 3\)</span>. D has a population mean <span class="math inline">\(\mu = 10\)</span>, and a population SD <span class="math inline">\(\sigma = 3\)</span>. We can generate these populations in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)

C &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">500000</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">3</span>)
D &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">500000</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">3</span>)</code></pre></div>
<p>We can then examine their means and standard deviations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># and their summary descriptive stats</span>
<span class="kw">mean</span>(C)   </code></pre></div>
<pre><code>## [1] 9.99855</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(C)     </code></pre></div>
<pre><code>## [1] 3.000043</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(D)   </code></pre></div>
<pre><code>## [1] 10.00173</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(D)     </code></pre></div>
<pre><code>## [1] 3.001071</code></pre>
<p>We can then calculate the true difference between these populations in their population means and see that <span class="math inline">\(\mu_{C}-\mu_{D} = 0.00\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(C) -<span class="st"> </span><span class="kw">mean</span>(D)</code></pre></div>
<pre><code>## [1] -0.003182429</code></pre>
<p>We can also visualize these populations - it can be hard to see both populations because they are identical and D is directly on top of C, but they are both there!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(C, D) %&gt;%
<span class="st">  </span><span class="kw">pivot_longer</span>(<span class="dv">1</span>:<span class="dv">2</span>) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>value, <span class="dt">fill=</span>name)) +
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Comparison of Population C vs D&quot;</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">10</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">1</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">10</span>, <span class="dt">color=</span><span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-368-1.png" width="672" /></p>
<p>For this example, let's look at one sample of size 11 for population C and one sample of size 14 for population D.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
c1 &lt;-<span class="st"> </span><span class="kw">sample</span>(C, <span class="dv">11</span>)
d1 &lt;-<span class="st"> </span><span class="kw">sample</span>(D, <span class="dv">14</span>)

c1</code></pre></div>
<pre><code>##  [1]  5.138396  6.594813 12.484512  9.097602 13.984427  8.240521 12.101860
##  [8]  6.798203  9.485584  8.794962 14.107981</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d1</code></pre></div>
<pre><code>##  [1] 11.237805  7.779749 10.525978  9.202548  9.009089 11.607358  6.935309
##  [8]  6.883245  9.111570  9.569286 11.226204 15.672788 12.318739  7.637000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(c1)</code></pre></div>
<pre><code>## [1] 9.711715</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(d1)</code></pre></div>
<pre><code>## [1] 9.908333</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(c1) -<span class="st"> </span><span class="kw">mean</span>(d1)  </code></pre></div>
<pre><code>## [1] -0.1966186</code></pre>
<p>Here we find that the difference in sample means <span class="math inline">\(\overline{x}_C - \overline{x}_D = -0.197\)</span>.</p>
<p>If we did this lots of times, we'd sometimes get sample means that were larger in C and sometimes they would be larger in D. Let's do it five more times:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(C,<span class="dv">11</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(D,<span class="dv">14</span>))</code></pre></div>
<pre><code>## [1] 0.3432349</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(C,<span class="dv">11</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(D,<span class="dv">14</span>))</code></pre></div>
<pre><code>## [1] -0.007209497</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(C,<span class="dv">11</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(D,<span class="dv">14</span>))</code></pre></div>
<pre><code>## [1] -0.4427334</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(C,<span class="dv">11</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(D,<span class="dv">14</span>))</code></pre></div>
<pre><code>## [1] -1.098177</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">sample</span>(C,<span class="dv">11</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(D,<span class="dv">14</span>))</code></pre></div>
<pre><code>## [1] -0.3257204</code></pre>
<p>If we did this thousands and thousands of times, we will get our sampling distribution of the differences in sample means. We can use the code below to simulate this, collecting 20,000 samples of C and D and determining the difference in means for each sample collected:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)

difs1 &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&#39;list&#39;</span>, <span class="dv">20000</span>)

for(i in <span class="dv">1</span>:<span class="dv">20000</span>){
  difs1[[i]] &lt;-<span class="st">  </span><span class="kw">mean</span>(<span class="kw">sample</span>(C, <span class="dv">11</span>)) -<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sample</span>(D, <span class="dv">14</span>))
}</code></pre></div>
<p>We can visualize this sampling distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## plot as a histogram:
df1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">dif =</span> <span class="kw">unlist</span>(difs1))

<span class="kw">ggplot</span>(df1, <span class="kw">aes</span>(<span class="dt">x=</span>dif)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;mistyrose&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">binwidth =</span> .<span class="dv">2</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Sampling Distribution of Difference in Sample Means&quot;</span>) +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(<span class="kw">unlist</span>(difs1)), <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-372-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">unlist</span>(difs1))  </code></pre></div>
<pre><code>## [1] -0.005749998</code></pre>
<p>After 20,000 samples of size 11 from population C and size 14 from population D, we find that the mean difference in sample means is <span class="math inline">\(\mu_{\overline{x}_C - \overline{x}_D} = -0.006\)</span>. This is pretty close to the true difference in population means of 0. If we were to run our simulation for longer to get even more differences in sample means, then our mean difference would get even closer to 0.</p>
<p>So, we have a symmetrical sampling distribution of differences in sample means with a mean <span class="math inline">\(\mu_{\overline{x}_C - \overline{x}_D}\)</span> approximately equal to 0.0. What is the Standard Deviation of this distribution? Again, we can calculate this directly from our simulated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(<span class="kw">unlist</span>(difs1))</code></pre></div>
<pre><code>## [1] 1.207817</code></pre>
<p>Here we find that the standard deviation is <span class="math inline">\(\sigma_{\overline{x}_C - \overline{x}_D} = 1.2\)</span>. The question becomes, how do you estimate this standard deviation when you only have one sample of each population? We shall get to how this is done in the next few sections.</p>
</div>
</div>
<div id="pooled-standard-deviation" class="section level2">
<h2><span class="header-section-number">10.4</span> Pooled Standard Deviation</h2>
<p>In the Student's t-test, we assume that our two samples come from populations that have &quot;equal variance&quot;, i.e. that their standard deviations are equivalent. The reason behind this is that it enables us to use this estimate in calculating the Standard Deviation of the Sampling Distribution of Differences in Sample Means.</p>
<p>When we assume that two samples have the same variance, we estimate this by pooling their standard deviations. Let's look in more detail about how standard deviations are pooled.</p>
<p>When we have two samples, they each have sample standard deviations, that are usually not equal. For example, let's look at these two samples. The first sample is of size 9 and the second sample is of size 10.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp1 &lt;-<span class="st">  </span><span class="kw">c</span>(<span class="dv">23</span>, <span class="dv">25</span>, <span class="dv">33</span>, <span class="dv">19</span>, <span class="dv">21</span>, <span class="dv">27</span>, <span class="dv">26</span>, <span class="dv">31</span>, <span class="dv">20</span>)
samp1</code></pre></div>
<pre><code>## [1] 23 25 33 19 21 27 26 31 20</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(samp1)</code></pre></div>
<pre><code>## [1] 4.821825</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp2 &lt;-<span class="st">  </span><span class="kw">c</span>(<span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">23</span>, <span class="dv">19</span>, <span class="dv">20</span>, <span class="dv">24</span>, <span class="dv">25</span>, <span class="dv">21</span>, <span class="dv">23</span>, <span class="dv">22</span>)
samp2</code></pre></div>
<pre><code>##  [1] 21 22 23 19 20 24 25 21 23 22</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(samp2)</code></pre></div>
<pre><code>## [1] 1.825742</code></pre>
<p>It's clear that our sample standard deviations differ, with sample 1 having a sample standard deviation <span class="math inline">\(s=4.82\)</span> and sample 2 having a sample standard deviation of <span class="math inline">\(s=1.83\)</span>.</p>
<p>So, if we assumed that they both come from populations with the same SD, how do we estimate that value? We have to pool the standard devation. Essentially, we want to calculate a value that is likely somewhere in between the two sample SDs. In practice we use a weighted average.</p>
<p>There are two ways of going about calculating this pooled standard deviation value. The first is to use first principles as to what a standard deviation is. This is the one that makes most sense logically (to us at least), but it is also the more long-winded way. The second way is to use a shortcut formula.</p>
<p>** Option 1: From first principles **</p>
<p>Let's first calculate the pooled standard deviation based on first principles. Remember, a standard deviation is the square root of the average squared deviation of each value from the mean.</p>
<p>What we could do is to get the difference of every score from it's group mean, then square those differences, then add them all up, then divide by <span class="math inline">\(n-2\)</span>, and then square root. We divide by <span class="math inline">\(n-2\)</span> instead of <span class="math inline">\(n-1\)</span> because we have two samples that we are using and we are usings two different estimated means to determine the standard deviation. Therefore, to avoid underestimating our standard deviation we divide by <span class="math inline">\(n-2\)</span>.</p>
<p>Here, we break this down step by step:</p>
<ol style="list-style-type: lower-roman">
<li>Calculate each sample mean:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp1)</code></pre></div>
<pre><code>## [1] 25</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(samp2)</code></pre></div>
<pre><code>## [1] 22</code></pre>
<ol start="2" style="list-style-type: lower-roman">
<li>Let's get the differences of each score from their group mean:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get the differences from each group mean:</span>

samp1_dif  &lt;-<span class="st"> </span>samp1 -<span class="st"> </span><span class="kw">mean</span>(samp1)
samp2_dif  &lt;-<span class="st"> </span>samp2 -<span class="st"> </span><span class="kw">mean</span>(samp2)

samp1_dif</code></pre></div>
<pre><code>## [1] -2  0  8 -6 -4  2  1  6 -5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp2_dif</code></pre></div>
<pre><code>##  [1] -1  0  1 -3 -2  2  3 -1  1  0</code></pre>
<ol start="3" style="list-style-type: lower-roman">
<li>Now square all these differences:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp1_dif2 &lt;-<span class="st"> </span>samp1_dif^<span class="dv">2</span>
samp2_dif2 &lt;-<span class="st"> </span>samp2_dif^<span class="dv">2</span>

samp1_dif2</code></pre></div>
<pre><code>## [1]  4  0 64 36 16  4  1 36 25</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp2_dif2</code></pre></div>
<pre><code>##  [1] 1 0 1 9 4 4 9 1 1 0</code></pre>
<ol start="4" style="list-style-type: lower-roman">
<li>Get the sum of all the squares and add them up across both samples:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ssq &lt;-<span class="st"> </span><span class="kw">sum</span>(samp1_dif2) +<span class="st"> </span><span class="kw">sum</span>(samp2_dif2)
ssq</code></pre></div>
<pre><code>## [1] 216</code></pre>
<ol start="22" style="list-style-type: lower-alpha">
<li>Now we get the 'average' squared deviation by dividing by n-2</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n2 &lt;-<span class="st"> </span><span class="dv">9</span> +<span class="st"> </span><span class="dv">10</span> -<span class="st"> </span><span class="dv">2</span>
asd &lt;-<span class="st"> </span>ssq/n2
asd</code></pre></div>
<pre><code>## [1] 12.70588</code></pre>
<ol start="6" style="list-style-type: lower-roman">
<li>Finally, we get the pooled standard devation by squarerooting this:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(asd)</code></pre></div>
<pre><code>## [1] 3.564531</code></pre>
<p>Thus our estimate of the pooled standard deviation is <span class="math inline">\(\hat\sigma_p = 3.56\)</span>. This value is between our original sample standard deviations of 4.82 and 1.83, which makes sense.</p>
<p>You may see the following forumla for calculating the pooled standard deviation in the way we just did.</p>
<p>$_p =  $</p>
<p>Here, <span class="math inline">\(n_1\)</span> is the sample size of sample 1, <span class="math inline">\(n_2\)</span> is the sample size of sample 2. The <span class="math inline">\(\Sigma_{ik}(x_{ik} - \overline{x}_k)^2\)</span> part of the formula is simply saying to take each data point away from it's sample mean and then square it - and then add all of these up. It looks quite intimidating, but hopefully the steps we outlined above make sense.</p>
<p>** Option 2: Using a shortcut formula **</p>
<p>The second way to calculate the pooled standard deviations doesn't require you to know the raw data. It is a formula that only requires you to know the sample standard deviation of each sample and the sample size of each sample. This is the formula:</p>
<p>$_p =  $</p>
<p>Here, <span class="math inline">\(w_1 = n_1-1\)</span> where <span class="math inline">\(n_1\)</span> is the sample size of sample 1. <span class="math inline">\(w_2 = n_2-1\)</span> where <span class="math inline">\(n_2\)</span> is the sample size of sample 2. <span class="math inline">\(s_1\)</span> is the sample standard deviation of sample 1, and <span class="math inline">\(s_1\)</span> is the sample standard deviation of sample 2.</p>
<p>We can calculate the pooled standard deviation using this formula for our two samples above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s1sd &lt;-<span class="st"> </span><span class="kw">sd</span>(samp1)
s2sd &lt;-<span class="st"> </span><span class="kw">sd</span>(samp2)

w1 &lt;-<span class="st"> </span><span class="kw">length</span>(samp1) -<span class="st"> </span><span class="dv">1</span>
w2 &lt;-<span class="st"> </span><span class="kw">length</span>(samp2) -<span class="st"> </span><span class="dv">1</span>

numerator &lt;-<span class="st"> </span>(w1*s1sd^<span class="dv">2</span>) +<span class="st"> </span>(w2*s2sd^<span class="dv">2</span>)
denominator &lt;-<span class="st"> </span>w1+w2

<span class="kw">sqrt</span>(numerator/denominator) </code></pre></div>
<pre><code>## [1] 3.564531</code></pre>
<p>This approach also calculates the pooled standard deviation to be <span class="math inline">\(\hat\sigma_p = 3.56\)</span>.</p>
</div>
<div id="theory-behind-students-t-test" class="section level2">
<h2><span class="header-section-number">10.5</span> Theory behind Student's t-test</h2>
<p>Hopefully you have a sense of the sampling distribution for the difference in sample means, and an idea about what pooled standard deviation is. In this section we'll bring together these ideas to show you the behind the scenes working of the Student's t-test.</p>
<p>We'll use some example data from <a href="https://learningstatisticswithr.com/book/ttest.html#studentttest">the Navarro book</a>. These data are the exam scores of students who were TA-ed by either Anastasia or Bernadette. The question at hand is whether these two samples could have come from populations with the same mean, or from populations with different means. We assume that the variance (and therefore standard deviations) of both of these populations are the same.</p>
<p>Here are the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">anastasia &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">65</span>, <span class="dv">74</span>, <span class="dv">73</span>, <span class="dv">83</span>, <span class="dv">76</span>, <span class="dv">65</span>, <span class="dv">86</span>, <span class="dv">70</span>, <span class="dv">80</span>, <span class="dv">55</span>, <span class="dv">78</span>, <span class="dv">78</span>, <span class="dv">90</span>, <span class="dv">77</span>, <span class="dv">68</span>)
bernadette &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">72</span>, <span class="dv">66</span>, <span class="dv">71</span>, <span class="dv">66</span>, <span class="dv">76</span>, <span class="dv">69</span>, <span class="dv">79</span>, <span class="dv">73</span>, <span class="dv">62</span>, <span class="dv">69</span>, <span class="dv">68</span>, <span class="dv">60</span>, <span class="dv">73</span>, <span class="dv">68</span>, <span class="dv">67</span>, <span class="dv">74</span>, <span class="dv">56</span>, <span class="dv">74</span>)</code></pre></div>
<p>We can make a boxplot to compare the two samples:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data:</span>
d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">values =</span> <span class="kw">c</span>(anastasia, bernadette),
                <span class="dt">group =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;anastasia&quot;</span>,<span class="dv">15</span>), <span class="kw">rep</span>(<span class="st">&quot;bernadette&quot;</span>, <span class="dv">18</span>))
)

<span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> group, <span class="dt">y =</span> values, <span class="dt">fill =</span> group)) +
<span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">alpha=</span>.<span class="dv">3</span>) +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width=</span>.<span class="dv">1</span>, <span class="dt">size=</span><span class="dv">2</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;firebrick&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-384-1.png" width="672" /></p>
<p>We can also get the sample mean <span class="math inline">\(\overline{x}\)</span>, standard deviation <span class="math inline">\(s\)</span> and <span class="math inline">\(n\)</span> of each sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(anastasia)            </code></pre></div>
<pre><code>## [1] 74.53333</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(anastasia)              </code></pre></div>
<pre><code>## [1] 8.998942</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nA&lt;-<span class="kw">length</span>(anastasia)      
nA</code></pre></div>
<pre><code>## [1] 15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(bernadette)           </code></pre></div>
<pre><code>## [1] 69.05556</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(bernadette)             </code></pre></div>
<pre><code>## [1] 5.774918</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nB &lt;-<span class="st"> </span><span class="kw">length</span>(bernadette)   
nB</code></pre></div>
<pre><code>## [1] 18</code></pre>
<p>Although we are actually interested in whether these two samples come from populations that differ in their population means, we can directly measure if the two samples differ in their sample means by subtracting one from the other:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(anastasia)  -<span class="st"> </span><span class="kw">mean</span>(bernadette)  </code></pre></div>
<pre><code>## [1] 5.477778</code></pre>
<p>We can see that Anastasia students have on average 5.48 higher scores than Bernadette students. But how meaningful is this difference?</p>
<p>What we are going to do is to work out how unusual or usual our one observed sample mean difference is. We need to construct the sampling distribution of differences in sample means. We hypothesise that the true mean difference in population means is 0 (no difference between groups).</p>
<p>These are our hypotheses:</p>
<p><span class="math inline">\(H_{0}: \mu_A = \mu_A\)</span> <span class="math inline">\(H_{1}: \mu_A \ne \mu_A\)</span></p>
<p>If we assume that the true difference in population means is 0 (the null hypothesis), then this will mean that the mean of the sampling distribution of the difference in sample means will also be 0 (<span class="math inline">\(\mu_{\overline{x}_A-\overline{x}_B}=0\)</span>). If we know the standard deviation of this sampling distribution, then we could work out how many standard deviations away our one observed sample mean differnce of 5.48 is from 0. That would enable us to work out the probability of getting a value that extreme.</p>
<p>So our next step is to work out the standard deviation of the sampling distribution. If we assume equal variances between the populations of group A and B, then we can calculate the standard deviation of this sampling distribution as</p>
<p><span class="math inline">\(\sigma_{\overline{x}_A-\overline{x}_B} = \hat\sigma_p \times \sqrt{\frac{1}{n_1} + \frac{1}{n_2} }\)</span></p>
<p>So, the first step in this formula is to calculate the pooled standard deviation <span class="math inline">\(\hat\sigma_p\)</span>. We can either calculate this from first principles (see section xxx.xx) or using the shortcut formula (see section xxx.xxx). Below, we calculate the pooled standard deviation using the first principles:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># from first principles, calculating deviations from each group mean</span>
difA2 &lt;-<span class="st"> </span>(anastasia -<span class="st"> </span><span class="kw">mean</span>(anastasia))^<span class="dv">2</span>
difB2 &lt;-<span class="st"> </span>(bernadette -<span class="st"> </span><span class="kw">mean</span>(bernadette))^<span class="dv">2</span>
sumsq &lt;-<span class="st"> </span><span class="kw">sum</span>(difA2) +<span class="st"> </span><span class="kw">sum</span>(difB2)
n &lt;-<span class="st"> </span>nA +<span class="st"> </span>nB <span class="co">#33</span>
sd.pool &lt;-<span class="st"> </span><span class="kw">sqrt</span>(sumsq/(n<span class="dv">-2</span>))

sd.pool <span class="co"># 7.41  this is the estimated pooled s.d.</span></code></pre></div>
<pre><code>## [1] 7.406792</code></pre>
<p>This gives us a value of <span class="math inline">\(\hat\sigma_p = 7.407\)</span>. This makes sense being in between our two sample standard deviations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(anastasia) <span class="co">#8.999</span></code></pre></div>
<pre><code>## [1] 8.998942</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(bernadette) <span class="co">#5.775</span></code></pre></div>
<pre><code>## [1] 5.774918</code></pre>
<p>Once we've calculated the pooled standard deviation <span class="math inline">\(\hat\sigma_p\)</span>, we can insert it to the formula above to calculate the standard deviation of the sampling distribution of differences in sample means <span class="math inline">\(\sigma_{\overline{x}_A-\overline{x}_B}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sedm &lt;-<span class="st">  </span>sd.pool *<span class="st"> </span><span class="kw">sqrt</span>( (<span class="dv">1</span>/nA) +<span class="st"> </span>(<span class="dv">1</span>/nB))

sedm  <span class="co"># this is the Standard Deviation of the Sampling Distribution of differences in sample means</span></code></pre></div>
<pre><code>## [1] 2.589436</code></pre>
<p>Doing this, we calculate <span class="math inline">\(\sigma_{\overline{x}_A-\overline{x}_B} = 2.59\)</span>. This tells us that the standard deviation of the theoretical sampling distribution of the differences in sample means is 2.59. With that information, we can work out how many standard deviations away from the mean (of 0) our one observed difference in sample means of <span class="math inline">\(\overline{x}_A - \overline{x}_B = 5.48\)</span> is.</p>
<p>Lets' visualize what this sampling distribution looks like:</p>
<div class="figure">
<img src="img/tt5.png" />

</div>
<p>By estimating the standard deviation, we are able to plot what our sampling distribution would look like. We are then able to see that our observed difference in sample means of <span class="math inline">\(\overline{x}_A - \overline{x}_B = 5.48\)</span> is quite far away from the hypothesized mean of 0 in terms of standard deviations. What we have not discussed so far is what kind of shape that this distribution is other than it is symmetrical. Similar to when we were looking at sampling distributions of sample means (see section xxx.xxx), the shape of this sampling distribution of the difference in sample means is also <span class="math inline">\(t\)</span>-shaped. In the example given above, it is a <span class="math inline">\(t\)</span>-distribution with 31 degrees of freedom. The degrees of freedom of these sampling distributions are equal to <span class="math inline">\(df = n_A + n_B - 2\)</span>. So in our case, this is equal to:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nA +<span class="st"> </span>nB -<span class="st"> </span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 31</code></pre>
<p>Given that the shape of our distribution is <span class="math inline">\(t\)</span>-shaped, when we determine how many standard deviations away from the mean our single observed difference in sample means is, we call this our <em>observed t-value</em>.</p>
<p>We calculate t by the following formula:</p>
<p><span class="math inline">\(t = \frac{(\overline{x}_A - \overline{x}_B) - (\mu_{\overline{x}_{A} - \overline{x}_{B}})}{\sigma_{\overline{x}_{A} - \overline{x}_{B}}}\)</span></p>
<p>But because we assume the mean of the sampling distribution of sample means to be zero, we can write it as:</p>
<p><span class="math inline">\(t = \frac{\overline{x}_A - \overline{x}_B}{\sigma_{\overline{x}_{A} - \overline{x}_{B}}}\)</span></p>
<p>Essentially, it is taking our observed differnce in sample means and dividing it by the standard deviation of the sampling distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tobs &lt;-<span class="st"> </span>(<span class="kw">mean</span>(anastasia)  -<span class="st"> </span><span class="kw">mean</span>(bernadette))  /<span class="st"> </span>sedm  

tobs </code></pre></div>
<pre><code>## [1] 2.115432</code></pre>
<p>Our observed t-value is <span class="math inline">\(t = 2.115\)</span>.</p>
<p>Our final thing to do is to calculate the p-value. The first step in this is to work out the proportion of the t distribution that is higher than our observed t-value. Because we are running a two-tailed test (we are examining whether the true difference in population means is 0 or not equal to 0), we need to double that proportion to get our p-value.</p>
<div class="figure">
<img src="img/tt6.png" />

</div>
<p>The image above helps to explain how we'd calculate the p-value. Because it's a two-tailed test, we need to calculate the proportion of the distribution in the red area - this is the area that is greater than <span class="math inline">\(t=2.115\)</span> and less than <span class="math inline">\(t=-2.115\)</span>. We can use the <code>pt()</code> function in R to calculate this - remember that for this example, our degrees of freedom are 31:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pt</span>(<span class="fl">2.115</span>, <span class="dt">df=</span><span class="dv">31</span>)</code></pre></div>
<pre><code>## [1] 0.02128454</code></pre>
<p>This tells us that the area beyond <span class="math inline">\(t=2.115\)</span> is 0.021. Therefore to get our p-value we double this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dv">1</span> -<span class="st"> </span><span class="kw">pt</span>(<span class="fl">2.115</span>, <span class="dt">df=</span><span class="dv">31</span>)) *<span class="st"> </span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.04256907</code></pre>
<p>Our p-value is <span class="math inline">\(p = 0.043\)</span>.</p>
<p>** One-tailed test **</p>
<p>If we had predicted beforehand that Anastaia's students would score more highly on the exam than Bernadette's then we could perform a one-tailed test. The hypotheses in this situation would be:</p>
<p><span class="math inline">\(H_{0}: \mu_A \le \mu_A\)</span> <span class="math inline">\(H_{1}: \mu_A &gt; \mu_A\)</span></p>
<p>All of the steps for calculating the observed <span class="math inline">\(t\)</span>-value would be the same as above. The only difference is when calculating the p-value, we only need to consider the area under the curve beyond <span class="math inline">\(t=2.115\)</span>. Therefore the p-value for this one-tailed test is <span class="math inline">\(p = 0.021\)</span>.</p>
</div>
<div id="confidence-interval-for-difference-in-means" class="section level2">
<h2><span class="header-section-number">10.6</span> Confidence Interval for Difference in Means</h2>
<p>When we take a sample of two independent groups, we are able to directly calculate the observed difference in sample means <span class="math inline">\(\overline{x}_A - \overline{x}_B\)</span>. What we are usually interested in is whether this observed difference is meaningfully different from 0. One way to gain more insight into whether this might be true or not is to generate a 95% confidence interval around our observed sample difference <span class="math inline">\(\overline{x}_A - \overline{x}_B\)</span>. If 0 is not inside our confidence interval, then this is pretty good evidence that the true difference in population means <span class="math inline">\(\mu_A - \mu_B\)</span> between the two groups is not 0, and therefore there is a difference in population means.</p>
<p>As with all confidence intervals, the longer explanation as to what a 95% confidence interval really is, is a bit more long winded. Technically, this 95% confidence intervals means that if we were to take many samples of A and many samples of B, and calculate the difference and confidence intervals for all of them, in 95% of these confidence intervals we would have captured the true difference in population means <span class="math inline">\(\mu_A - \mu_B\)</span>.</p>
<p>Let's illustrate with a small example, and then we'll turn to our TA data.</p>
<p><em>Example 1</em></p>
<p>A researcher compares the reaction times of two groups on a motor test. Individuals are different (independent) in each group. GroupA took a stimulant prior to the test. GroupB is the control group.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">groupA &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">5.5</span>, <span class="fl">5.1</span>, <span class="fl">3.5</span>, <span class="fl">5.1</span>, <span class="fl">4.6</span>, <span class="fl">4.6</span>, <span class="fl">5.9</span>, <span class="fl">4.0</span>, <span class="fl">3.1</span>, <span class="fl">3.8</span>)
<span class="kw">mean</span>(groupA)     <span class="co"># 4.52</span></code></pre></div>
<pre><code>## [1] 4.52</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(groupA)       <span class="co"># 0.91</span></code></pre></div>
<pre><code>## [1] 0.9065196</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">groupB &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">5.7</span>, <span class="fl">5.3</span>, <span class="fl">5.9</span>, <span class="fl">5.0</span>, <span class="fl">5.0</span>, <span class="fl">4.3</span>, <span class="fl">4.1</span>, <span class="fl">5.9</span>, <span class="fl">5.9</span>, <span class="fl">5.8</span>, <span class="fl">5.4</span>, <span class="fl">5.2</span>, <span class="fl">4.9</span>, <span class="fl">5.5</span>)
<span class="kw">mean</span>(groupB)     <span class="co"># 5.28</span></code></pre></div>
<pre><code>## [1] 5.278571</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(groupB)       <span class="co"># 0.58</span></code></pre></div>
<pre><code>## [1] 0.5766996</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data:</span>
dd &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">values =</span> <span class="kw">c</span>(groupA, groupB),
                <span class="dt">group =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;A&quot;</span>,<span class="dv">10</span>), <span class="kw">rep</span>(<span class="st">&quot;B&quot;</span>, <span class="dv">14</span>))
)

<span class="kw">head</span>(dd)</code></pre></div>
<pre><code>##   values group
## 1    5.5     A
## 2    5.1     A
## 3    3.5     A
## 4    5.1     A
## 5    4.6     A
## 6    4.6     A</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dd, <span class="kw">aes</span>(<span class="dt">x =</span> group, <span class="dt">y =</span> values, <span class="dt">fill =</span> group)) +
<span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">alpha=</span>.<span class="dv">3</span>) +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width=</span>.<span class="dv">1</span>, <span class="dt">size=</span><span class="dv">2</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;firebrick&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-395-1.png" width="672" /></p>
<p>We can see from our data, that the observed difference in sample means between the two groups is <span class="math inline">\(\overline{x}_A - \overline{x}_B = -0.759\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(groupA) -<span class="st"> </span><span class="kw">mean</span>(groupB)</code></pre></div>
<pre><code>## [1] -0.7585714</code></pre>
<p>Group B (the control group) is slower by 0.76 seconds on average than group A (the group that took the stimulant).</p>
<p>The first step in constructing a confidence interval around this observed difference in sample means, is to imagine that this observed difference in sample means came from a sampling distribution of difference in sample means. Our observed value was just one possible sample that we could have got from this sampling distribution.</p>
<p>This sampling distribution of differences in sample means, is <span class="math inline">\(t\)</span>-shaped with degrees of freedom equal to <span class="math inline">\(n-2\)</span>. We assume that our observed value of the difference in sample means is a good estimate of the true difference in population means. If we know the standard deviation of this sampling distribution, then we can create the 95% confidence interval around it using the formula:</p>
<p><span class="math inline">\(CI_{95\%} = \overline{x}_A - \overline{x}_B \pm t \times \sigma_{\overline{x}_A - \overline{x}_B}\)</span></p>
<p>As discussed in the previous sections (see xxx.xxx) the standard deviation of the sampling distribution of differences in sample means, can be calculated using the formula:</p>
<p><span class="math inline">\(\sigma_{\overline{x}_A - \overline{x}_B} = \hat{\sigma_{\rho}} \times \sqrt{\frac{1}{n_A} + \frac{1}{n_B}}\)</span></p>
<p>where <span class="math inline">\(\hat{\sigma_{\rho}}\)</span> is the pooled estimate of the standard deviation between the two groups.</p>
<p>The below diagram illustrates how this formula determines the 95% confidence interval. Our margin of error is the value of <span class="math inline">\(t\)</span> for the <span class="math inline">\(t\)</span>-distribution with the appropriate degrees of freedom that leaves 2.5% in each tail, and therefore 95% in the middle of the distribution. We multiply this value of <span class="math inline">\(t\)</span> by the standard deviation of the sampling distribution of the differences in sample means to get the margin of error.</p>
<div class="figure">
<img src="img/tt7.png" />

</div>
<p>The first step is therefore to calculate the pooled estimate of the common standard deviation <span class="math inline">\(\hat{\sigma_{\rho}}\)</span> (see section xxx.xxx). Using the method of calculating deviations from each group mean, we can find that <span class="math inline">\(\hat{\sigma_{\rho}} = 0.73\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">difA2 &lt;-<span class="st"> </span>(groupA -<span class="st"> </span><span class="kw">mean</span>(groupA))^<span class="dv">2</span>
difB2 &lt;-<span class="st"> </span>(groupB -<span class="st"> </span><span class="kw">mean</span>(groupB))^<span class="dv">2</span>
sumsq &lt;-<span class="st"> </span><span class="kw">sum</span>(difA2) +<span class="st"> </span><span class="kw">sum</span>(difB2)
n &lt;-<span class="st"> </span><span class="kw">length</span>(groupA) +<span class="st"> </span><span class="kw">length</span>(groupB) <span class="co">#24</span>
sd.pool &lt;-<span class="st"> </span><span class="kw">sqrt</span>(sumsq/(n<span class="dv">-2</span>))
sd.pool</code></pre></div>
<pre><code>## [1] 0.7298683</code></pre>
<p>Putting this value into the above formula, we can calculate the standard deviation of the sampling distribution of the differences in sample means to be <span class="math inline">\(\sigma_{\overline{x}_A - \overline{x}_B} = 0.302\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sedm1 &lt;-<span class="st">  </span>sd.pool *<span class="st"> </span><span class="kw">sqrt</span>( (<span class="dv">1</span>/<span class="kw">length</span>(groupA)) +<span class="st"> </span>(<span class="dv">1</span>/<span class="kw">length</span>(groupB)))

sedm1</code></pre></div>
<pre><code>## [1] 0.3021942</code></pre>
<p>So now we have the estimated mean <span class="math inline">\(\overline{x}_A - \overline{x}_B\)</span> and the standard deviation <span class="math inline">\(\sigma_{\overline{x}_A - \overline{x}_B}\)</span> of our sampling distribution of differences in sample means. Because our sampling distribution approximates to a <span class="math inline">\(t\)</span>-distribution of degrees of freedom = <span class="math inline">\(n-2\)</span>, we calculate the value of <span class="math inline">\(t\)</span> which leaves 2.5% in the tails using the <code>qt()</code> function in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">length</span>(groupA) +<span class="st"> </span><span class="kw">length</span>(groupB)
tval &lt;-<span class="st"> </span><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df =</span> n<span class="dv">-2</span>)
tval</code></pre></div>
<pre><code>## [1] 2.073873</code></pre>
<p>Using this value, we can calculate the margin of error, and the lower and upper bounds of the confidence interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dm &lt;-<span class="st"> </span><span class="kw">mean</span>(groupA) -<span class="st"> </span><span class="kw">mean</span>(groupB)
dm</code></pre></div>
<pre><code>## [1] -0.7585714</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tval*sedm1</code></pre></div>
<pre><code>## [1] 0.6267124</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dm +<span class="st">  </span>(tval*sedm1)  <span class="co"># upper bound = -0.13</span></code></pre></div>
<pre><code>## [1] -0.131859</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dm -<span class="st">  </span>(tval*sedm1)  <span class="co"># lower bound = -1.39</span></code></pre></div>
<pre><code>## [1] -1.385284</code></pre>
<p>So the confidence interval is equal to: <span class="math inline">\(CI_{95\%} = -0.76 \pm 0.63\)</span> or <span class="math inline">\(CI_{95\%} = -0.76[-0.13, -1.39]\)</span>. This shows us that the true difference in population means is unlikely to include 0, so we can conclude that there is likely a true difference in population means between the two groups.</p>
<p>The above is the 'by hand' way to calculating the confidence interval, that explains the theory behind it. The quickest way is to use the R function <code>t.test()</code>. Inside the brackets, you include the two groups of data, and use <code>var.equal=T</code>. This last argument ensures that you are assuming that the variances between the two populations that the samples come from are equal. The <code>$conf.int</code> bit on the end just gives us the confidence intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(groupA, groupB, <span class="dt">var.equal =</span> T)$conf.int</code></pre></div>
<pre><code>## [1] -1.385284 -0.131859
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<p><em>Example 2</em></p>
<p>Let's look at this with a second example using the <code>anastasia</code> and <code>bernadette</code> data.</p>
<p>First, we get the value of <span class="math inline">\(\overline{x}_A - \overline{x}_B\)</span>, which is our observed difference in sample means. This is what we assume the mean of the sampling distribution of differences in sample means is.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meandif &lt;-<span class="st"> </span><span class="kw">mean</span>(anastasia)  -<span class="st"> </span><span class="kw">mean</span>(bernadette)   <span class="co"># 5.48</span>
meandif <span class="co"># this is our observed difference in sample means</span></code></pre></div>
<pre><code>## [1] 5.477778</code></pre>
<p>We find that <span class="math inline">\(\overline{x}_A - \overline{x}_B=5.48\)</span></p>
<p>Previously in section xxx.xxx we calculated the standard deviation of this sampling distribution to be <span class="math inline">\(\sigma_{\overline{x}_A - \overline{x}_B} = 2.59\)</span>. We saved this earlier as <code>sedm</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sedm  </code></pre></div>
<pre><code>## [1] 2.589436</code></pre>
<p>To calculate our margin of error, we need to multiply <span class="math inline">\(\sigma_{\overline{x}_A - \overline{x}_B}\)</span> by the value of <span class="math inline">\(t\)</span> that leaves 2.5% in each tail for a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tval &lt;-<span class="st"> </span><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df =</span> <span class="kw">length</span>(anastasia) +<span class="st"> </span><span class="kw">length</span>(bernadette) -<span class="dv">2</span>)
tval  <span class="co">#2.04</span></code></pre></div>
<pre><code>## [1] 2.039513</code></pre>
<p>We can now calculate the confidence intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tval*sedm</code></pre></div>
<pre><code>## [1] 5.28119</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meandif +<span class="st">  </span>(tval*sedm) <span class="co"># 10.76</span></code></pre></div>
<pre><code>## [1] 10.75897</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meandif -<span class="st">  </span>(tval*sedm) <span class="co"># 0.20</span></code></pre></div>
<pre><code>## [1] 0.1965873</code></pre>
<p>So the 95% confidence interval is equal to: <span class="math inline">\(CI_{95\%} = 5.48 \pm 5.28\)</span> or <span class="math inline">\(CI_{95\%} = 5.48[0.20, 10.76]\)</span>. These CIs suggest that 0 is not likely to be the true difference in population means, therefore it is plausible that the population of students that have Anastasia as a TA score more highly on their exams than the population of students that have Bernadette as a TA.</p>
<p>We could have checked this using <code>ttest()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(anastasia, bernadette, <span class="dt">var.equal =</span> T)$conf.int</code></pre></div>
<pre><code>## [1]  0.1965873 10.7589683
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<p>Finally, we could also plot confidence intervals like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## plotting confidence intervals

dci &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">group =</span> <span class="st">&quot;&quot;</span>, 
                  <span class="dt">mean =</span> meandif, 
                  <span class="dt">lower =</span> meandif -<span class="st">  </span>(tval*sedm), 
                  <span class="dt">upper =</span> meandif +<span class="st">  </span>(tval*sedm)
)


<span class="kw">ggplot</span>(dci, <span class="kw">aes</span>(<span class="dt">x=</span>group, <span class="dt">y=</span>mean, <span class="dt">ymax=</span>upper, <span class="dt">ymin=</span>lower)) +
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="dt">width=</span><span class="fl">0.2</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">4</span>, <span class="dt">shape=</span><span class="dv">21</span>, <span class="dt">fill=</span><span class="st">&quot;white&quot;</span>) +
<span class="st">  </span><span class="kw">theme_minimal</span>() +
<span class="st">  </span><span class="kw">coord_flip</span>() +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Estimate of Difference in Sample Means&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-407-1.png" width="672" /></p>
</div>
<div id="conducting-the-student-t-test-in-r" class="section level2">
<h2><span class="header-section-number">10.7</span> Conducting the Student t-test in R</h2>
<p>The function we need to conduct a Student's t-test in R is <code>t.test()</code>.</p>
<p>For instance, we want to test whether there is a difference in population means between the <code>anastasia</code> and <code>bernadette</code> samples. Here is the data visualized:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># boxplot</span>
<span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> group, <span class="dt">y =</span> values, <span class="dt">fill =</span> group)) +
<span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">alpha=</span>.<span class="dv">3</span>, <span class="dt">outlier.shape =</span> <span class="ot">NA</span>) +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width=</span>.<span class="dv">1</span>, <span class="dt">size=</span><span class="dv">2</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;firebrick&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-408-1.png" width="672" /></p>
<p>You can do the Student's t-test either from data stored in vectors, or in dataframes.</p>
<p>Here's how to do it for data stored in vectors. The first two arguments should be the names of the two independent samples. The final argument of <code>var.equal = T</code> tells R to do a Student's t-test by assuming that variances are equal between the two groups.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(anastasia, bernadette, <span class="dt">var.equal =</span> T)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  anastasia and bernadette
## t = 2.1154, df = 31, p-value = 0.04253
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   0.1965873 10.7589683
## sample estimates:
## mean of x mean of y 
##  74.53333  69.05556</code></pre>
<p>As you can see from the output, the default Student's t-test is to do a 2-tailed test. In this output you are given the observed <span class="math inline">\(t\)</span>-value, the degrees of freedom and the p-value. Also given are the sample means of each sample, as well as a 95% confidence interval of the true difference in population means (see section xxx.xxx for more on this confidence interval).</p>
<p>It's also possible to use <code>t-test</code> with data in long format in dataframes. Here is our data in long data format (we used this to make the boxplot):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(d)</code></pre></div>
<pre><code>##   values     group
## 1     65 anastasia
## 2     74 anastasia
## 3     73 anastasia
## 4     83 anastasia
## 5     76 anastasia
## 6     65 anastasia</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(d)</code></pre></div>
<pre><code>##    values      group
## 28     73 bernadette
## 29     68 bernadette
## 30     67 bernadette
## 31     74 bernadette
## 32     56 bernadette
## 33     74 bernadette</code></pre>
<p>In this format, we use <code>t.test()</code> and use the tilde <code>~</code> like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(values ~<span class="st"> </span>group, <span class="dt">data=</span>d, <span class="dt">var.equal =</span> T)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  values by group
## t = 2.1154, df = 31, p-value = 0.04253
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   0.1965873 10.7589683
## sample estimates:
##  mean in group anastasia mean in group bernadette 
##                 74.53333                 69.05556</code></pre>
<p>As you can see, this gives the exact same output as using data stored in vectors.</p>
<p>To perform a one-tailed t-test, you need to add the argument <code>alternative = &quot;greater&quot;</code> or <code>alternative = &quot;less&quot;</code> as appropriate for your hypothesis. So, if you had hypothesized Anastasia's students came from a population with a greater population mean than the population that Bernadette's you would use <code>alternative = &quot;greater&quot;</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(anastasia, bernadette, <span class="dt">var.equal =</span> T, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  anastasia and bernadette
## t = 2.1154, df = 31, p-value = 0.02126
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  1.08734     Inf
## sample estimates:
## mean of x mean of y 
##  74.53333  69.05556</code></pre>
<p>You could also do the following if your data are in long format:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(values ~<span class="st"> </span>group, <span class="dt">data=</span>d, <span class="dt">var.equal =</span> T, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  values by group
## t = 2.1154, df = 31, p-value = 0.02126
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  1.08734     Inf
## sample estimates:
##  mean in group anastasia mean in group bernadette 
##                 74.53333                 69.05556</code></pre>
<p>Notice that the observed <span class="math inline">\(t\)</span>-value is the same as are the degrees of freedom compared to the two-tailed t-test. The p-value has actually halved. The confidence intervals also changed in the output, but you should use the confidence intervals from the two-tailed version of the test.</p>
</div>
<div id="assumptions-of-the-independent-t-test" class="section level2">
<h2><span class="header-section-number">10.8</span> Assumptions of the Independent t-test</h2>
<p>The Student's t-test has two important assumptions that should be checked before you can fully be confident in the results of your statistical test.</p>
<p>** Independence fo Data **</p>
<p>As the name suggests, the data in each sample should be independent of each other. That is, individuals in each sample must be different - no individual should be in both groups. Also, subjects in one sample should not influence subjects in the other sample. e.g. If students who were in the Anastasia group studied with students who were in the Bernadette group, then the groups are no longer independent.</p>
<p>** Normality **</p>
<p>Firstly, the test assumes that both samples comes from populations that are normally distributed. We can test this by examining whether each sample is approximately normally distributed. Two options for doing this are to run a Shapiro-Wilk test, or make a QQ plot. Both test whether the distribution of data in your sample follow the normal distribution shape.</p>
<p>The Shapiro-Wilk test is run using <code>shapiro.test()</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(anastasia)  </code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  anastasia
## W = 0.98186, p-value = 0.9806</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(bernadette) </code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  bernadette
## W = 0.96908, p-value = 0.7801</code></pre>
<p>Because the p-values of these tests are both <span class="math inline">\(p&gt;.05\)</span>, we do not have sufficient evidence to suggest that our data are not approximately normal. In other words, we can assume that our data are approximately normal.</p>
<p>To make a QQ plot, we use <code>qnorm()</code> followed by <code>qqline()</code>. What we are looking for here is for our data points to roughly stick to the diagonal line. This would mean that our data values are approximately in line with what we'd expect for a normal distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(anastasia)
<span class="kw">qqline</span>(anastasia, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>) <span class="co"># looks ok</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-415-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(bernadette)
<span class="kw">qqline</span>(bernadette, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>) <span class="co"># looks ok</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-415-2.png" width="672" /></p>
<p>In both of our plots, the data points are roughly against the line, which is good evidence that our data are normally distributed. Having some variability is fine, especially with datapoints at the tail of the x-axes.</p>
<p>** Homogeneity of Variance **</p>
<p>Another assumption of the Student's t-test is that we have homogeneity of variance between the populations that our samples come from. We assume this when we calculate the pooled standard deviation which helps us calculate the standard deviation of the sampling distribution.</p>
<p>We can examine the standard deviations of each group with <code>sd()</code> but this alone does not tell us if they are similar enough to qualify for having equality of variance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(anastasia)</code></pre></div>
<pre><code>## [1] 8.998942</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(bernadette)</code></pre></div>
<pre><code>## [1] 5.774918</code></pre>
<p>There is a formal test you can to to test for the equality of variance. This test is called a Levene's test. To do it, we use the <code>leveneTest()</code> function from the <code>car</code> library. To use it, you need your data in long format:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car)
<span class="kw">leveneTest</span>(<span class="dt">y =</span> d$values, <span class="dt">group =</span> d$group)</code></pre></div>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  1  2.1287 0.1546
##       31</code></pre>
<p>A p-value of <span class="math inline">\(p&gt;.05\)</span> indicates that there is not sufficient evidence to suggest that the groups have different variances. In other words, we can assume that their variances are equal and we are ok to do a Student's t-test.</p>
</div>
<div id="welchs-t-test" class="section level2">
<h2><span class="header-section-number">10.9</span> Welch's t-test</h2>
<p>It is important to mention that the Student's t-test is only on version of an Independent t-test. It is the one that is taught first because computationally it is the most straightforward (even though it has several strange formulas in it). An issue with the Student's t-test is that we assume equal variances betwen the groups. As we saw in the previous section, we can test for this formally with a Levene's test, and proceed with a Student's t-test if we pass that. However, a more commonly taken approach is to computationally account for any difference in variance between the groups using the Welch's test. We won't go into how this is done, but you can do this in R using <code>t.test()</code>.</p>
<p>To do the Welch's test, just remove the <code>var.equal=T</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(anastasia, bernadette)  <span class="co"># notice the changes...</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  anastasia and bernadette
## t = 2.0342, df = 23.025, p-value = 0.05361
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.09249349 11.04804904
## sample estimates:
## mean of x mean of y 
##  74.53333  69.05556</code></pre>
<p>You'll notice here that the t-value, degrees of freedom and p-value have all changed! This is a result of the test accounting for the slight differences in variance between the two samples.</p>
<p>For a one-tailed Welch's test, you again use <code>alternative = &quot;greater&quot;</code>, and remove <code>equal.var=T</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(anastasia, bernadette, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>) </code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  anastasia and bernadette
## t = 2.0342, df = 23.025, p-value = 0.0268
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.8627718       Inf
## sample estimates:
## mean of x mean of y 
##  74.53333  69.05556</code></pre>
<p>Again, the t-value and degrees of freedom are the same of the two-tailed Welch's t-test, and the p-value is halved.</p>
<p>We actually recommend that if you are doing an independent t-test then you should do a Welch's test rather than a Student's t-test, even though we've spent a lot of time in this chapter outlining how to do a Student's t-test by hand! There is no real penalty for doing a Welch's test, and it is technially more accurate.</p>
</div>
<div id="effect-size-for-independent-two-sample-t-tests" class="section level2">
<h2><span class="header-section-number">10.10</span> Effect Size for Independent two sample t-tests:</h2>
<p>Just because you observe a “significant” difference in the population means between your two groups doesn’t mean that it’s necessarily interesting or relevant. The test is designed to pick up on whether there is a difference in the population means, not to tell you how large or small that difference is. We can determine how great the difference in means is by calculating the effect size. There are many different effect size measures we could choose, but perhaps the most commonly used is Cohen's <span class="math inline">\(\delta\)</span>.</p>
<p>The formula for this effect size measure is:</p>
<p><span class="math inline">\(\Huge \delta = \frac{\overline{X}_{1} - \overline{X}_{2}}{\hat{\sigma}_{\rho}}\)</span></p>
<p>This is saying to divide the difference between the means of each group by the pooled standard deviation. For our <code>anastasia</code> and <code>bernadette</code> samples, we could do this 'by hand' like this (note: we calculated the pooled standard deviation <span class="math inline">\({\hat{\sigma}_{\rho}}\)</span> for these samples earlier in section xxx.xxx and saved it as <code>sd.pool</code>.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">mean</span>(anastasia) -<span class="st"> </span><span class="kw">mean</span>(bernadette))  /<span class="st"> </span>sd.pool </code></pre></div>
<pre><code>## [1] 7.505159</code></pre>
<p>Here, <span class="math inline">\(\delta = 0.74\)</span>.</p>
<p>The <code>t.test()</code> function does not produce the effect size by default. Instead, if we didn't want to calculate the effect size by hand, we can use the function <code>cohensD</code> from the library <code>lsr</code> on long format data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lsr)</code></pre></div>
<pre><code>## Warning: package &#39;lsr&#39; was built under R version 3.5.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cohensD</span>(values ~<span class="st"> </span>group, <span class="dt">data =</span> d)  </code></pre></div>
<pre><code>## [1] 0.7395614</code></pre>
<p>Again, <span class="math inline">\(\delta = 0.74\)</span>. This would be considered a large effect size. Any value of <span class="math inline">\(\delta\)</span> above 0.7 is considered to be a large effect size. Anything above 0.5 is a moderate effect size, and anything above 0.3 is a small effect size.</p>
<p>When reporting results of your t-test, you should always give the effect size too.</p>
</div>
<div id="paired-t-tests" class="section level2">
<h2><span class="header-section-number">10.11</span> Paired t-tests</h2>
<p>Paired data are data where we have two related scores per individual subject. For instance, we may have a 'before' and 'after' score on some test. Importantly, the scores need to be measured on the same scale. For instance, if we had 'height' and 'weight' of each subject, then these would not be paired data as they are measured on different scales.</p>
<p>When we have paired data, we may be interested in knowning if there is a difference between the two groups of scores in means over all subjects. We cannot proceed with an independent t-test to examine this, as the data all violate the assumption of independence. These data by their very nature are non-independent of each other. Instead, we need to account for the fact the data are paired.</p>
<p>Let's look at the following data which show exam scores in Dr Chico's class. We have 20 students who took two exams during the class. These are called <code>grade_test1</code> and <code>grade_test2</code>. As you can see below, the data are currently in &quot;wide format&quot; (see section xxx.xxx).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">chico &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/chico.csv&quot;</span>)
chico</code></pre></div>
<pre><code>## # A tibble: 20 x 3
##    id        grade_test1 grade_test2
##    &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;
##  1 student1         42.9        44.6
##  2 student2         51.8        54  
##  3 student3         71.7        72.3
##  4 student4         51.6        53.4
##  5 student5         63.5        63.8
##  6 student6         58          59.3
##  7 student7         59.8        60.8
##  8 student8         50.8        51.6
##  9 student9         62.5        64.3
## 10 student10        61.9        63.2
## 11 student11        50.4        51.8
## 12 student12        52.6        52.2
## 13 student13        63          63  
## 14 student14        58.3        60.5
## 15 student15        53.3        57.1
## 16 student16        58.7        60.1
## 17 student17        50.1        51.7
## 18 student18        64.2        65.6
## 19 student19        57.4        58.3
## 20 student20        57.1        60.1</code></pre>
<p>A typical visualization would be to look at the boxplot to compare the two samples:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">chico.long &lt;-<span class="st"> </span>chico %&gt;%<span class="st"> </span><span class="kw">pivot_longer</span>(<span class="dv">2</span>:<span class="dv">3</span>) 


<span class="co"># boxplot</span>
<span class="kw">ggplot</span>(chico.long, <span class="kw">aes</span>(<span class="dt">x=</span>name, <span class="dt">y=</span>value, <span class="dt">fill=</span>name))+
<span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">outlier.shape =</span> <span class="ot">NA</span>, <span class="dt">alpha=</span>.<span class="dv">5</span>) +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width=</span>.<span class="dv">1</span>, <span class="dt">size=</span><span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&quot;lightseagreen&quot;</span>,<span class="st">&quot;darkseagreen&quot;</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-423-1.png" width="672" /></p>
<p>The issue with this plot however, is that our data our paired. It looks as if there isn't much of a difference between the two groups. But with paired data we are actually more interested in how each individual changes their own score over time, not how the overall mean of each group changes.</p>
<p>There are two other ways to present paired data. A good visualization to use on paired data is a scatterplot, plotting each subject's twos scores as one datapoint.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Scatterplot
<span class="kw">head</span>(chico)</code></pre></div>
<pre><code>## # A tibble: 6 x 3
##   id       grade_test1 grade_test2
##   &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;
## 1 student1        42.9        44.6
## 2 student2        51.8        54  
## 3 student3        71.7        72.3
## 4 student4        51.6        53.4
## 5 student5        63.5        63.8
## 6 student6        58          59.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(chico, <span class="kw">aes</span>(<span class="dt">x=</span>grade_test1, <span class="dt">y=</span>grade_test2)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">theme_classic</span>()+
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span><span class="dv">0</span> , <span class="dt">slope =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-424-1.png" width="672" /></p>
<p>As you can see with this scatterplot, we have also added a diagonal line through the plot. This line represents values that are equivalent on test 1 and test 2. Most dots are above this diagonal line. This shows us that for most subjects, they have a higher score on test 2 than they do on test 1. We have 20 dots because we have 20 individual subjects.</p>
<p>Another option is to use a slope graph. Here we plot the individual dots of each group as if we are doing a boxplot, but then we join dots who are the same individual in each group together:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Slope Graph
<span class="kw">ggplot</span>(chico.long, <span class="kw">aes</span>(<span class="dt">x=</span>name, <span class="dt">y=</span>value, <span class="dt">group=</span>id))+
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha=</span>.<span class="dv">6</span>, <span class="dt">size=</span><span class="dv">2</span>)+
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color=</span><span class="st">&quot;gray50&quot;</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>()</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-425-1.png" width="672" /></p>
<p>What we are looking for in a plot such as this is whether a majority of the lines are going in the same direction as each other - either up or down. In this particular example, the absolute increase of each line is quite small and so it's harder to see the differences in this graph than the scatterplot. For other datasets, where there is a lot more variation in the magnitude and direction of the differences for each individual, the slope graph might be better than the scatterplot.</p>
<div id="the-paired-t-test-is-a-one-sample-t-test" class="section level3">
<h3><span class="header-section-number">10.11.1</span> The paired t-test is a one-sample t-test</h3>
<p>The general principle behind a paired t-test is that it is not a new test at all. In fact, we will be doing a one-sample t-test based on the difference between the two scores. We will be testing whether the true population mean difference in scores is likely to include 0. Please see section xxx.xxx for more information on the theory of one-sample t-tests. Here, we'll just walk through how to write the code in R, what assumptions you need to check, and what plots you should make.</p>
<p>Let's first do this by hand, before we see the very quick way to do this in R.</p>
<p>The first step is to calculate the difference scores. We create a new column called <code>improvement</code> that shows whether the test 2 score is higher or lower than the test 2 score. Positive scores indidcate that subjects scored more highly on test 2 than on test 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a new column that is the difference between our two columns:</span>
chico$improvement &lt;-<span class="st"> </span>chico$grade_test2 -<span class="st"> </span>chico$grade_test1 
chico</code></pre></div>
<pre><code>## # A tibble: 20 x 4
##    id        grade_test1 grade_test2 improvement
##    &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
##  1 student1         42.9        44.6       1.7  
##  2 student2         51.8        54         2.2  
##  3 student3         71.7        72.3       0.600
##  4 student4         51.6        53.4       1.80 
##  5 student5         63.5        63.8       0.300
##  6 student6         58          59.3       1.30 
##  7 student7         59.8        60.8       1    
##  8 student8         50.8        51.6       0.8  
##  9 student9         62.5        64.3       1.80 
## 10 student10        61.9        63.2       1.3  
## 11 student11        50.4        51.8       1.40 
## 12 student12        52.6        52.2      -0.400
## 13 student13        63          63         0    
## 14 student14        58.3        60.5       2.2  
## 15 student15        53.3        57.1       3.8  
## 16 student16        58.7        60.1       1.40 
## 17 student17        50.1        51.7       1.6  
## 18 student18        64.2        65.6       1.40 
## 19 student19        57.4        58.3       0.900
## 20 student20        57.1        60.1       3</code></pre>
<p>We can visually inspect these data by doing a historam of the difference scores:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># histogram of difference scores</span>
<span class="kw">ggplot</span>(chico, <span class="kw">aes</span>(<span class="dt">x=</span>improvement)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">fill=</span><span class="st">&#39;green&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>, <span class="dt">boundary=</span><span class="dv">0</span>, <span class="dt">binwidth =</span> .<span class="dv">5</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>()+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">color=</span><span class="st">&#39;black&#39;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-427-1.png" width="672" /></p>
<p>This shows us that the vast majority of scores are positive, indicating that the students generally did better on test 2 than test 1. The fist bar contains two values - one is a negative improvement score, and the other is 0.</p>
<p>We can also get the observed sample mean and standard deviation of these differences:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># descriptives of the difference scores</span>
<span class="kw">mean</span>(chico$improvement)</code></pre></div>
<pre><code>## [1] 1.405</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(chico$improvement)</code></pre></div>
<pre><code>## [1] 0.9703363</code></pre>
<p>This shows us that the average improvement score from our 'one sample' of difference scores is <span class="math inline">\(\overline{x}=1.405\)</span>.</p>
<p>Before we proceed with the test, we need to check that our data is normally distributed, as this is one of the assumptions of the test. We can do that using QQ plots or a Shapiro-Wilk test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check normality of difference scores</span>
<span class="kw">qqnorm</span>(chico$improvement)
<span class="kw">qqline</span>(chico$improvement, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>) <span class="co"># bit better</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-429-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(chico$improvement)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  chico$improvement
## W = 0.9664, p-value = 0.6778</code></pre>
<p>As the dots fall largely on the line, especially in the middle of our plot, the QQ plot suggests that date are approximately normal. Likewise, because the p-value of the Shapiro-Wilk test is greater than 0.05, this indicates that our data are approximately normal.</p>
<p>If you recall back to section xxx.xxx, to run a one-sample t-test in R we use <code>t.test()</code> and set <code>mu=0</code> to indicate that we are testing whether our sample comes from a population whose mean could be equal to 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(chico$improvement, <span class="dt">mu=</span><span class="dv">0</span>) </code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  chico$improvement
## t = 6.4754, df = 19, p-value = 3.321e-06
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.9508686 1.8591314
## sample estimates:
## mean of x 
##     1.405</code></pre>
<p>This output suggests that the 95% confidence interval of the true population mean of difference (improvement) scores is <span class="math inline">\(CI_{95\%} = 1.405[0.951, 1.859]\)</span>. The observed <span class="math inline">\(t\)</span>-value is <span class="math inline">\(t=6.475\)</span> suggesting that the observed sample mean difference score is relatively large compare to the range of mean difference scores we could have got. Indeed, we have a significant p-value, <span class="math inline">\(p=0.000003\)</span> which indicates that our difference scores are unlikely to have come from a population with a true mean difference of <span class="math inline">\(\mu=0\)</span>.</p>
<p>Above, we just performed a one-sample t-test on the difference scores (the improvement column in our data). To do this, we needed to create the extra column of data. We actually didn't need to do this, we could have just done a paired t-test directly on our data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(chico$grade_test2, chico$grade_test1, <span class="dt">paired=</span>T)  </code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  chico$grade_test2 and chico$grade_test1
## t = 6.4754, df = 19, p-value = 3.321e-06
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.9508686 1.8591314
## sample estimates:
## mean of the differences 
##                   1.405</code></pre>
<p>By including <code>paired=T</code> we are telling R to essentially run a one-sample t-test on the difference scores - this is the paired t-test. Notice the output is identical.</p>
</div>
<div id="one-tailed-paired-t-tests" class="section level3">
<h3><span class="header-section-number">10.11.2</span> One-tailed paired t-tests</h3>
<p>The above test we just completed is a two-tailed t-test. This is testing the null hypothesis that the population mean of the difference scores is equal to 0, and the alternative hypothesis is that it is not equal to 0. If we had made an a priori prediction that test 2 would have higher scores than test 1, we could run a one-tailed test. The hypotheses for a one-tailed test would be:</p>
<p><span class="math inline">\(H_0: \mu \le 0\)</span> <span class="math inline">\(H_1: \mu &gt; 0\)</span></p>
<p>To run the one-tailed test, we simply add <code>alternative=&quot;greater&quot;</code> or <code>alternative=&quot;less&quot;</code> depending upon which direction we are testing. In our case, we predict that test 2 will be greater than test 1, therefore we do the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(chico$grade_test2, chico$grade_test1, <span class="dt">paired=</span>T, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)  <span class="co"># this is 1-tailed</span></code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  chico$grade_test2 and chico$grade_test1
## t = 6.4754, df = 19, p-value = 1.66e-06
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  1.029823      Inf
## sample estimates:
## mean of the differences 
##                   1.405</code></pre>
</div>
<div id="calculating-effect-sizes" class="section level3">
<h3><span class="header-section-number">10.11.3</span> Calculating effect sizes</h3>
<p>As with the independent t-test, just because you observe a 'signficant' difference between the two groups does not indicate that that difference is particularly large or interesting. To get a better indicator of how large or meaningful a difference in group means is, we should calculate an effect size. There are numerous different effect size measures that exist. One that is commonly reported for the paired t-test is another version of Cohen's <span class="math inline">\(\delta\)</span>. For a paired t-test, this is calculated by the formula:</p>
<p><span class="math inline">\(\delta = \frac{\overline{d}}{s_d}\)</span></p>
<p>where <span class="math inline">\(\overline{d}\)</span> refers to the mean of the differences in scores for each individual, and <span class="math inline">\(s_d\)</span> is the standard deviation of the differences in scores for each individual.</p>
<p>Therefore for our <code>chico</code> dataset, the effect size <span class="math inline">\(\delta = 1.45\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Effect Size is equal to:</span>
<span class="kw">mean</span>(chico$improvement) /<span class="st"> </span><span class="kw">sd</span>(chico$improvement) </code></pre></div>
<pre><code>## [1] 1.447952</code></pre>
<p>This indicates a very large effect. As with the indendent t-test (see section xxx.xxx) anything above 0.7 can be considered a large effect, anything above 0.5 is a moderate effect, and anything above 0.3 is a small effect.</p>
</div>
</div>
<div id="non-parametric-alternatives-for-independent-t-tests" class="section level2">
<h2><span class="header-section-number">10.12</span> Non-parametric Alternatives for Independent t-tests</h2>
<p>One of the key assumptions of the independent t-test is that the data come from populations that are approximately normally distributed. If your data do not appear to be normal, there are several options still available. One of these is to do a permutation t-test (see section xxx.xxx). Another option is to run a non-parametric statistical test. We won't go into the theory here, instead just focsuing on how to run these tests in R.</p>
<p>Let's look at an example dataset of how we would do these non-parametric tests.</p>
<p>In a study, a researcher measured anxiety scores of subjects 1hour after taking placebo or anti-anxiety drug. These are the scores. Because different individuals are in different groups, this is a between subjects design, and therefore would be appropriate for an independent t-test.</p>
<p>These are the scores:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">placebo &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>, <span class="dv">16</span>, <span class="dv">19</span>, <span class="dv">19</span>, <span class="dv">17</span>, <span class="dv">20</span>, <span class="dv">18</span>, <span class="dv">14</span>, <span class="dv">18</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">13</span>, <span class="dv">11</span>, <span class="dv">16</span>, <span class="dv">19</span>, <span class="dv">19</span>, <span class="dv">16</span>, <span class="dv">10</span>) 

drug &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>, <span class="dv">15</span>, <span class="dv">16</span>, <span class="dv">13</span>, <span class="dv">11</span>, <span class="dv">19</span>, <span class="dv">17</span>, <span class="dv">17</span>, <span class="dv">11</span>, <span class="dv">14</span>, <span class="dv">10</span>, <span class="dv">18</span>, <span class="dv">19</span>, <span class="dv">14</span>, <span class="dv">13</span>, <span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">17</span>, <span class="dv">14</span>, <span class="dv">10</span>, <span class="dv">14</span>)</code></pre></div>
<p>We can put these into a dataframe:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># put into dataframe - long format</span>
df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">anxiety =</span> <span class="kw">c</span>(placebo, drug),
           <span class="dt">group =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;placebo&quot;</span>, <span class="kw">length</span>(placebo)), 
                     <span class="kw">rep</span>(<span class="st">&quot;drug&quot;</span>, <span class="kw">length</span>(drug))
                     )
           )

<span class="kw">head</span>(df)</code></pre></div>
<pre><code>##   anxiety   group
## 1      15 placebo
## 2      16 placebo
## 3      19 placebo
## 4      19 placebo
## 5      17 placebo
## 6      20 placebo</code></pre>
<p>We can visualize these data and group differences by making a boxplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Plot Data of anxiety by group

<span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">x=</span>group, <span class="dt">y=</span>anxiety, <span class="dt">fill=</span>group)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">outlier.shape =</span> <span class="ot">NA</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>) +
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width=</span>.<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;brown&quot;</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-436-1.png" width="672" /></p>
<p>Looking at these data, it seems that the anti-anxiety drug group does have subjects with generally lower overall anxiety scores than the placebo group. A logical next step would be to think about doing an independent t-test.</p>
<p>To check whether our data are normally distributed, and therefore appropriate for an independent t-test, we need to perform a Shapiro-Wilk test on the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test if each group is approximately normally distributed</span>
<span class="kw">shapiro.test</span>(drug)    </code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  drug
## W = 0.95184, p-value = 0.3688</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(placebo)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  placebo
## W = 0.88372, p-value = 0.02494</code></pre>
<p>As you can se from this output, this suggests that the placebo data are not normally distributed as the p-value is less than 0.05. This means that continuing with the independent t-test would be inappropriate.</p>
<p>An alternative approach would be to do what's called a non-parametric alternative to the independent t-test. This test is called <em>Wilcoxon Ranked Sum Test</em> (also called a Mann-Whitney U test). Essentially this test ranks the data from both groups and examines whether one group has significantly more of the higher or lower ranks. Practically, what it is testing is whether there is a difference in the medians of the populations that the two samples came from.</p>
<p>To run this test in R, we just use the function <code>wilcox.test()</code> and include our two vectors of data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(placebo, drug) <span class="co"># 2-tailed</span></code></pre></div>
<pre><code>## Warning in wilcox.test.default(placebo, drug): cannot compute exact p-value with
## ties</code></pre>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  placebo and drug
## W = 286, p-value = 0.0191
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>The default is to run a 2-tailed test. You are given a test-statistic <span class="math inline">\(W\)</span> and a p-value. Here, <span class="math inline">\(p=0.0191\)</span> which indicates that our two samples have significantly different medians. With this test, you will get a warning about not being able to compute an exact p-value with ties. This isn't really important and can be ignored. In fact, you can turn it off by using the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(placebo, drug, <span class="dt">exact=</span>F) <span class="co"># this gets rid of obnoxious warnings, but not a big deal</span></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  placebo and drug
## W = 286, p-value = 0.0191
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>We can also get a 95% confidence interval of the difference in population medians between the two groups by including <code>conf.int=T</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(placebo, drug, <span class="dt">exact=</span>F, <span class="dt">conf.int =</span> T) </code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  placebo and drug
## W = 286, p-value = 0.0191
## alternative hypothesis: true location shift is not equal to 0
## 95 percent confidence interval:
##  3.072682e-05 4.000088e+00
## sample estimates:
## difference in location 
##               2.000002</code></pre>
<p>This tells us that the lower bound of the confidence interval is 0.000031 and the upper bound is 4.0.</p>
<p>If your data are in a dataframe, you can run the test if your data are in long format like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(df)</code></pre></div>
<pre><code>##   anxiety   group
## 1      15 placebo
## 2      16 placebo
## 3      19 placebo
## 4      19 placebo
## 5      17 placebo
## 6      20 placebo</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(anxiety ~<span class="st"> </span>group, <span class="dt">data=</span>df, <span class="dt">exact=</span>F, <span class="dt">conf.int=</span>T)</code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  anxiety by group
## W = 113, p-value = 0.0191
## alternative hypothesis: true location shift is not equal to 0
## 95 percent confidence interval:
##  -4.000088e+00 -3.072682e-05
## sample estimates:
## difference in location 
##              -2.000002</code></pre>
<p>There is also a one-tailed version of this test, which you can do in the same way as you do for the t-test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(placebo, drug, <span class="dt">exact=</span>F, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>) <span class="co"># 1-tailed</span></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  placebo and drug
## W = 286, p-value = 0.00955
## alternative hypothesis: true location shift is greater than 0</code></pre>
<p>Finally, as with t-tests, we should also report effect sizes for Wilcoxon Ranked Sum tests. This is because just determining there is a significant difference in population medians, does not tell us enough about how large this difference is. There are many different effect size measures that exist, all of which effectively evaluate the degree to which one group has higher ranks of data than the other group. Without going into formulaic details, the easiest way to calculate this effect size is to use the function <code>wilcoxonR()</code> from the package <code>rcompanion</code>. This function needs your data to be in long-data format.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rcompanion)</code></pre></div>
<pre><code>## Warning: package &#39;rcompanion&#39; was built under R version 3.5.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcoxonR</span>(<span class="dt">x  =</span> df$anxiety,
          <span class="dt">g  =</span> df$group,
          <span class="dt">ci =</span> T)</code></pre></div>
<pre><code>##        r lower.ci upper.ci
## 1 -0.373   -0.634  -0.0702</code></pre>
<p>We can ignore the sign here. What we notice is that the effect size is 0.37, with a possible range of 0.05 to 0.63. With this effect size measure, medium effects are above 0.3, and large effects are over 0.5.</p>
</div>
<div id="non-parametric-alternatives-to-the-two-sample-t-tests" class="section level2">
<h2><span class="header-section-number">10.13</span> Non-parametric Alternatives to the Two Sample t-tests</h2>
<p>Likewise if Paired Data are not normal, you should not perform a paired t-test. Instead, you can use a <em>Wilcoxon Signed Rank Test</em>. This is basically the non-parametric version of the one-sample t-test. It essentially assesses how likely your difference scores are to come from a population whose population median is not equal to 0.</p>
<p>In this example, we look at the <code>bloodwork</code> dataset. The two columns called <code>immuncount</code> and <code>immuncount2</code> refer to before and after measurements of some immune cells in blood.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df1 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/bloodwork.csv&quot;</span>) %&gt;%<span class="st"> </span><span class="kw">select</span>(ids, immuncount, immuncount2)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   ids = col_character(),
##   age = col_integer(),
##   sex = col_character(),
##   state = col_character(),
##   children = col_integer(),
##   smoker = col_character(),
##   hrate = col_integer(),
##   bpsyst = col_integer(),
##   cellcount = col_double(),
##   immuncount = col_double(),
##   immuncount2 = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(df1)</code></pre></div>
<pre><code>## # A tibble: 6 x 3
##   ids   immuncount immuncount2
##   &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;
## 1 JEC        0.993       0.921
## 2 GBH        1.18        1.19 
## 3 EDH        4.34        3.21 
## 4 AAA        2.56        4.01 
## 5 AJF        6.45        9.13 
## 6 FJC        3.97        2.85</code></pre>
<p>If we examine whether the data are approximately normal, we find out that they are definitely not approximately normally distributed as the Shapio-Wilk test on each give p-values far below 0.05:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(df1$immuncount)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  df1$immuncount
## W = 0.9028, p-value = 0.00984</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(df1$immuncount2)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  df1$immuncount2
## W = 0.80749, p-value = 9.006e-05</code></pre>
<p>We can visualize the differences between time point 1 and time point 2 using a scatterplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># scatterplot</span>
<span class="kw">ggplot</span>(df1, <span class="kw">aes</span>(<span class="dt">x =</span> immuncount, <span class="dt">y=</span>immuncount2 )) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">theme_classic</span>() +
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span><span class="dv">0</span> , <span class="dt">slope =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-446-1.png" width="672" /></p>
<p>Looking at this, it looks like lots of low values are close to the line, some middle values are below the line and some are a long way above the line.</p>
<p>We can next test if the population median of the difference scores is equal to 0 or not. We use the <code>wilcox.test</code> function but use the <code>paired=T</code> argument to ensure we do a paired test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Wilcoxon Signed Rank Test
<span class="kw">wilcox.test</span>(df1$immuncount, df1$immuncount2,  <span class="dt">paired=</span>T, <span class="dt">conf.int =</span> T) <span class="co">#with CIs</span></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test
## 
## data:  df1$immuncount and df1$immuncount2
## V = 202, p-value = 0.5425
## alternative hypothesis: true location shift is not equal to 0
## 95 percent confidence interval:
##  -1.1615  0.1365
## sample estimates:
## (pseudo)median 
##        -0.1345</code></pre>
<p>This gives us confidence intervals of the population median in differences cores of -0.135[-1.162, 0.137] which includes 0 and therefore suggets that there is no difference in medians between the paired groups. The p-value confirms this being <span class="math inline">\(p=0.54\)</span>. The <span class="math inline">\(V\)</span> value is the test-statistic.</p>
<p>Should you need a 1-tailed test you can do it this way - this time we use <code>alternative=&quot;less&quot;</code> as we're seeing if time point 1 has a lower median that time point 2 across individuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(df1$immuncount, df1$immuncount2,  <span class="dt">paired=</span>T, <span class="dt">alternative =</span> <span class="st">&quot;less&quot;</span>) <span class="co"># 1-tailed</span></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test
## 
## data:  df1$immuncount and df1$immuncount2
## V = 202, p-value = 0.2713
## alternative hypothesis: true location shift is less than 0</code></pre>
<p>Again, we have an effct size measure for this test. This can be done using the <code>wilcoxonPairedR()</code> function from the <code>rcompanion</code> package.</p>
<p>The downside of this function is that it requires the data to be ordered specifically. The top half of the data are the values for group1. The bottom half of the data are the values for group2. Then the id variable needs to be in the same order for both groups.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df2 &lt;-<span class="st"> </span>df1 %&gt;%<span class="st"> </span><span class="kw">pivot_longer</span>(<span class="dv">2</span>:<span class="dv">3</span>) %&gt;%<span class="st"> </span><span class="kw">arrange</span>(name,ids)

<span class="kw">head</span>(df2)</code></pre></div>
<pre><code>## # A tibble: 6 x 3
##   ids   name       value
##   &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;
## 1 AAA   immuncount 2.56 
## 2 ACC   immuncount 4.66 
## 3 AGC   immuncount 1.44 
## 4 AJF   immuncount 6.45 
## 5 BED   immuncount 1.18 
## 6 BFB   immuncount 0.724</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcoxonPairedR</span>(<span class="dt">x  =</span> df2$value,
          <span class="dt">g  =</span> df2$name,
          <span class="dt">ci =</span> T)</code></pre></div>
<pre><code>##        r lower.ci upper.ci
## 1 -0.114   -0.445     0.29</code></pre>
<p>What we can see from this output is that the estimate of the effect size is -0.11[-0.45, 0.27]. This suggests that the likely effect size is 0.11 but 0 is also likely to be the effect size as it is inside the confidence interval. This is further evidence that there is no real effect of changes in scores from time point 1 to time point 2 in these data.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="one-sample-inferential-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="correlation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
